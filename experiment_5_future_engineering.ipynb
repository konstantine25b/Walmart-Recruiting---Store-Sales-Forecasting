{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# experiment_5_future_engineering.ipynb"
      ],
      "metadata": {
        "id": "NQYMcDRbeZXU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "rT7xnxa1coCn",
        "outputId": "7d3e8211-ebfc-4b5b-abae-3a746657202e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "0ZsCV4XkfVZr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "OI4oFsbMfWsV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "hZjJ0PdvfYH8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "mypEvmtGfZkt",
        "outputId": "8d80634c-11a3-4a18-9332-363ce39130bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 464MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "id": "GTRAYmmzfmKn",
        "outputId": "052cda8e-7f4c-49c3-80e2-22c87614d8ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "ecDAePQ5foIh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prophet plotly mlflow dagshub xgboost -q"
      ],
      "metadata": {
        "id": "Tv_ufHuFfrLX",
        "outputId": "1e76f97b-cfc7-4d51-8e76-3d0ab213cd3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m96.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")"
      ],
      "metadata": {
        "id": "MJe1694TfuoT",
        "outputId": "fc59ed42-e746-4fbc-9a2b-03c59f1bab8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 1: SETUP MLflow AND DAGSHUB\n",
        "# ================================================================================\n",
        "\n",
        "def setup_mlflow():\n",
        "    \"\"\"Setup MLflow and DagsHub tracking\"\"\"\n",
        "    print(\"🔧 Setting up MLflow and DagsHub...\")\n",
        "\n",
        "    # End any active runs first\n",
        "    try:\n",
        "        mlflow.end_run()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize DagsHub\n",
        "    try:\n",
        "        dagshub.init(\n",
        "            repo_owner='konstantine25b',\n",
        "            repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "            mlflow=True\n",
        "        )\n",
        "        print(\"✅ DagsHub initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ DagsHub init warning: {e}\")\n",
        "\n",
        "    # Set MLflow tracking URI\n",
        "    mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "\n",
        "    # Create unique experiment name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_name = f\"Experiment_5_Feature_Engineering_Pipeline_{timestamp}\"\n",
        "\n",
        "    try:\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"✅ Created new experiment: {experiment_name}\")\n",
        "    except mlflow.exceptions.MlflowException as e:\n",
        "        if \"already exists\" in str(e):\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"✅ Using existing experiment: {experiment_name}\")\n",
        "        else:\n",
        "            # Fallback to default experiment\n",
        "            experiment_name = \"Default\"\n",
        "            mlflow.set_experiment(experiment_name)\n",
        "            print(f\"⚠️ Using default experiment due to: {e}\")\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    print(f\"✅ MLflow setup complete!\")\n",
        "    print(f\"🔗 Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    print(f\"📊 Experiment: {experiment_name}\")\n",
        "\n",
        "    return experiment_name\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 2: DATA LOADING\n",
        "# ================================================================================\n",
        "\n",
        "def load_raw_data():\n",
        "    \"\"\"Load and merge all datasets\"\"\"\n",
        "    with mlflow.start_run(run_name=\"Data_Loading_Complete\", nested=True) as run:\n",
        "        print(\"📂 Loading datasets...\")\n",
        "\n",
        "        try:\n",
        "            # Load main datasets\n",
        "            train_df = pd.read_csv('train.csv')\n",
        "            stores_df = pd.read_csv('stores.csv')\n",
        "            features_df = pd.read_csv('features.csv')\n",
        "\n",
        "            print(f\"✅ Loaded datasets:\")\n",
        "            print(f\"   📊 train.csv: {train_df.shape}\")\n",
        "            print(f\"   🏪 stores.csv: {stores_df.shape}\")\n",
        "            print(f\"   📅 features.csv: {features_df.shape}\")\n",
        "\n",
        "            # Log raw dataset shapes\n",
        "            mlflow.log_metric(\"raw_train_rows\", len(train_df))\n",
        "            mlflow.log_metric(\"raw_train_cols\", train_df.shape[1])\n",
        "            mlflow.log_metric(\"raw_stores_count\", len(stores_df))\n",
        "            mlflow.log_metric(\"raw_features_rows\", len(features_df))\n",
        "\n",
        "            # Parse dates\n",
        "            train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "            features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "            # Merge datasets - IMPORTANT: Include IsHoliday in merge keys to avoid duplicates\n",
        "            # Both train.csv and features.csv have IsHoliday column, so we merge on it\n",
        "            data = train_df.merge(stores_df, on='Store', how='left')\n",
        "            data = data.merge(features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "\n",
        "            print(f\"📊 Merged dataset shape: {data.shape}\")\n",
        "            mlflow.log_metric(\"merged_rows\", len(data))\n",
        "            mlflow.log_metric(\"merged_cols\", data.shape[1])\n",
        "\n",
        "            # Verify IsHoliday column exists\n",
        "            if 'IsHoliday' not in data.columns:\n",
        "                raise ValueError(\"❌ Critical: IsHoliday column not found in merged data!\")\n",
        "\n",
        "            holiday_count = data['IsHoliday'].sum()\n",
        "            print(f\"🎄 Holiday weeks in dataset: {holiday_count}\")\n",
        "            mlflow.log_metric(\"total_holiday_weeks\", int(holiday_count))\n",
        "\n",
        "            # Remove markdown columns that cause issues\n",
        "            markdown_cols = [col for col in data.columns if 'markdown' in col.lower()]\n",
        "            if markdown_cols:\n",
        "                data = data.drop(columns=markdown_cols)\n",
        "                print(f\"🗑️ Removed markdown columns: {len(markdown_cols)}\")\n",
        "                mlflow.log_metric(\"markdown_cols_removed\", len(markdown_cols))\n",
        "\n",
        "            # Basic data info\n",
        "            print(f\"📊 Final dataset: {data.shape}\")\n",
        "            print(f\"📅 Date range: {data['Date'].min()} to {data['Date'].max()}\")\n",
        "            print(f\"🏪 Stores: {data['Store'].nunique()}\")\n",
        "            print(f\"🏷️ Departments: {data['Dept'].nunique()}\")\n",
        "\n",
        "            # Log dataset metrics\n",
        "            mlflow.log_metric(\"total_records\", len(data))\n",
        "            mlflow.log_metric(\"stores_count\", data['Store'].nunique())\n",
        "            mlflow.log_metric(\"departments_count\", data['Dept'].nunique())\n",
        "            mlflow.log_metric(\"features_count\", data.shape[1])\n",
        "            mlflow.log_param(\"date_range_start\", str(data['Date'].min()))\n",
        "            mlflow.log_param(\"date_range_end\", str(data['Date'].max()))\n",
        "            mlflow.log_param(\"has_isholiday\", True)\n",
        "\n",
        "            # Log data quality metrics\n",
        "            missing_values = data.isnull().sum().sum()\n",
        "            mlflow.log_metric(\"total_missing_values\", int(missing_values))\n",
        "            mlflow.log_metric(\"missing_percentage\", float(missing_values / (len(data) * data.shape[1]) * 100))\n",
        "\n",
        "            print(\"✅ Data loading completed!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading data: {str(e)}\")\n",
        "            mlflow.log_param(\"loading_error\", str(e))\n",
        "            raise\n",
        "\n",
        "        return data\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 3: REUSABLE FEATURE ENGINEERING CLASS\n",
        "# ================================================================================\n",
        "\n",
        "class WalmartFeatureEngineer:\n",
        "    \"\"\"Reusable feature engineering for Walmart sales data\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_names = []\n",
        "        self.label_encoders = {}\n",
        "        self.fitted = False\n",
        "\n",
        "    def create_date_features(self, df):\n",
        "        \"\"\"Create date-based features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic date features\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "        # Cyclical features (important for seasonality)\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Quarter_sin'] = np.sin(2 * np.pi * df['Quarter'] / 4)\n",
        "        df['Quarter_cos'] = np.cos(2 * np.pi * df['Quarter'] / 4)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "        # Time progression features\n",
        "        reference_date = pd.Timestamp('2010-02-05')\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "\n",
        "        # Holiday-related features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsQ4'] = (df['Quarter'] == 4).astype(int)\n",
        "\n",
        "        date_features = ['Year', 'Month', 'Quarter', 'DayOfWeek', 'Week', 'DayOfYear',\n",
        "                        'Month_sin', 'Month_cos', 'Quarter_sin', 'Quarter_cos',\n",
        "                        'DayOfWeek_sin', 'DayOfWeek_cos', 'DaysFromStart', 'WeeksFromStart',\n",
        "                        'IsHolidayMonth', 'IsQ4']\n",
        "\n",
        "        return df, date_features\n",
        "\n",
        "    def create_lag_features(self, df, target_col='Weekly_Sales', lags=[1, 2, 4, 8]):\n",
        "        \"\"\"Create lag features without data leakage\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        lag_features = []\n",
        "        for lag in lags:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "            lag_features.append(lag_col)\n",
        "\n",
        "        return df, lag_features\n",
        "\n",
        "    def create_rolling_features(self, df, target_col='Weekly_Sales', windows=[4, 8, 12]):\n",
        "        \"\"\"Create rolling window features\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        rolling_features = []\n",
        "        for window in windows:\n",
        "            # Rolling statistics with proper index handling\n",
        "            rolling_group = df.groupby(['Store', 'Dept'])[target_col].rolling(window=window, min_periods=1)\n",
        "\n",
        "            # Use .values to avoid index conflicts\n",
        "            df[f'{target_col}_rolling_mean_{window}'] = rolling_group.mean().values\n",
        "            df[f'{target_col}_rolling_std_{window}'] = rolling_group.std().fillna(0).values\n",
        "            df[f'{target_col}_rolling_min_{window}'] = rolling_group.min().values\n",
        "            df[f'{target_col}_rolling_max_{window}'] = rolling_group.max().values\n",
        "\n",
        "            # Rolling ratios with safe division\n",
        "            mean_values = df[f'{target_col}_rolling_mean_{window}']\n",
        "            df[f'{target_col}_vs_rolling_mean_{window}'] = np.where(\n",
        "                mean_values != 0,\n",
        "                df[target_col] / mean_values,\n",
        "                1.0\n",
        "            )\n",
        "\n",
        "            rolling_features.extend([\n",
        "                f'{target_col}_rolling_mean_{window}',\n",
        "                f'{target_col}_rolling_std_{window}',\n",
        "                f'{target_col}_rolling_min_{window}',\n",
        "                f'{target_col}_rolling_max_{window}',\n",
        "                f'{target_col}_vs_rolling_mean_{window}'\n",
        "            ])\n",
        "\n",
        "        return df, rolling_features\n",
        "\n",
        "    def create_store_dept_features(self, df):\n",
        "        \"\"\"Create store and department interaction features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Store-Department interaction\n",
        "        df['Store_Dept_Interaction'] = df['Store'].astype(str) + '_' + df['Dept'].astype(str)\n",
        "\n",
        "        # Store size categories\n",
        "        df['Store_Size_Category'] = pd.cut(df['Size'], bins=3, labels=['Small', 'Medium', 'Large'])\n",
        "\n",
        "        store_dept_features = ['Store_Dept_Interaction', 'Store_Size_Category']\n",
        "\n",
        "        return df, store_dept_features\n",
        "\n",
        "    def fit_transform(self, train_df):\n",
        "        \"\"\"Fit feature engineering on training data and transform\"\"\"\n",
        "        print(\"🔧 Fitting and transforming data...\")\n",
        "\n",
        "        df = train_df.copy()\n",
        "        all_features = []\n",
        "\n",
        "        # 1. Date features\n",
        "        df, date_features = self.create_date_features(df)\n",
        "        all_features.extend(date_features)\n",
        "\n",
        "        # 2. Lag features\n",
        "        df, lag_features = self.create_lag_features(df)\n",
        "        all_features.extend(lag_features)\n",
        "\n",
        "        # 3. Rolling features\n",
        "        df, rolling_features = self.create_rolling_features(df)\n",
        "        all_features.extend(rolling_features)\n",
        "\n",
        "        # 4. Store/Department features\n",
        "        df, store_dept_features = self.create_store_dept_features(df)\n",
        "        all_features.extend(store_dept_features)\n",
        "\n",
        "        # Handle categorical variables\n",
        "        categorical_cols = ['Type', 'Store_Size_Category', 'Store_Dept_Interaction']\n",
        "        for col in categorical_cols:\n",
        "            if col in df.columns:\n",
        "                le = LabelEncoder()\n",
        "                df[col] = le.fit_transform(df[col].astype(str))\n",
        "                self.label_encoders[col] = le\n",
        "\n",
        "        # Fill NaN values\n",
        "        # For lag and rolling features, forward fill then backward fill\n",
        "        lag_rolling_cols = lag_features + rolling_features\n",
        "        for col in lag_rolling_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df.groupby(['Store', 'Dept'])[col].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        # Fill remaining NaNs with 0\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        # Store feature names\n",
        "        self.feature_names = all_features\n",
        "        self.fitted = True\n",
        "\n",
        "        print(f\"✅ Feature engineering completed!\")\n",
        "        print(f\"📊 Total features created: {len(all_features)}\")\n",
        "        print(f\"   📅 Date features: {len(date_features)}\")\n",
        "        print(f\"   ⏰ Lag features: {len(lag_features)}\")\n",
        "        print(f\"   📊 Rolling features: {len(rolling_features)}\")\n",
        "        print(f\"   🏪 Store/Dept features: {len(store_dept_features)}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def transform(self, test_df):\n",
        "        \"\"\"Transform test/validation data using fitted parameters\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"❌ FeatureEngineer must be fitted before transform!\")\n",
        "\n",
        "        print(\"🔄 Transforming data...\")\n",
        "\n",
        "        df = test_df.copy()\n",
        "\n",
        "        # Apply same transformations as training\n",
        "        df, _ = self.create_date_features(df)\n",
        "        df, _ = self.create_lag_features(df)\n",
        "        df, _ = self.create_rolling_features(df)\n",
        "        df, _ = self.create_store_dept_features(df)\n",
        "\n",
        "        # Apply saved label encoders\n",
        "        for col, le in self.label_encoders.items():\n",
        "            if col in df.columns:\n",
        "                # Handle unseen categories\n",
        "                unique_vals = df[col].astype(str).unique()\n",
        "                for val in unique_vals:\n",
        "                    if val not in le.classes_:\n",
        "                        # Add new class to encoder\n",
        "                        le.classes_ = np.append(le.classes_, val)\n",
        "                df[col] = le.transform(df[col].astype(str))\n",
        "\n",
        "        # Fill NaN values same way as training\n",
        "        lag_rolling_cols = [col for col in df.columns if 'lag_' in col or 'rolling_' in col]\n",
        "        for col in lag_rolling_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = df.groupby(['Store', 'Dept'])[col].fillna(method='ffill').fillna(method='bfill')\n",
        "\n",
        "        df = df.fillna(0)\n",
        "\n",
        "        print(\"✅ Data transformed!\")\n",
        "        return df\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 4: COMPLETE REUSABLE PIPELINE CLASS\n",
        "# ================================================================================\n",
        "\n",
        "class WalmartFeaturePipeline:\n",
        "    \"\"\"Complete reusable pipeline with save/load functionality\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.feature_engineer = WalmartFeatureEngineer()\n",
        "        self.fitted = False\n",
        "        self.feature_columns = None\n",
        "        self.outlier_bounds = None\n",
        "\n",
        "    def remove_outliers(self, data):\n",
        "        \"\"\"Remove outliers using IQR method\"\"\"\n",
        "        print(\"🗑️ Removing outliers...\")\n",
        "\n",
        "        # Calculate outlier bounds by Store-Dept groups\n",
        "        sales_by_group = data.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "        Q1 = sales_by_group.transform(lambda x: x.quantile(0.25))\n",
        "        Q3 = sales_by_group.transform(lambda x: x.quantile(0.75))\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        # Store bounds for later use\n",
        "        self.outlier_bounds = {'lower': lower_bound, 'upper': upper_bound}\n",
        "\n",
        "        # Identify and remove outliers\n",
        "        outliers_mask = (data['Weekly_Sales'] < lower_bound) | (data['Weekly_Sales'] > upper_bound)\n",
        "        outliers_count = outliers_mask.sum()\n",
        "\n",
        "        data_clean = data[~outliers_mask].copy()\n",
        "\n",
        "        print(f\"🗑️ Removed {outliers_count:,} outliers ({outliers_count/len(data)*100:.1f}%)\")\n",
        "        return data_clean\n",
        "\n",
        "    def fit(self, data, remove_outliers=True):\n",
        "        \"\"\"Fit pipeline on data with outlier removal and feature engineering\"\"\"\n",
        "        with mlflow.start_run(run_name=\"Pipeline_Fit\", nested=True) as run:\n",
        "            print(\"🔧 Fitting pipeline...\")\n",
        "\n",
        "            # Log parameters\n",
        "            mlflow.log_param(\"remove_outliers\", remove_outliers)\n",
        "            mlflow.log_param(\"input_data_shape\", str(data.shape))\n",
        "            mlflow.log_metric(\"input_samples\", len(data))\n",
        "\n",
        "            # Apply outlier removal if requested\n",
        "            if remove_outliers:\n",
        "                print(\"🗑️ Removing outliers...\")\n",
        "                data_clean = self.remove_outliers(data)\n",
        "                outliers_removed = len(data) - len(data_clean)\n",
        "                outlier_rate = outliers_removed / len(data) * 100\n",
        "                print(f\"🗑️ Removed {outliers_removed:,} outliers ({outlier_rate:.1f}%)\")\n",
        "\n",
        "                mlflow.log_metric(\"outliers_removed\", outliers_removed)\n",
        "                mlflow.log_metric(\"outlier_rate_percent\", outlier_rate)\n",
        "                mlflow.log_metric(\"clean_samples\", len(data_clean))\n",
        "            else:\n",
        "                data_clean = data.copy()\n",
        "                mlflow.log_metric(\"outliers_removed\", 0)\n",
        "                mlflow.log_metric(\"outlier_rate_percent\", 0.0)\n",
        "\n",
        "            # Apply feature engineering\n",
        "            print(\"🔧 Fitting and transforming data...\")\n",
        "            data_featured = self.feature_engineer.fit_transform(data_clean)\n",
        "\n",
        "            # Store feature columns (excluding target and metadata)\n",
        "            exclude_cols = ['Weekly_Sales', 'Date', 'Store_Dept_Interaction', 'IsHoliday']\n",
        "            self.feature_columns = [col for col in data_featured.columns if col not in exclude_cols]\n",
        "\n",
        "            print(\"✅ Feature engineering completed!\")\n",
        "            print(f\"📊 Total features created: {len(self.feature_columns)}\")\n",
        "\n",
        "            # Count feature types\n",
        "            date_features = [col for col in self.feature_columns if any(x in col for x in ['Year', 'Month', 'Quarter', 'Day', 'Week', 'sin', 'cos', 'Holiday', 'FromStart'])]\n",
        "            lag_features = [col for col in self.feature_columns if 'lag_' in col]\n",
        "            rolling_features = [col for col in self.feature_columns if any(x in col for x in ['mean_', 'std_', 'min_', 'max_', 'ratio_'])]\n",
        "            store_dept_features = [col for col in self.feature_columns if any(x in col for x in ['Store', 'Dept', 'Size'])]\n",
        "\n",
        "            print(f\"   📅 Date features: {len(date_features)}\")\n",
        "            print(f\"   ⏰ Lag features: {len(lag_features)}\")\n",
        "            print(f\"   📊 Rolling features: {len(rolling_features)}\")\n",
        "            print(f\"   🏪 Store/Dept features: {len(store_dept_features)}\")\n",
        "\n",
        "            # Log feature counts\n",
        "            mlflow.log_metric(\"total_features\", len(self.feature_columns))\n",
        "            mlflow.log_metric(\"date_features\", len(date_features))\n",
        "            mlflow.log_metric(\"lag_features\", len(lag_features))\n",
        "            mlflow.log_metric(\"rolling_features\", len(rolling_features))\n",
        "            mlflow.log_metric(\"store_dept_features\", len(store_dept_features))\n",
        "\n",
        "            # Log feature names as parameters (first 10 to avoid parameter limit)\n",
        "            for i, feature in enumerate(self.feature_columns[:10]):\n",
        "                mlflow.log_param(f\"feature_{i+1}\", feature)\n",
        "\n",
        "            # Mark as fitted\n",
        "            self.fitted = True\n",
        "\n",
        "            print(f\"✅ Pipeline fitted on {len(data_featured):,} samples\")\n",
        "            print(f\"📊 Features available: {len(data_featured.columns)}\")\n",
        "\n",
        "            mlflow.log_param(\"pipeline_fitted\", True)\n",
        "            mlflow.log_metric(\"final_samples\", len(data_featured))\n",
        "            mlflow.log_metric(\"final_columns\", data_featured.shape[1])\n",
        "\n",
        "            return data_featured\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"Transform new data using fitted pipeline\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"❌ Pipeline must be fitted before transform!\")\n",
        "\n",
        "        print(\"🔄 Transforming new data...\")\n",
        "\n",
        "        # Apply feature engineering\n",
        "        data_featured = self.feature_engineer.transform(data)\n",
        "\n",
        "        # Select same features as training (excluding IsHoliday from feature matrix)\n",
        "        X = data_featured[self.feature_columns].copy()\n",
        "\n",
        "        # Ensure numeric and fill NaNs\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        print(f\"✅ Transformed {len(X):,} samples\")\n",
        "        return X\n",
        "\n",
        "    def fit_transform(self, data, remove_outliers=True):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        data_featured = self.fit(data, remove_outliers)\n",
        "        return self.transform_fitted_data(data_featured)\n",
        "\n",
        "    def transform_fitted_data(self, data_featured):\n",
        "        \"\"\"Transform already featured data\"\"\"\n",
        "        exclude_cols = ['Weekly_Sales', 'Date', 'Store_Dept_Interaction', 'IsHoliday']\n",
        "        # Select only the feature columns (exclude target and metadata)\n",
        "        feature_cols = [col for col in data_featured.columns if col not in exclude_cols]\n",
        "        X = data_featured[feature_cols].copy()\n",
        "\n",
        "        for col in X.columns:\n",
        "            if X[col].dtype == 'object':\n",
        "                X[col] = pd.to_numeric(X[col], errors='coerce')\n",
        "        X = X.fillna(0)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def save_pipeline(self, filepath='walmart_feature_pipeline.pkl'):\n",
        "        \"\"\"Save fitted pipeline\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"❌ Pipeline must be fitted before saving!\")\n",
        "\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "        print(f\"💾 Pipeline saved to {filepath}\")\n",
        "        return filepath\n",
        "\n",
        "    @staticmethod\n",
        "    def load_pipeline(filepath='walmart_feature_pipeline.pkl'):\n",
        "        \"\"\"Load fitted pipeline\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"❌ Pipeline file not found: {filepath}\")\n",
        "\n",
        "        with open(filepath, 'rb') as f:\n",
        "            pipeline = pickle.load(f)\n",
        "\n",
        "        print(f\"📂 Pipeline loaded from {filepath}\")\n",
        "        return pipeline\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 5: EVALUATION METRICS\n",
        "# ================================================================================\n",
        "\n",
        "def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "    \"\"\"\n",
        "    Calculate Weighted Mean Absolute Error (WMAE) for Walmart competition\n",
        "\n",
        "    Args:\n",
        "        y_true: Actual sales values\n",
        "        y_pred: Predicted sales values\n",
        "        is_holiday: Boolean array indicating holiday weeks\n",
        "        holiday_weight: Weight multiplier for holiday weeks (default 5.0)\n",
        "\n",
        "    Returns:\n",
        "        wmae: Weighted Mean Absolute Error\n",
        "    \"\"\"\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "    wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "    return wmae\n",
        "\n",
        "def evaluate_model_performance(y_train, train_pred, train_is_holiday,\n",
        "                             y_val, val_pred, val_is_holiday):\n",
        "    \"\"\"\n",
        "    Comprehensive model evaluation with all metrics\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing all evaluation metrics\n",
        "    \"\"\"\n",
        "    # Training metrics\n",
        "    train_mae = mean_absolute_error(y_train, train_pred)\n",
        "    train_wmae = calculate_wmae(y_train, train_pred, train_is_holiday)\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "\n",
        "    # Validation metrics\n",
        "    val_mae = mean_absolute_error(y_val, val_pred)\n",
        "    val_wmae = calculate_wmae(y_val, val_pred, val_is_holiday)\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "\n",
        "    metrics = {\n",
        "        'train_mae': train_mae,\n",
        "        'train_wmae': train_wmae,\n",
        "        'train_r2': train_r2,\n",
        "        'val_mae': val_mae,\n",
        "        'val_wmae': val_wmae,\n",
        "        'val_r2': val_r2\n",
        "    }\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 6: USAGE PATTERNS\n",
        "# ================================================================================\n",
        "\n",
        "def experiment_pattern():\n",
        "    \"\"\"Pattern 1: For experiments with train/val split\"\"\"\n",
        "    with mlflow.start_run(run_name=\"Experiment_Pattern_Train_Val\", nested=True) as run:\n",
        "        print(\"🧪 EXPERIMENT PATTERN: Train/Validation Split\")\n",
        "\n",
        "        # Load and split data\n",
        "        data = load_raw_data()\n",
        "        train_data, val_data, split_date = create_temporal_split(data)\n",
        "\n",
        "        # Create pipeline and fit on training only\n",
        "        pipeline = WalmartFeaturePipeline()\n",
        "        train_featured = pipeline.fit(train_data, remove_outliers=True)\n",
        "\n",
        "        # Prepare training data\n",
        "        X_train = pipeline.transform_fitted_data(train_featured)\n",
        "        y_train = train_featured['Weekly_Sales']\n",
        "        train_is_holiday = train_featured['IsHoliday']\n",
        "\n",
        "        # Transform validation data\n",
        "        X_val = pipeline.transform(val_data)\n",
        "        y_val = val_data['Weekly_Sales']\n",
        "        val_is_holiday = val_data['IsHoliday']\n",
        "\n",
        "        # Log experiment metrics\n",
        "        mlflow.log_param(\"pattern\", \"experiment\")\n",
        "        mlflow.log_param(\"split_date\", str(split_date))\n",
        "        mlflow.log_metric(\"train_samples\", len(X_train))\n",
        "        mlflow.log_metric(\"val_samples\", len(X_val))\n",
        "        mlflow.log_metric(\"features_count\", X_train.shape[1])\n",
        "\n",
        "        print(f\"📊 Training: {X_train.shape}\")\n",
        "        print(f\"📊 Validation: {X_val.shape}\")\n",
        "\n",
        "        return pipeline, X_train, X_val, y_train, y_val, train_is_holiday, val_is_holiday\n",
        "\n",
        "def create_temporal_split(data, train_ratio=0.8):\n",
        "    \"\"\"Create proper temporal split for time series data\"\"\"\n",
        "    with mlflow.start_run(run_name=\"Temporal_Split_80_20\", nested=True) as run:\n",
        "        print(\"✂️ Creating temporal split...\")\n",
        "\n",
        "        # Log split parameters\n",
        "        mlflow.log_param(\"split_method\", \"temporal\")\n",
        "        mlflow.log_param(\"train_ratio\", train_ratio)\n",
        "        mlflow.log_param(\"validation_ratio\", 1 - train_ratio)\n",
        "        mlflow.log_param(\"split_type\", \"time_series_aware\")\n",
        "\n",
        "        # Sort data chronologically\n",
        "        data_sorted = data.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # Calculate split date\n",
        "        min_date = data_sorted['Date'].min()\n",
        "        max_date = data_sorted['Date'].max()\n",
        "        total_days = (max_date - min_date).days\n",
        "        split_days = int(total_days * train_ratio)\n",
        "        split_date = min_date + timedelta(days=split_days)\n",
        "\n",
        "        # Align to Friday (since data is weekly)\n",
        "        while split_date.weekday() != 4:\n",
        "            split_date += timedelta(days=1)\n",
        "\n",
        "        # Create splits\n",
        "        train_data = data_sorted[data_sorted['Date'] < split_date].copy()\n",
        "        val_data = data_sorted[data_sorted['Date'] >= split_date].copy()\n",
        "\n",
        "        # Verify no data leakage\n",
        "        assert train_data['Date'].max() < val_data['Date'].min(), \"❌ DATA LEAKAGE DETECTED!\"\n",
        "\n",
        "        print(f\"📅 Split date: {split_date}\")\n",
        "        print(f\"🚂 Training: {len(train_data):,} records ({len(train_data)/len(data)*100:.1f}%)\")\n",
        "        print(f\"🔮 Validation: {len(val_data):,} records ({len(val_data)/len(data)*100:.1f}%)\")\n",
        "        print(f\"📊 Training date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "        print(f\"📊 Validation date range: {val_data['Date'].min()} to {val_data['Date'].max()}\")\n",
        "\n",
        "        # Log split metrics\n",
        "        mlflow.log_param(\"split_date\", str(split_date))\n",
        "        mlflow.log_metric(\"train_records\", len(train_data))\n",
        "        mlflow.log_metric(\"val_records\", len(val_data))\n",
        "        mlflow.log_metric(\"train_percentage\", len(train_data)/len(data)*100)\n",
        "        mlflow.log_metric(\"val_percentage\", len(val_data)/len(data)*100)\n",
        "        mlflow.log_param(\"train_date_start\", str(train_data['Date'].min()))\n",
        "        mlflow.log_param(\"train_date_end\", str(train_data['Date'].max()))\n",
        "        mlflow.log_param(\"val_date_start\", str(val_data['Date'].min()))\n",
        "        mlflow.log_param(\"val_date_end\", str(val_data['Date'].max()))\n",
        "\n",
        "        print(\"✅ Temporal split completed - no data leakage!\")\n",
        "        return train_data, val_data, split_date\n",
        "\n",
        "def production_pattern():\n",
        "    \"\"\"Pattern 2: For final model on full data\"\"\"\n",
        "    with mlflow.start_run(run_name=\"Production_Pattern_Full_Data\", nested=True) as run:\n",
        "        print(\"🚀 PRODUCTION PATTERN: Full Dataset Training\")\n",
        "\n",
        "        # Load full dataset\n",
        "        data = load_raw_data()\n",
        "\n",
        "        # Create pipeline and fit on ALL data\n",
        "        pipeline = WalmartFeaturePipeline()\n",
        "        full_featured = pipeline.fit(data, remove_outliers=True)\n",
        "\n",
        "        # Prepare full dataset\n",
        "        X_full = pipeline.transform_fitted_data(full_featured)\n",
        "        y_full = full_featured['Weekly_Sales']\n",
        "        is_holiday_full = full_featured['IsHoliday']\n",
        "\n",
        "        # Save pipeline for later use\n",
        "        pipeline_path = pipeline.save_pipeline('walmart_feature_pipeline.pkl')\n",
        "\n",
        "        # Log production metrics\n",
        "        mlflow.log_param(\"pattern\", \"production\")\n",
        "        mlflow.log_metric(\"full_samples\", len(X_full))\n",
        "        mlflow.log_metric(\"features_count\", X_full.shape[1])\n",
        "        mlflow.log_artifact(pipeline_path)\n",
        "\n",
        "        print(f\"📊 Full dataset: {X_full.shape}\")\n",
        "        print(f\"💾 Pipeline saved for production use\")\n",
        "\n",
        "        return pipeline, X_full, y_full, is_holiday_full\n",
        "\n",
        "def testing_pattern(test_data_path=None):\n",
        "    \"\"\"Pattern 3: For testing on completely new data\"\"\"\n",
        "    with mlflow.start_run(run_name=\"Testing_Pattern_New_Data\", nested=True) as run:\n",
        "        print(\"🔍 TESTING PATTERN: New Data Transformation\")\n",
        "\n",
        "        # Load fitted pipeline\n",
        "        pipeline = WalmartFeaturePipeline.load_pipeline('walmart_feature_pipeline.pkl')\n",
        "\n",
        "        # Load new test data (you would provide this)\n",
        "        if test_data_path:\n",
        "            test_data = pd.read_csv(test_data_path)\n",
        "        else:\n",
        "            print(\"ℹ️  No test data provided, using sample\")\n",
        "            # For demo, just use part of existing data\n",
        "            data = load_raw_data()\n",
        "            test_data = data.sample(100).copy()\n",
        "\n",
        "        # Transform new data\n",
        "        X_test = pipeline.transform(test_data)\n",
        "\n",
        "        # Log testing metrics\n",
        "        mlflow.log_param(\"pattern\", \"testing\")\n",
        "        mlflow.log_metric(\"test_samples\", len(X_test))\n",
        "        mlflow.log_metric(\"features_count\", X_test.shape[1])\n",
        "\n",
        "        print(f\"📊 Test data: {X_test.shape}\")\n",
        "\n",
        "        return X_test\n",
        "\n",
        "# ================================================================================\n",
        "# MAIN EXECUTION PIPELINE\n",
        "# ================================================================================\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution demonstrating complete feature engineering pipeline functionality\"\"\"\n",
        "    print(\"🚀 EXPERIMENT 5: Complete Feature Engineering Pipeline\")\n",
        "    print(\"🎯 GOAL: Validate full pipeline functionality with XGBoost\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Setup\n",
        "    experiment_name = setup_mlflow()\n",
        "\n",
        "    print(f\"📋 PIPELINE VALIDATION STEPS:\")\n",
        "    print(f\"   1. 📂 Load and prepare data\")\n",
        "    print(f\"   2. ✂️ Create temporal split (no data leakage)\")\n",
        "    print(f\"   3. 🔧 Fit pipeline on training data (with outlier removal)\")\n",
        "    print(f\"   4. 🔄 Transform validation data\")\n",
        "    print(f\"   5. 🤖 Validate with XGBoost model\")\n",
        "    print(f\"   6. 📊 Evaluate WMAE and feature importance\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    try:\n",
        "        with mlflow.start_run(run_name=\"Complete_Pipeline_Validation\") as main_run:\n",
        "            # Step 1: Load and prepare data\n",
        "            print(f\"\\n📂 STEP 1: Loading and preparing data...\")\n",
        "            data = load_raw_data()\n",
        "\n",
        "            # Step 2: Create temporal split\n",
        "            print(f\"\\n✂️ STEP 2: Creating temporal split...\")\n",
        "            train_data, val_data, split_date = create_temporal_split(data)\n",
        "\n",
        "            # Step 3: Fit pipeline on training data\n",
        "            print(f\"\\n🔧 STEP 3: Fitting pipeline on training data...\")\n",
        "            pipeline = WalmartFeaturePipeline()\n",
        "            train_featured = pipeline.fit(train_data, remove_outliers=True)\n",
        "\n",
        "            # Prepare training data (IsHoliday is preserved in train_featured after outlier removal)\n",
        "            X_train = pipeline.transform_fitted_data(train_featured)\n",
        "            y_train = train_featured['Weekly_Sales']\n",
        "            train_is_holiday = train_featured['IsHoliday'].values  # Use IsHoliday from the featured data\n",
        "\n",
        "            # Step 4: Transform validation data\n",
        "            print(f\"\\n🔄 STEP 4: Transforming validation data...\")\n",
        "            X_val = pipeline.transform(val_data)\n",
        "            y_val = val_data['Weekly_Sales']\n",
        "            val_is_holiday = val_data['IsHoliday'].values  # Use IsHoliday from original val_data\n",
        "\n",
        "            # Log pipeline metrics\n",
        "            mlflow.log_param(\"pipeline_type\", \"complete_validation\")\n",
        "            mlflow.log_param(\"split_date\", str(split_date))\n",
        "            mlflow.log_metric(\"train_samples\", len(X_train))\n",
        "            mlflow.log_metric(\"val_samples\", len(X_val))\n",
        "            mlflow.log_metric(\"features_count\", X_train.shape[1])\n",
        "            mlflow.log_metric(\"stores_count\", data['Store'].nunique())\n",
        "            mlflow.log_metric(\"departments_count\", data['Dept'].nunique())\n",
        "            mlflow.log_metric(\"train_holiday_weeks\", int(train_is_holiday.sum()))\n",
        "            mlflow.log_metric(\"val_holiday_weeks\", int(val_is_holiday.sum()))\n",
        "\n",
        "            print(f\"\\n📊 PIPELINE SUMMARY:\")\n",
        "            print(f\"   🚂 Training data: {X_train.shape}\")\n",
        "            print(f\"   🔮 Validation data: {X_val.shape}\")\n",
        "            print(f\"   📈 Features created: {X_train.shape[1]}\")\n",
        "            print(f\"   📅 Split date: {split_date}\")\n",
        "            print(f\"   🎄 Training holiday weeks: {train_is_holiday.sum()}\")\n",
        "            print(f\"   🎄 Validation holiday weeks: {val_is_holiday.sum()}\")\n",
        "\n",
        "            # Verify data consistency\n",
        "            print(f\"\\n🔍 DATA CONSISTENCY CHECKS:\")\n",
        "            print(f\"   ✅ Train X shape matches y: {len(X_train) == len(y_train)}\")\n",
        "            print(f\"   ✅ Train IsHoliday matches: {len(train_is_holiday) == len(y_train)}\")\n",
        "            print(f\"   ✅ Val X shape matches y: {len(X_val) == len(y_val)}\")\n",
        "            print(f\"   ✅ Val IsHoliday matches: {len(val_is_holiday) == len(y_val)}\")\n",
        "\n",
        "            # Step 5 & 6: Validate with XGBoost and evaluate\n",
        "            print(f\"\\n🤖 STEP 5-6: Final XGBoost validation...\")\n",
        "            final_metrics = validate_complete_pipeline_with_xgboost(\n",
        "                X_train, X_val, y_train, y_val, train_is_holiday, val_is_holiday\n",
        "            )\n",
        "\n",
        "            # Log final results\n",
        "            for metric_name, value in final_metrics.items():\n",
        "                mlflow.log_metric(f\"final_{metric_name}\", value)\n",
        "\n",
        "            # Log additional useful metrics\n",
        "            mlflow.log_metric(\"data_leakage_check\", 1.0)  # Passed if we get here\n",
        "            mlflow.log_metric(\"feature_engineering_success\", 1.0)\n",
        "            mlflow.log_metric(\"outlier_removal_rate\", (len(train_data) - len(train_featured)) / len(train_data))\n",
        "\n",
        "            print(f\"\\n\" + \"=\"*80)\n",
        "            print(f\"🎉 COMPLETE PIPELINE VALIDATION SUCCESSFUL!\")\n",
        "            print(f\"🎯 Final Validation WMAE: ${final_metrics['val_wmae']:,.2f}\")\n",
        "            print(f\"📊 Features: {X_train.shape[1]}\")\n",
        "            print(f\"🚂 Training samples: {X_train.shape[0]:,}\")\n",
        "            print(f\"🔮 Validation samples: {X_val.shape[0]:,}\")\n",
        "            print(f\"📈 Validation R²: {final_metrics['val_r2']:.4f}\")\n",
        "            print(f\"🎄 Holiday weeks processed: {train_is_holiday.sum() + val_is_holiday.sum()}\")\n",
        "            print(f\"💾 Pipeline ready for production use!\")\n",
        "            print(f\"=\"*80)\n",
        "\n",
        "            # Optional: Show how to use in production\n",
        "            print(f\"\\n💡 PRODUCTION USAGE:\")\n",
        "            print(f\"   1. Call production_pattern() to fit on full dataset\")\n",
        "            print(f\"   2. Save pipeline with pipeline.save_pipeline()\")\n",
        "            print(f\"   3. Load with WalmartFeaturePipeline.load_pipeline()\")\n",
        "            print(f\"   4. Transform new data with pipeline.transform()\")\n",
        "            print(f\"   5. Keep IsHoliday column separate for WMAE calculation\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error in pipeline validation: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        raise\n",
        "\n",
        "    return final_metrics\n",
        "\n",
        "def validate_complete_pipeline_with_xgboost(X_train, X_val, y_train, y_val, train_is_holiday, val_is_holiday):\n",
        "    \"\"\"Complete pipeline validation using XGBoost with comprehensive evaluation\"\"\"\n",
        "    with mlflow.start_run(run_name=\"XGBoost_Pipeline_Validation\", nested=True) as run:\n",
        "        print(\"🤖 Validating complete pipeline with XGBoost...\")\n",
        "\n",
        "        # XGBoost parameters optimized for Walmart sales\n",
        "        xgb_params = {\n",
        "            'n_estimators': 500,\n",
        "            'max_depth': 8,\n",
        "            'learning_rate': 0.1,\n",
        "            'subsample': 0.85,\n",
        "            'colsample_bytree': 0.85,\n",
        "            'reg_alpha': 1.0,\n",
        "            'reg_lambda': 1.5,\n",
        "            'random_state': 42,\n",
        "            'n_jobs': -1,\n",
        "            'objective': 'reg:squarederror'\n",
        "        }\n",
        "\n",
        "        # Train XGBoost model\n",
        "        print(\"   🏋️ Training XGBoost model...\")\n",
        "        model = xgb.XGBRegressor(**xgb_params)\n",
        "        model.fit(X_train, y_train, verbose=False)\n",
        "\n",
        "        # Make predictions\n",
        "        print(\"   🔮 Making predictions...\")\n",
        "        train_pred = model.predict(X_train)\n",
        "        val_pred = model.predict(X_val)\n",
        "\n",
        "        # Comprehensive evaluation\n",
        "        print(\"   📊 Evaluating performance...\")\n",
        "        metrics = evaluate_model_performance(\n",
        "            y_train, train_pred, train_is_holiday,\n",
        "            y_val, val_pred, val_is_holiday\n",
        "        )\n",
        "\n",
        "        # Feature importance analysis\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_train.columns,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Display comprehensive results\n",
        "        print(f\"\\n\" + \"=\"*60)\n",
        "        print(f\"🎯 COMPLETE PIPELINE VALIDATION RESULTS\")\n",
        "        print(f\"=\"*60)\n",
        "\n",
        "        print(f\"\\n🚂 TRAINING PERFORMANCE:\")\n",
        "        print(f\"   WMAE: ${metrics['train_wmae']:,.2f}\")\n",
        "        print(f\"   MAE:  ${metrics['train_mae']:,.2f}\")\n",
        "        print(f\"   R²:   {metrics['train_r2']:.4f}\")\n",
        "\n",
        "        print(f\"\\n🔮 VALIDATION PERFORMANCE:\")\n",
        "        print(f\"   WMAE: ${metrics['val_wmae']:,.2f}\")\n",
        "        print(f\"   MAE:  ${metrics['val_mae']:,.2f}\")\n",
        "        print(f\"   R²:   {metrics['val_r2']:.4f}\")\n",
        "\n",
        "        print(f\"\\n🎯 TOP 15 MOST IMPORTANT FEATURES:\")\n",
        "        for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
        "            print(f\"   {i+1:2d}. {row['feature']:30s}: {row['importance']:.4f}\")\n",
        "\n",
        "        # Performance assessment\n",
        "        if metrics['val_wmae'] < 5000:\n",
        "            performance = \"🟢 EXCELLENT\"\n",
        "        elif metrics['val_wmae'] < 10000:\n",
        "            performance = \"🟡 GOOD\"\n",
        "        else:\n",
        "            performance = \"🔴 NEEDS IMPROVEMENT\"\n",
        "\n",
        "        print(f\"\\n📈 PIPELINE ASSESSMENT: {performance}\")\n",
        "        print(f\"   Validation WMAE: ${metrics['val_wmae']:,.2f}\")\n",
        "\n",
        "        # Log all metrics and parameters\n",
        "        for metric_name, value in metrics.items():\n",
        "            mlflow.log_metric(metric_name, value)\n",
        "\n",
        "        for param_name, value in xgb_params.items():\n",
        "            mlflow.log_param(f\"xgb_{param_name}\", value)\n",
        "\n",
        "        mlflow.log_param(\"model_type\", \"XGBRegressor\")\n",
        "        mlflow.log_param(\"validation_type\", \"complete_pipeline\")\n",
        "\n",
        "        # Log feature importance\n",
        "        mlflow.log_metric(\"top_feature_importance\", feature_importance.iloc[0]['importance'])\n",
        "        mlflow.log_param(\"most_important_feature\", feature_importance.iloc[0]['feature'])\n",
        "\n",
        "        # Log model artifact\n",
        "        mlflow.sklearn.log_model(\n",
        "            model,\n",
        "            \"xgboost_model\",\n",
        "            registered_model_name=\"Walmart_Sales_XGBoost_Pipeline\"\n",
        "        )\n",
        "\n",
        "        # Log feature importance as artifact\n",
        "        feature_importance_path = \"feature_importance.csv\"\n",
        "        feature_importance.to_csv(feature_importance_path, index=False)\n",
        "        mlflow.log_artifact(feature_importance_path)\n",
        "\n",
        "        # Log additional model metrics\n",
        "        mlflow.log_metric(\"num_features\", len(X_train.columns))\n",
        "        mlflow.log_metric(\"train_val_wmae_ratio\", metrics['train_wmae'] / metrics['val_wmae'])\n",
        "        mlflow.log_metric(\"train_samples\", len(X_train))\n",
        "        mlflow.log_metric(\"val_samples\", len(X_val))\n",
        "\n",
        "        print(f\"\\n✅ Complete pipeline validation finished!\")\n",
        "        print(f\"📊 All metrics logged to MLflow\")\n",
        "        print(f\"🤖 Model saved as artifact\")\n",
        "        print(f\"📈 Feature importance saved as CSV\")\n",
        "        print(f\"=\"*60)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    results = main()\n"
      ],
      "metadata": {
        "id": "kx_JJUvJvg3V",
        "outputId": "5fb24154-a05e-4e6f-a4bb-a2c1e07c6bca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 EXPERIMENT 5: Complete Feature Engineering Pipeline\n",
            "🎯 GOAL: Validate full pipeline functionality with XGBoost\n",
            "================================================================================\n",
            "🔧 Setting up MLflow and DagsHub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ DagsHub initialized successfully!\n",
            "✅ Created new experiment: Experiment_5_Feature_Engineering_Pipeline_20250703_102701\n",
            "✅ MLflow setup complete!\n",
            "🔗 Tracking URI: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\n",
            "📊 Experiment: Experiment_5_Feature_Engineering_Pipeline_20250703_102701\n",
            "📋 PIPELINE VALIDATION STEPS:\n",
            "   1. 📂 Load and prepare data\n",
            "   2. ✂️ Create temporal split (no data leakage)\n",
            "   3. 🔧 Fit pipeline on training data (with outlier removal)\n",
            "   4. 🔄 Transform validation data\n",
            "   5. 🤖 Validate with XGBoost model\n",
            "   6. 📊 Evaluate WMAE and feature importance\n",
            "================================================================================\n",
            "\n",
            "📂 STEP 1: Loading and preparing data...\n",
            "📂 Loading datasets...\n",
            "✅ Loaded datasets:\n",
            "   📊 train.csv: (421570, 5)\n",
            "   🏪 stores.csv: (45, 3)\n",
            "   📅 features.csv: (8190, 12)\n",
            "📊 Merged dataset shape: (421570, 16)\n",
            "🎄 Holiday weeks in dataset: 29661\n",
            "🗑️ Removed markdown columns: 5\n",
            "📊 Final dataset: (421570, 11)\n",
            "📅 Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "🏪 Stores: 45\n",
            "🏷️ Departments: 81\n",
            "✅ Data loading completed!\n",
            "🏃 View run Data_Loading_Complete at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14/runs/8edd997a52da4918a6a3a8aeecd73cc9\n",
            "🧪 View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14\n",
            "\n",
            "✂️ STEP 2: Creating temporal split...\n",
            "✂️ Creating temporal split...\n",
            "📅 Split date: 2012-04-13 00:00:00\n",
            "🚂 Training: 335,761 records (79.6%)\n",
            "🔮 Validation: 85,809 records (20.4%)\n",
            "📊 Training date range: 2010-02-05 00:00:00 to 2012-04-06 00:00:00\n",
            "📊 Validation date range: 2012-04-13 00:00:00 to 2012-10-26 00:00:00\n",
            "✅ Temporal split completed - no data leakage!\n",
            "🏃 View run Temporal_Split_80_20 at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14/runs/7ac9eff49d254aba89df6d1398c69eca\n",
            "🧪 View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14\n",
            "\n",
            "🔧 STEP 3: Fitting pipeline on training data...\n",
            "🔧 Fitting pipeline...\n",
            "🗑️ Removing outliers...\n",
            "🗑️ Removing outliers...\n",
            "🗑️ Removed 15,467 outliers (4.6%)\n",
            "🗑️ Removed 15,467 outliers (4.6%)\n",
            "🔧 Fitting and transforming data...\n",
            "🔧 Fitting and transforming data...\n",
            "✅ Feature engineering completed!\n",
            "📊 Total features created: 37\n",
            "   📅 Date features: 16\n",
            "   ⏰ Lag features: 4\n",
            "   📊 Rolling features: 15\n",
            "   🏪 Store/Dept features: 2\n",
            "✅ Feature engineering completed!\n",
            "📊 Total features created: 44\n",
            "   📅 Date features: 34\n",
            "   ⏰ Lag features: 4\n",
            "   📊 Rolling features: 15\n",
            "   🏪 Store/Dept features: 4\n",
            "✅ Pipeline fitted on 320,294 samples\n",
            "📊 Features available: 48\n",
            "🏃 View run Pipeline_Fit at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14/runs/5455ec2a0c874d758e2db4aa1808ab8a\n",
            "🧪 View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14\n",
            "\n",
            "🔄 STEP 4: Transforming validation data...\n",
            "🔄 Transforming new data...\n",
            "🔄 Transforming data...\n",
            "✅ Data transformed!\n",
            "✅ Transformed 85,809 samples\n",
            "\n",
            "📊 PIPELINE SUMMARY:\n",
            "   🚂 Training data: (320294, 44)\n",
            "   🔮 Validation data: (85809, 44)\n",
            "   📈 Features created: 44\n",
            "   📅 Split date: 2012-04-13 00:00:00\n",
            "   🎄 Training holiday weeks: 23991\n",
            "   🎄 Validation holiday weeks: 2966\n",
            "\n",
            "🔍 DATA CONSISTENCY CHECKS:\n",
            "   ✅ Train X shape matches y: True\n",
            "   ✅ Train IsHoliday matches: True\n",
            "   ✅ Val X shape matches y: True\n",
            "   ✅ Val IsHoliday matches: True\n",
            "\n",
            "🤖 STEP 5-6: Final XGBoost validation...\n",
            "🤖 Validating complete pipeline with XGBoost...\n",
            "   🏋️ Training XGBoost model...\n",
            "   🔮 Making predictions...\n",
            "   📊 Evaluating performance...\n",
            "\n",
            "============================================================\n",
            "🎯 COMPLETE PIPELINE VALIDATION RESULTS\n",
            "============================================================\n",
            "\n",
            "🚂 TRAINING PERFORMANCE:\n",
            "   WMAE: $72.59\n",
            "   MAE:  $72.65\n",
            "   R²:   1.0000\n",
            "\n",
            "🔮 VALIDATION PERFORMANCE:\n",
            "   WMAE: $134.98\n",
            "   MAE:  $132.71\n",
            "   R²:   0.9992\n",
            "\n",
            "🎯 TOP 15 MOST IMPORTANT FEATURES:\n",
            "    1. Weekly_Sales_rolling_mean_4   : 0.7579\n",
            "    2. Weekly_Sales_rolling_mean_8   : 0.1273\n",
            "    3. Weekly_Sales_rolling_min_4    : 0.0797\n",
            "    4. Weekly_Sales_rolling_max_4    : 0.0176\n",
            "    5. Weekly_Sales_rolling_mean_12  : 0.0068\n",
            "    6. Weekly_Sales_vs_rolling_mean_4: 0.0030\n",
            "    7. Weekly_Sales_rolling_min_12   : 0.0012\n",
            "    8. Weekly_Sales_rolling_max_8    : 0.0011\n",
            "    9. Weekly_Sales_vs_rolling_mean_8: 0.0011\n",
            "   10. Weekly_Sales_rolling_max_12   : 0.0006\n",
            "   11. Weekly_Sales_rolling_min_8    : 0.0005\n",
            "   12. Weekly_Sales_lag_1            : 0.0005\n",
            "   13. Weekly_Sales_vs_rolling_mean_12: 0.0004\n",
            "   14. Weekly_Sales_rolling_std_12   : 0.0004\n",
            "   15. Weekly_Sales_lag_4            : 0.0004\n",
            "\n",
            "📈 PIPELINE ASSESSMENT: 🟢 EXCELLENT\n",
            "   Validation WMAE: $134.98\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/03 10:30:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run XGBoost_Pipeline_Validation at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14/runs/d1e84ea2b1784c748e266081ce44581b\n",
            "🧪 View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14\n",
            "🏃 View run Complete_Pipeline_Validation at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14/runs/0a13de54f09d4c12b728c09cd14b6330\n",
            "🧪 View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/14\n",
            "❌ Error in pipeline validation: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-64-2955994909.py\", line 789, in main\n",
            "    final_metrics = validate_complete_pipeline_with_xgboost(\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-64-2955994909.py\", line 916, in validate_complete_pipeline_with_xgboost\n",
            "    mlflow.sklearn.log_model(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/sklearn/__init__.py\", line 426, in log_model\n",
            "    return Model.log(\n",
            "           ^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\", line 1161, in log\n",
            "    model = mlflow.initialize_logged_model(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\", line 2130, in initialize_logged_model\n",
            "    model = _create_logged_model(\n",
            "            ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\", line 2257, in _create_logged_model\n",
            "    return MlflowClient().create_logged_model(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\", line 5371, in create_logged_model\n",
            "    return self._tracking_client.create_logged_model(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\", line 824, in create_logged_model\n",
            "    return self.store.create_logged_model(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\", line 936, in create_logged_model\n",
            "    response_proto = self._call_endpoint(CreateLoggedModel, req_body)\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\", line 135, in _call_endpoint\n",
            "    return call_endpoint(\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\", line 590, in call_endpoint\n",
            "    response = verify_rest_response(response, endpoint)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\", line 304, in verify_rest_response\n",
            "    raise RestException(json.loads(response.text))\n",
            "mlflow.exceptions.RestException: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RestException",
          "evalue": "INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRestException\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-64-2955994909.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-64-2955994909.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    787\u001b[0m             \u001b[0;31m# Step 5 & 6: Validate with XGBoost and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\n🤖 STEP 5-6: Final XGBoost validation...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             final_metrics = validate_complete_pipeline_with_xgboost(\n\u001b[0m\u001b[1;32m    790\u001b[0m                 \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_is_holiday\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_is_holiday\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             )\n",
            "\u001b[0;32m/tmp/ipython-input-64-2955994909.py\u001b[0m in \u001b[0;36mvalidate_complete_pipeline_with_xgboost\u001b[0;34m(X_train, X_val, y_train, y_val, train_is_holiday, val_is_holiday)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;31m# Log model artifact\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m         mlflow.sklearn.log_model(\n\u001b[0m\u001b[1;32m    917\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m             \u001b[0;34m\"xgboost_model\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/sklearn/__init__.py\u001b[0m in \u001b[0;36mlog_model\u001b[0;34m(sk_model, artifact_path, conda_env, code_paths, serialization_format, registered_model_name, signature, input_example, await_registration_for, pip_requirements, extra_pip_requirements, pyfunc_predict_fn, metadata, params, tags, model_type, step, model_id, name)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \"\"\"\n\u001b[0;32m--> 426\u001b[0;31m     return Model.log(\n\u001b[0m\u001b[1;32m    427\u001b[0m         \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0martifact_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/models/model.py\u001b[0m in \u001b[0;36mlog\u001b[0;34m(cls, artifact_path, flavor, registered_model_name, await_registration_for, metadata, run_id, resources, auth_policy, prompts, name, model_type, params, tags, step, model_id, **kwargs)\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_id\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m                 }\n\u001b[0;32m-> 1161\u001b[0;31m                 model = mlflow.initialize_logged_model(\n\u001b[0m\u001b[1;32m   1162\u001b[0m                     \u001b[0;31m# TODO: Update model name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36minitialize_logged_model\u001b[0;34m(name, source_run_id, tags, params, model_type, experiment_id)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mnew\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoggedModel\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPENDING\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m     \"\"\"\n\u001b[0;32m-> 2130\u001b[0;31m     model = _create_logged_model(\n\u001b[0m\u001b[1;32m   2131\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m         \u001b[0msource_run_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_run_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36m_create_logged_model\u001b[0;34m(name, source_run_id, tags, params, model_type, experiment_id)\u001b[0m\n\u001b[1;32m   2255\u001b[0m         )\n\u001b[1;32m   2256\u001b[0m     \u001b[0mresolved_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext_registry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresolve_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2257\u001b[0;31m     return MlflowClient().create_logged_model(\n\u001b[0m\u001b[1;32m   2258\u001b[0m         \u001b[0mexperiment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/client.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m   5369\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5370\u001b[0m         \"\"\"\n\u001b[0;32m-> 5371\u001b[0;31m         return self._tracking_client.create_logged_model(\n\u001b[0m\u001b[1;32m   5372\u001b[0m             \u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_run_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5373\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m    822\u001b[0m         \u001b[0mmodel_type\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     ) -> LoggedModel:\n\u001b[0;32m--> 824\u001b[0;31m         return self.store.create_logged_model(\n\u001b[0m\u001b[1;32m    825\u001b[0m             \u001b[0mexperiment_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mcreate_logged_model\u001b[0;34m(self, experiment_id, name, source_run_id, tags, params, model_type)\u001b[0m\n\u001b[1;32m    934\u001b[0m         )\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCreateLoggedModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLoggedModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_METHOD_TO_INFO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         return call_endpoint(\n\u001b[0m\u001b[1;32m    136\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mcall_endpoint\u001b[0;34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcall_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mverify_rest_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m     \u001b[0mresponse_to_parse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/mlflow/utils/rest_utils.py\u001b[0m in \u001b[0;36mverify_rest_response\u001b[0;34m(response, endpoint)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_can_parse_as_json_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRestException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             base_msg = (\n",
            "\u001b[0;31mRestException\u001b[0m: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}