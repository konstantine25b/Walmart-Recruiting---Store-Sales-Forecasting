{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/experiment_7_xgboost_Markdowns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eb7WJ3HLY0K8",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# experiment_7_xgboost.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MMZqRTtCY31E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfc18f7c-0b64-47c1-c430-3a3f9b43c64e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "foFWtundZCTx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3bc2e4d-622f-4afc-b815-076fdd01de95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‚Äò/root/.kaggle‚Äô: File exists\n"
          ]
        }
      ],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "CqdtFSAaZDlt"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Oyi1Zq8cZFIW"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TWwWkCerZGX1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23de0fd-57f5-473c-af56-06e096d4affe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrKcnvgKZH_b",
        "outputId": "57154f88-1392-47da-892f-4e1cb99630e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "replace features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qBXkuKkCZJvL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "08iOjzo_ZYhR"
      },
      "outputs": [],
      "source": [
        "!pip install prophet plotly mlflow dagshub xgboost -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4mT-I9HzZZRg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44bdbfa3-9fa4-4939-dbad-9b2888bef057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "kDwu85PQZ9Xw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "import dagshub\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3u8gZpJdelcN"
      },
      "outputs": [],
      "source": [
        "class WalmartPreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline for Walmart sales data.\n",
        "    Supports fit/transform pattern for proper train/validation handling.\n",
        "    Updated to allow control over MarkDown feature inclusion.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, remove_outliers=True, remove_markdowns=False, enable_lag_features=False):\n",
        "        # Configuration parameters\n",
        "        self.remove_outliers = remove_outliers\n",
        "        # Flag to control MarkDown removal (False means keep MarkDowns)\n",
        "        self.remove_markdowns = remove_markdowns\n",
        "        self.enable_lag_features = enable_lag_features\n",
        "\n",
        "        # Internal state variables\n",
        "        self.fitted = False\n",
        "        self.outlier_thresholds = None\n",
        "        self.feature_columns = None\n",
        "        self.train_data_for_lags = None\n",
        "\n",
        "        # Define outlier thresholds for Weekly_Sales based on store type\n",
        "        self.weekly_sales_outlier_thresholds = {\n",
        "            'A': {'lower': -1000, 'upper': 50000},  # Type A stores\n",
        "            'B': {'lower': -500, 'upper': 25000},   # Type B stores\n",
        "            'C': {'lower': -200, 'upper': 15000}    # Type C stores\n",
        "        }\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load and merge train.csv, stores.csv, features.csv datasets\"\"\"\n",
        "        print(\"üìä Loading datasets...\")\n",
        "\n",
        "        # Assuming data files are available in the environment\n",
        "        try:\n",
        "            train_df = pd.read_csv('train.csv')\n",
        "            stores_df = pd.read_csv('stores.csv')\n",
        "            features_df = pd.read_csv('features.csv')\n",
        "        except FileNotFoundError as e:\n",
        "            print(f\"Error loading data files: {e}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"   üìà Train data: {train_df.shape}\")\n",
        "        print(f\"   üè™ Stores data: {stores_df.shape}\")\n",
        "        print(f\"   üéØ Features data: {features_df.shape}\")\n",
        "\n",
        "        # Convert Date column to datetime\n",
        "        train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "        features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "        # Merge datasets\n",
        "        train_stores = train_df.merge(stores_df, on='Store', how='left')\n",
        "        train_full = train_stores.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        print(f\"   ‚úÖ Merged data: {train_full.shape}\")\n",
        "        print(f\"   üìÖ Date range: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n",
        "\n",
        "        return train_full\n",
        "\n",
        "    def clean_merged_data(self, train_full):\n",
        "        \"\"\"Clean merged data by handling duplicate IsHoliday columns\"\"\"\n",
        "        print(\"üßπ Cleaning merged data...\")\n",
        "\n",
        "        initial_shape = train_full.shape\n",
        "\n",
        "        # Handle duplicate IsHoliday columns if they exist\n",
        "        if 'IsHoliday_x' in train_full.columns and 'IsHoliday_y' in train_full.columns:\n",
        "            print(\"   üîÑ Resolving duplicate IsHoliday columns...\")\n",
        "            # Combine the boolean holidays using OR logic\n",
        "            train_full['IsHoliday'] = train_full['IsHoliday_x'] | train_full['IsHoliday_y']\n",
        "            train_full = train_full.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "\n",
        "        print(f\"   ‚úÖ Cleaned data: {train_full.shape} (was {initial_shape})\")\n",
        "        return train_full\n",
        "\n",
        "    def create_temporal_split(self, df, train_ratio=0.8):\n",
        "        \"\"\"Create temporal split to prevent data leakage\"\"\"\n",
        "        print(f\"üìÖ Creating temporal split ({int(train_ratio*100)}/{int((1-train_ratio)*100)})...\")\n",
        "\n",
        "        # Sort by date to ensure temporal order\n",
        "        df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Find split point\n",
        "        split_idx = int(len(df_sorted) * train_ratio)\n",
        "        split_date = df_sorted.iloc[split_idx]['Date']\n",
        "\n",
        "        # Create splits\n",
        "        train_data = df_sorted.iloc[:split_idx].copy()\n",
        "        val_data = df_sorted.iloc[split_idx:].copy()\n",
        "\n",
        "        # Create split info dictionary\n",
        "        split_info = {\n",
        "            'split_date': split_date,\n",
        "            'train_size': len(train_data),\n",
        "            'val_size': len(val_data),\n",
        "            'train_date_range': (train_data['Date'].min(), train_data['Date'].max()),\n",
        "            'val_date_range': (val_data['Date'].min(), val_data['Date'].max())\n",
        "        }\n",
        "\n",
        "        print(f\"   üìä Split date: {split_date}\")\n",
        "        print(f\"   üìà Train: {len(train_data):,} records ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
        "        print(f\"   üìâ Val: {len(val_data):,} records ({val_data['Date'].min()} to {val_data['Date'].max()})\")\n",
        "\n",
        "        return train_data, val_data, split_info\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"Fit the preprocessing pipeline on training data\"\"\"\n",
        "        print(\"üîß Fitting preprocessing pipeline on training data...\")\n",
        "\n",
        "        # Store training data for lag feature creation\n",
        "        self.train_data_for_lags = train_data.copy()\n",
        "\n",
        "        # Fit outlier removal thresholds on training data only\n",
        "        # We use the predefined thresholds from __init__\n",
        "        self.outlier_thresholds = self.weekly_sales_outlier_thresholds\n",
        "\n",
        "        print(\"‚úÖ Pipeline fitted on training data\")\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, data, is_validation=False):\n",
        "        \"\"\"Transform data using fitted pipeline\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before transform!\")\n",
        "\n",
        "        print(f\"üîÑ Transforming {'validation' if is_validation else 'training'} data...\")\n",
        "\n",
        "        df = data.copy()\n",
        "\n",
        "        # Step 1: Create date features\n",
        "        df = self._create_date_features(df)\n",
        "\n",
        "        # Step 2: Create holiday features\n",
        "        df = self._create_holiday_features(df)\n",
        "\n",
        "        # Step 3: Handle missing values (Crucial for MarkDowns if used)\n",
        "        df = self._handle_missing_values(df)\n",
        "\n",
        "        # Step 4: Encode categorical features (BEFORE outlier removal!)\n",
        "        df = self._encode_categorical_features(df)\n",
        "\n",
        "        # Step 5: Create lag features (different for train vs validation, conditional)\n",
        "        if self.enable_lag_features:\n",
        "            if is_validation:\n",
        "                df = self._create_lag_features_validation(df)\n",
        "            else:\n",
        "                df = self._create_lag_features_training(df)\n",
        "\n",
        "        # Step 6: Remove outliers (only on training data, conditional on self.remove_outliers)\n",
        "        if not is_validation and self.remove_outliers:\n",
        "            df = self._remove_outliers(df)\n",
        "\n",
        "        # Step 7: Remove markdown features (conditional on self.remove_markdowns)\n",
        "        # If self.remove_markdowns is False, MarkDown features will be kept.\n",
        "        if self.remove_markdowns:\n",
        "            df = self._remove_markdown_features(df)\n",
        "        else:\n",
        "            print(\"   üìÅ Keeping MarkDown features.\")\n",
        "\n",
        "        # Step 8: Remove redundant features\n",
        "        df = self._remove_redundant_features(df)\n",
        "\n",
        "        print(f\"‚úÖ Transform complete. Shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, train_data):\n",
        "        \"\"\"Fit and transform training data in one step\"\"\"\n",
        "        return self.fit(train_data).transform(train_data, is_validation=False)\n",
        "\n",
        "    def _create_date_features(self, df):\n",
        "        \"\"\"Create date features\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "        df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "        df['IsQuarterStart'] = df['Date'].dt.is_quarter_start.astype(int)\n",
        "        df['IsQuarterEnd'] = df['Date'].dt.is_quarter_end.astype(int)\n",
        "        start_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - start_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        return df\n",
        "\n",
        "    def _create_holiday_features(self, df):\n",
        "        \"\"\"Create holiday features\"\"\"\n",
        "        df = df.copy()\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features_training(self, df):\n",
        "        \"\"\"Create lag features for training data - DISABLED by default\"\"\"\n",
        "        # Lag features removed to prevent overfitting\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features_validation(self, df):\n",
        "        \"\"\"Create lag features for validation data - DISABLED by default\"\"\"\n",
        "        # Lag features removed to prevent overfitting\n",
        "        return df\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        \"\"\"\n",
        "        Fills missing numerical values, particularly for MarkDown columns, with 0.\n",
        "        MarkDown values are NaN when a markdown is not active.\n",
        "        \"\"\"\n",
        "        # Identify MarkDown columns\n",
        "        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n",
        "\n",
        "        # Identify other numerical columns that might have NaNs (e.g., CPI, Unemployment)\n",
        "        # For simplicity and correctness with MarkDowns, we fill MarkDowns with 0.\n",
        "\n",
        "        # Fill NaNs in MarkDown columns with 0\n",
        "        if markdown_cols:\n",
        "            df[markdown_cols] = df[markdown_cols].fillna(0)\n",
        "            print(\"   ‚úÖ Handled missing values (filled MarkDown NaNs with 0).\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df):\n",
        "        \"\"\"Remove outliers from training data only\"\"\"\n",
        "        initial_len = len(df)\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        for store_type, thresholds in self.outlier_thresholds.items():\n",
        "            # Check if the one-hot encoded column exists (it should, after _encode_categorical_features)\n",
        "            if f'Type_{store_type}' in df_clean.columns:\n",
        "                type_mask = df_clean[f'Type_{store_type}'] == 1\n",
        "                outlier_mask = (\n",
        "                    (df_clean['Weekly_Sales'] < thresholds['lower']) |\n",
        "                    (df_clean['Weekly_Sales'] > thresholds['upper'])\n",
        "                )\n",
        "                # Remove rows that match both the store type AND the outlier condition\n",
        "                df_clean = df_clean[~(type_mask & outlier_mask)]\n",
        "\n",
        "        removed = initial_len - len(df_clean)\n",
        "        print(f\"   üóëÔ∏è Removed {removed:,} outliers from training data\")\n",
        "        return df_clean\n",
        "\n",
        "    def _remove_markdown_features(self, df):\n",
        "        \"\"\"Remove markdown columns (called only if self.remove_markdowns is True)\"\"\"\n",
        "        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n",
        "        if markdown_cols:\n",
        "            df = df.drop(markdown_cols, axis=1)\n",
        "        return df\n",
        "\n",
        "    def _remove_redundant_features(self, df):\n",
        "        \"\"\"Remove redundant features\"\"\"\n",
        "        redundant_cols = ['Year', 'Quarter', 'Day', 'WeekOfYear', 'DaysFromStart',\n",
        "                         'IsQuarterStart', 'IsQuarterEnd']\n",
        "        existing_redundant = [col for col in redundant_cols if col in df.columns]\n",
        "        if existing_redundant:\n",
        "            df = df.drop(existing_redundant, axis=1)\n",
        "        return df\n",
        "\n",
        "    def _remove_id_columns(self, df):\n",
        "        \"\"\"Remove high-cardinality ID columns that cause overfitting\"\"\"\n",
        "        id_cols = ['Store', 'Dept']\n",
        "        existing_id_cols = [col for col in id_cols if col in df.columns]\n",
        "        if existing_id_cols:\n",
        "            print(f\"   üóëÔ∏è Removing ID columns to prevent overfitting: {existing_id_cols}\")\n",
        "            df = df.drop(existing_id_cols, axis=1)\n",
        "        return df\n",
        "\n",
        "    def _encode_categorical_features(self, df):\n",
        "        \"\"\"Encode categorical features using both one-hot and label encoding\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        if 'Type' in df.columns:\n",
        "            print(f\"   üîß Encoding Type column using both one-hot and label encoding...\")\n",
        "\n",
        "            # One-hot encoding\n",
        "            type_dummies = pd.get_dummies(df['Type'], prefix='Type', dtype=int)\n",
        "\n",
        "            # Label encoding (A=0, B=1, C=2)\n",
        "            type_mapping = {'A': 0, 'B': 1, 'C': 2}\n",
        "            df['Type_Encoded'] = df['Type'].map(type_mapping)\n",
        "\n",
        "            # Add one-hot columns\n",
        "            for col in type_dummies.columns:\n",
        "                df[col] = type_dummies[col]\n",
        "\n",
        "            # Remove original Type column\n",
        "            df = df.drop('Type', axis=1)\n",
        "\n",
        "            print(f\"   ‚úÖ Added both Type_Encoded and {list(type_dummies.columns)}\")\n",
        "\n",
        "        return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RnvlQNRoemSJ"
      },
      "outputs": [],
      "source": [
        "def setup_mlflow():\n",
        "    \"\"\"Setup MLflow and DagsHub tracking\"\"\"\n",
        "    print(\"üîß Setting up MLflow and DagsHub...\")\n",
        "\n",
        "    # End any active runs first\n",
        "    try:\n",
        "        mlflow.end_run()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize DagsHub\n",
        "    try:\n",
        "        dagshub.init(\n",
        "            repo_owner='konstantine25b',\n",
        "            repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "            mlflow=True\n",
        "        )\n",
        "        print(\"‚úÖ DagsHub initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è DagsHub init warning: {e}\")\n",
        "\n",
        "    # Set MLflow tracking URI\n",
        "    mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "\n",
        "    # Create unique experiment name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_name = f\"Experiment_7_XGBoost_{timestamp}\"\n",
        "\n",
        "    try:\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"‚úÖ Created new experiment: {experiment_name}\")\n",
        "    except mlflow.exceptions.MlflowException as e:\n",
        "        if \"already exists\" in str(e):\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"‚úÖ Using existing experiment: {experiment_name}\")\n",
        "        else:\n",
        "            # Fallback to default experiment\n",
        "            experiment_name = \"Default\"\n",
        "            mlflow.set_experiment(experiment_name)\n",
        "            print(f\"‚ö†Ô∏è Using default experiment due to: {e}\")\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    print(f\"‚úÖ MLflow setup complete!\")\n",
        "    print(f\"üîó Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    print(f\"üìä Experiment: {experiment_name}\")\n",
        "\n",
        "    return experiment_name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ZULbGpMMeod8"
      },
      "outputs": [],
      "source": [
        "def get_preprocessed_data():\n",
        "    \"\"\"\n",
        "    Use preprocessing pipeline to get model-ready data,\n",
        "    configured to include MarkDown features.\n",
        "\n",
        "    Returns:\n",
        "        X_train, y_train, X_val, y_val: Model-ready datasets\n",
        "        train_holidays, val_holidays: Holiday indicators for WMAE\n",
        "        split_info: Information about the temporal split\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Getting preprocessed data using pipeline...\")\n",
        "\n",
        "    # Create the preprocessing pipeline, setting remove_markdowns=False to ensure they are used\n",
        "    pipeline = WalmartPreprocessingPipeline(remove_markdowns=False)\n",
        "\n",
        "    # Load raw data\n",
        "    train_full = pipeline.load_and_prepare_data()\n",
        "    if train_full is None:\n",
        "        # Handle case where data loading failed (e.g., files missing)\n",
        "        return None, None, None, None, None, None, None\n",
        "\n",
        "    train_full = pipeline.clean_merged_data(train_full)\n",
        "\n",
        "    # Create temporal split\n",
        "    train_data, val_data, split_info = pipeline.create_temporal_split(train_full)\n",
        "\n",
        "    # Extract holiday information before preprocessing\n",
        "    val_holidays = val_data['IsHoliday'].values.astype(bool)\n",
        "\n",
        "    # Separate validation target (realistic test scenario)\n",
        "    y_val = val_data['Weekly_Sales'].copy()\n",
        "    val_data_no_target = val_data.drop('Weekly_Sales', axis=1).copy()\n",
        "\n",
        "    # Fit and transform data using pipeline\n",
        "    pipeline.fit(train_data)\n",
        "    train_processed = pipeline.transform(train_data, is_validation=False)\n",
        "    val_processed = pipeline.transform(val_data_no_target, is_validation=True)\n",
        "\n",
        "    # Extract holiday information AFTER training data processing (after outlier removal)\n",
        "    train_holidays = train_processed['IsHoliday'].values.astype(bool)\n",
        "\n",
        "    # Prepare model data\n",
        "    X_train = train_processed.drop(['Weekly_Sales', 'Date'], axis=1)\n",
        "    y_train = train_processed['Weekly_Sales']\n",
        "    X_val = val_processed.drop('Date', axis=1)\n",
        "\n",
        "    # Verify that MarkDown columns exist in the final datasets\n",
        "    markdown_cols_train = [col for col in X_train.columns if 'MarkDown' in col]\n",
        "    markdown_cols_val = [col for col in X_val.columns if 'MarkDown' in col]\n",
        "\n",
        "    print(\"\\nMarkDown columns in X_train:\", markdown_cols_train)\n",
        "    print(\"MarkDown columns in X_val:\", markdown_cols_val)\n",
        "    print(f\"‚úÖ Data ready:\")\n",
        "    print(f\"   X_train: {X_train.shape}\")\n",
        "    print(f\"   y_train: {y_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}\")\n",
        "    print(f\"   y_val: {y_val.shape}\")\n",
        "    print(f\"   train_holidays: {train_holidays.shape} ({train_holidays.sum()} holidays)\")\n",
        "    print(f\"   val_holidays: {val_holidays.shape} ({val_holidays.sum()} holidays)\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, train_holidays, val_holidays, split_info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "CidX2q1het7-"
      },
      "outputs": [],
      "source": [
        "def calculate_metrics(y_true, y_pred, is_holiday=None):\n",
        "    \"\"\"Calculate evaluation metrics including WMAE with correct Walmart formula\"\"\"\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "    # Calculate WMAE (Weighted Mean Absolute Error) - Walmart competition formula\n",
        "    # w_i = 5 if holiday week, 1 otherwise\n",
        "    if is_holiday is not None:\n",
        "        weights = np.where(is_holiday, 5, 1)  # 5 for holidays, 1 for regular weeks\n",
        "    else:\n",
        "        weights = np.ones(len(y_true))  # Default to all 1s if no holiday info\n",
        "\n",
        "    wmae = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "\n",
        "    return {\n",
        "        'mae': mae,\n",
        "        'rmse': rmse,\n",
        "        'r2': r2,\n",
        "        'wmae': wmae\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "gzZDX6wfe9Us"
      },
      "outputs": [],
      "source": [
        "def log_results_to_mlflow(model, train_metrics, val_metrics, X_train, X_val, params, feature_list, feature_categories):\n",
        "    \"\"\"Log training results to MLflow after training is complete\"\"\"\n",
        "    print(\"\\nüìä Logging results to MLflow...\")\n",
        "\n",
        "    try:\n",
        "        # Setup MLflow\n",
        "        experiment_name = setup_mlflow()\n",
        "\n",
        "        with mlflow.start_run(run_name=\"XGBoost_Walmart_Sales\"):\n",
        "            # Log parameters\n",
        "            mlflow.log_params(params)\n",
        "\n",
        "            # Log metrics\n",
        "            for metric_name, value in train_metrics.items():\n",
        "                mlflow.log_metric(f\"train_{metric_name}\", value)\n",
        "\n",
        "            for metric_name, value in val_metrics.items():\n",
        "                mlflow.log_metric(f\"val_{metric_name}\", value)\n",
        "\n",
        "            # Log feature importance\n",
        "            if hasattr(model, 'feature_importances_'):\n",
        "                importance_df = pd.DataFrame({\n",
        "                    'feature': feature_list,\n",
        "                    'importance': model.feature_importances_\n",
        "                }).sort_values('importance', ascending=False)\n",
        "\n",
        "                # Log top 10 features\n",
        "                for i, row in importance_df.head(10).iterrows():\n",
        "                    mlflow.log_metric(f\"importance_{row['feature']}\", row['importance'])\n",
        "\n",
        "            # Log model\n",
        "            mlflow.xgboost.log_model(model, \"xgboost_model\")\n",
        "\n",
        "            # Log split info\n",
        "            split_info = {\n",
        "                'train_size': len(X_train),\n",
        "                'val_size': len(X_val),\n",
        "                'n_features': X_train.shape[1]\n",
        "            }\n",
        "            mlflow.log_params(split_info)\n",
        "\n",
        "            # Log feature categories\n",
        "            mlflow.log_params(feature_categories)\n",
        "\n",
        "            run_id = mlflow.active_run().info.run_id\n",
        "            print(f\"‚úÖ Results logged to MLflow run: {run_id}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è MLflow logging failed: {e}\")\n",
        "        print(\"   Training results are still valid, just not logged to MLflow\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Zyi4CIjjpVIj"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_xgboost_model(X_train, y_train, X_val, y_val, train_holidays=None, val_holidays=None):\n",
        "    \"\"\"Train XGBoost model with comprehensive feature logging\"\"\"\n",
        "    print(\"üöÄ Training XGBoost model...\")\n",
        "\n",
        "    # Log feature information\n",
        "    feature_list = list(X_train.columns)\n",
        "    print(f\"   üìã Total Features: {len(feature_list)}\")\n",
        "    print(f\"   üìã Feature List: {feature_list}\")\n",
        "\n",
        "    # Categorize features for better understanding\n",
        "    feature_categories = {\n",
        "        'ID_Features': [f for f in feature_list if f in ['Store', 'Dept']],\n",
        "        'Store_Info': [f for f in feature_list if f in ['Size']],\n",
        "        'Economic': [f for f in feature_list if f in ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']],\n",
        "        'Date_Features': [f for f in feature_list if f in ['Month', 'DayOfWeek', 'WeeksFromStart']],\n",
        "        'Holiday_Features': [f for f in feature_list if 'Holiday' in f or 'BackToSchool' in f],\n",
        "        'Type_OneHot': [f for f in feature_list if f.startswith('Type_') and f != 'Type_Encoded'],\n",
        "        'Type_Label': [f for f in feature_list if f == 'Type_Encoded'],\n",
        "        'Boolean_Features': [f for f in feature_list if f in ['IsWeekend', 'IsMonthStart', 'IsMonthEnd']],\n",
        "        'Other': [f for f in feature_list if f not in\n",
        "                 ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                  'Month', 'DayOfWeek', 'WeeksFromStart', 'IsWeekend', 'IsMonthStart', 'IsMonthEnd',\n",
        "                  'Type_A', 'Type_B', 'Type_C', 'Type_Encoded'] and\n",
        "                  'Holiday' not in f and 'BackToSchool' not in f]\n",
        "    }\n",
        "\n",
        "    print(f\"   üìä Feature Categories:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        if features:\n",
        "            print(f\"      {category}: {features}\")\n",
        "\n",
        "    # XGBoost parameters matching experiment_2's successful configuration\n",
        "    params = {\n",
        "        'n_estimators': 200,          # Same as experiment_2\n",
        "        'max_depth': 8,               # Same as experiment_2\n",
        "        'learning_rate': 0.05,        # Same as experiment_2\n",
        "        'subsample': 0.8,             # Same as experiment_2\n",
        "        'colsample_bytree': 0.8,      # Same as experiment_2\n",
        "        'colsample_bylevel': 0.8,     # Same as experiment_2\n",
        "        'min_child_weight': 3,        # Same as experiment_2\n",
        "        'gamma': 0.1,                 # Same as experiment_2\n",
        "        'reg_alpha': 0.1,             # Same as experiment_2\n",
        "        'reg_lambda': 1.0,            # Same as experiment_2\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1\n",
        "    }\n",
        "\n",
        "    print(f\"   üìã Parameters: {params}\")\n",
        "    print(f\"   üîÑ Training XGBoost with regularization...\")\n",
        "\n",
        "    # Train model\n",
        "    model = xgb.XGBRegressor(**params)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"   üìä Making predictions...\")\n",
        "\n",
        "    # Make predictions\n",
        "    train_pred = model.predict(X_train)\n",
        "    val_pred = model.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    train_metrics = calculate_metrics(y_train, train_pred, train_holidays)\n",
        "    val_metrics = calculate_metrics(y_val, val_pred, val_holidays)\n",
        "\n",
        "    # Feature importance analysis\n",
        "    feature_importance = model.feature_importances_\n",
        "    importance_df = pd.DataFrame({\n",
        "        'feature': feature_list,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    return {\n",
        "        'model': model,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "        'feature_importance': importance_df,\n",
        "        'params': params,\n",
        "        'feature_list': feature_list,\n",
        "        'feature_categories': feature_categories\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9dWIdZmGpZLC"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    \"\"\"Main experiment pipeline\"\"\"\n",
        "    print(\"üéØ EXPERIMENT 7: XGBoost with Preprocessing Pipeline\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Get preprocessed data\n",
        "    X_train, y_train, X_val, y_val, train_holidays, val_holidays, split_info = get_preprocessed_data()\n",
        "\n",
        "    # Train XGBoost model with holiday information for correct WMAE\n",
        "    model_info = train_xgboost_model(\n",
        "        X_train, y_train, X_val, y_val, train_holidays, val_holidays\n",
        "    )\n",
        "\n",
        "    # Display comprehensive results\n",
        "    print(f\"\\n‚úÖ XGBoost Training Complete!\")\n",
        "    print(f\"=\" * 50)\n",
        "    print(f\"üìä TRAINING METRICS:\")\n",
        "    print(f\"   WMAE: {model_info['train_metrics']['wmae']:.2f}\")\n",
        "    print(f\"   RMSE: {model_info['train_metrics']['rmse']:.2f}\")\n",
        "    print(f\"   MAE: {model_info['train_metrics']['mae']:.2f}\")\n",
        "    print(f\"   R¬≤: {model_info['train_metrics']['r2']:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìä VALIDATION METRICS:\")\n",
        "    print(f\"   WMAE: {model_info['val_metrics']['wmae']:.2f} ‚≠ê\")\n",
        "    print(f\"   RMSE: {model_info['val_metrics']['rmse']:.2f}\")\n",
        "    print(f\"   MAE: {model_info['val_metrics']['mae']:.2f}\")\n",
        "    print(f\"   R¬≤: {model_info['val_metrics']['r2']:.4f}\")\n",
        "\n",
        "    # Holiday weight analysis\n",
        "    if train_holidays is not None and val_holidays is not None:\n",
        "        train_holiday_pct = (train_holidays.sum() / len(train_holidays)) * 100\n",
        "        val_holiday_pct = (val_holidays.sum() / len(val_holidays)) * 100\n",
        "\n",
        "        print(f\"\\nüéÑ HOLIDAY ANALYSIS:\")\n",
        "        print(f\"   Training holiday weeks: {train_holiday_pct:.1f}%\")\n",
        "        print(f\"   Validation holiday weeks: {val_holiday_pct:.1f}%\")\n",
        "        print(f\"   Holiday weight multiplier: 5x\")\n",
        "\n",
        "    # Calculate overfitting metrics\n",
        "    wmae_ratio = model_info['val_metrics']['wmae'] / model_info['train_metrics']['wmae']\n",
        "    r2_diff = model_info['train_metrics']['r2'] - model_info['val_metrics']['r2']\n",
        "\n",
        "    print(f\"\\nüîç OVERFITTING ANALYSIS:\")\n",
        "    print(f\"   WMAE Ratio (val/train): {wmae_ratio:.2f}\")\n",
        "    print(f\"   R¬≤ Difference (train-val): {r2_diff:.4f}\")\n",
        "    if wmae_ratio > 2.0:\n",
        "        print(\"   ‚ö†Ô∏è High overfitting detected (WMAE ratio > 2.0)\")\n",
        "    elif wmae_ratio > 1.5:\n",
        "        print(\"   ‚ö†Ô∏è Moderate overfitting detected (WMAE ratio > 1.5)\")\n",
        "    else:\n",
        "        print(\"   ‚úÖ Reasonable generalization\")\n",
        "\n",
        "    # Feature importance\n",
        "    print(f\"\\nüîù TOP 10 MOST IMPORTANT FEATURES:\")\n",
        "    for i, (_, row) in enumerate(model_info['feature_importance'].head(10).iterrows()):\n",
        "        print(f\"   {i+1:2d}. {row['feature']:<25} {row['importance']:.4f}\")\n",
        "\n",
        "    print(f\"\\nüìà MODEL INFO:\")\n",
        "    print(f\"   Training samples: {len(X_train):,}\")\n",
        "    print(f\"   Validation samples: {len(X_val):,}\")\n",
        "    print(f\"   Features: {X_train.shape[1]}\")\n",
        "    print(f\"   Estimators: {model_info['params']['n_estimators']}\")\n",
        "\n",
        "    # Log results to MLflow after training is complete\n",
        "    log_results_to_mlflow(model_info['model'], model_info['train_metrics'], model_info['val_metrics'],\n",
        "                          X_train, X_val, model_info['params'], model_info['feature_list'], model_info['feature_categories'])\n",
        "\n",
        "    print(f\"\\nüéâ EXPERIMENT 7 COMPLETED!\")\n",
        "    print(f\"=\" * 60)\n",
        "    print(f\"üèÜ Final Validation WMAE: {model_info['val_metrics']['wmae']:.2f}\")\n",
        "    print(f\"üìä Final Validation R¬≤: {model_info['val_metrics']['r2']:.4f}\")\n",
        "    print(f\"üéØ This experiment uses the correct Walmart WMAE formula:\")\n",
        "    print(f\"   ‚Ä¢ Holiday weeks weighted 5x\")\n",
        "    print(f\"   ‚Ä¢ Regular weeks weighted 1x\")\n",
        "    print(f\"   ‚Ä¢ Preprocessing pipeline with date and holiday features (no lag features)\")\n",
        "    print(f\"   ‚Ä¢ Lag features removed to prevent overfitting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MKfaEO0gpa-e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b2ce14c526d0486f8fe4adb2e92cd2b8",
            "39702f16075c4d1cb565c4d79986ceba"
          ]
        },
        "outputId": "ec2ed312-277b-4607-d4b5-9150326db3ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ EXPERIMENT 7: XGBoost with Preprocessing Pipeline\n",
            "============================================================\n",
            "üîÑ Getting preprocessed data using pipeline...\n",
            "üìä Loading datasets...\n",
            "   üìà Train data: (421570, 5)\n",
            "   üè™ Stores data: (45, 3)\n",
            "   üéØ Features data: (8190, 12)\n",
            "   ‚úÖ Merged data: (421570, 17)\n",
            "   üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üßπ Cleaning merged data...\n",
            "   üîÑ Resolving duplicate IsHoliday columns...\n",
            "   ‚úÖ Cleaned data: (421570, 16) (was (421570, 17))\n",
            "üìÖ Creating temporal split (80/19)...\n",
            "   üìä Split date: 2012-04-13 00:00:00\n",
            "   üìà Train: 337,256 records (2010-02-05 00:00:00 to 2012-04-13 00:00:00)\n",
            "   üìâ Val: 84,314 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n",
            "üîß Fitting preprocessing pipeline on training data...\n",
            "‚úÖ Pipeline fitted on training data\n",
            "üîÑ Transforming training data...\n",
            "   ‚úÖ Handled missing values (filled MarkDown NaNs with 0).\n",
            "   üîß Encoding Type column using both one-hot and label encoding...\n",
            "   ‚úÖ Added both Type_Encoded and ['Type_A', 'Type_B', 'Type_C']\n",
            "   üóëÔ∏è Removed 45,193 outliers from training data\n",
            "   üìÅ Keeping MarkDown features.\n",
            "‚úÖ Transform complete. Shape: (292063, 32)\n",
            "üîÑ Transforming validation data...\n",
            "   ‚úÖ Handled missing values (filled MarkDown NaNs with 0).\n",
            "   üîß Encoding Type column using both one-hot and label encoding...\n",
            "   ‚úÖ Added both Type_Encoded and ['Type_A', 'Type_B', 'Type_C']\n",
            "   üìÅ Keeping MarkDown features.\n",
            "‚úÖ Transform complete. Shape: (84314, 31)\n",
            "\n",
            "MarkDown columns in X_train: ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "MarkDown columns in X_val: ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
            "‚úÖ Data ready:\n",
            "   X_train: (292063, 30)\n",
            "   y_train: (292063,)\n",
            "   X_val: (84314, 30)\n",
            "   y_val: (84314,)\n",
            "   train_holidays: (292063,) (22984 holidays)\n",
            "   val_holidays: (84314,) (2966 holidays)\n",
            "üöÄ Training XGBoost model...\n",
            "   üìã Total Features: 30\n",
            "   üìã Feature List: ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday', 'Month', 'DayOfWeek', 'IsWeekend', 'IsMonthStart', 'IsMonthEnd', 'WeeksFromStart', 'IsSuperBowlWeek', 'IsLaborDayWeek', 'IsThanksgivingWeek', 'IsChristmasWeek', 'IsMajorHoliday', 'IsHolidayMonth', 'IsBackToSchool', 'Type_Encoded', 'Type_A', 'Type_B', 'Type_C']\n",
            "   üìä Feature Categories:\n",
            "      ID_Features: ['Store', 'Dept']\n",
            "      Store_Info: ['Size']\n",
            "      Economic: ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment']\n",
            "      Date_Features: ['Month', 'DayOfWeek', 'WeeksFromStart']\n",
            "      Holiday_Features: ['IsHoliday', 'IsMajorHoliday', 'IsHolidayMonth', 'IsBackToSchool']\n",
            "      Type_OneHot: ['Type_A', 'Type_B', 'Type_C']\n",
            "      Type_Label: ['Type_Encoded']\n",
            "      Boolean_Features: ['IsWeekend', 'IsMonthStart', 'IsMonthEnd']\n",
            "      Other: ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'IsSuperBowlWeek', 'IsLaborDayWeek', 'IsThanksgivingWeek', 'IsChristmasWeek']\n",
            "   üìã Parameters: {'n_estimators': 200, 'max_depth': 8, 'learning_rate': 0.05, 'subsample': 0.8, 'colsample_bytree': 0.8, 'colsample_bylevel': 0.8, 'min_child_weight': 3, 'gamma': 0.1, 'reg_alpha': 0.1, 'reg_lambda': 1.0, 'random_state': 42, 'n_jobs': -1}\n",
            "   üîÑ Training XGBoost with regularization...\n",
            "   üìä Making predictions...\n",
            "\n",
            "‚úÖ XGBoost Training Complete!\n",
            "==================================================\n",
            "üìä TRAINING METRICS:\n",
            "   WMAE: 1854.59\n",
            "   RMSE: 2842.06\n",
            "   MAE: 1827.66\n",
            "   R¬≤: 0.9262\n",
            "\n",
            "üìä VALIDATION METRICS:\n",
            "   WMAE: 5983.96 ‚≠ê\n",
            "   RMSE: 14582.25\n",
            "   MAE: 5932.02\n",
            "   R¬≤: 0.5585\n",
            "\n",
            "üéÑ HOLIDAY ANALYSIS:\n",
            "   Training holiday weeks: 7.9%\n",
            "   Validation holiday weeks: 3.5%\n",
            "   Holiday weight multiplier: 5x\n",
            "\n",
            "üîç OVERFITTING ANALYSIS:\n",
            "   WMAE Ratio (val/train): 3.23\n",
            "   R¬≤ Difference (train-val): 0.3676\n",
            "   ‚ö†Ô∏è High overfitting detected (WMAE ratio > 2.0)\n",
            "\n",
            "üîù TOP 10 MOST IMPORTANT FEATURES:\n",
            "    1. Type_A                    0.2723\n",
            "    2. Dept                      0.2075\n",
            "    3. Type_Encoded              0.1503\n",
            "    4. Size                      0.1113\n",
            "    5. Type_B                    0.0425\n",
            "    6. Type_C                    0.0334\n",
            "    7. Store                     0.0330\n",
            "    8. IsHolidayMonth            0.0193\n",
            "    9. Month                     0.0176\n",
            "   10. IsBackToSchool            0.0158\n",
            "\n",
            "üìà MODEL INFO:\n",
            "   Training samples: 292,063\n",
            "   Validation samples: 84,314\n",
            "   Features: 30\n",
            "   Estimators: 200\n",
            "\n",
            "üìä Logging results to MLflow...\n",
            "üîß Setting up MLflow and DagsHub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2ce14c526d0486f8fe4adb2e92cd2b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=d8f48e42-ff8b-4899-b71a-5e74f0d994a2&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=bcf3e2ee1f6f1b87c925dc3432052dc0597194df7990bb44322052711579cbd1\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-19-4082636367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mzip_ref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'features.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-18-1599487013.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# Log results to MLflow after training is complete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     log_results_to_mlflow(model_info['model'], model_info['train_metrics'], model_info['val_metrics'],\n\u001b[0m\u001b[1;32m     66\u001b[0m                           X_train, X_val, model_info['params'], model_info['feature_list'], model_info['feature_categories'])\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-16-1474720217.py\u001b[0m in \u001b[0;36mlog_results_to_mlflow\u001b[0;34m(model, train_metrics, val_metrics, X_train, X_val, params, feature_list, feature_categories)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;31m# Setup MLflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mexperiment_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup_mlflow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"XGBoost_Walmart_Sales\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-13-1814538471.py\u001b[0m in \u001b[0;36msetup_mlflow\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Initialize DagsHub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         dagshub.init(\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0mrepo_owner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'konstantine25b'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mrepo_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Walmart-Recruiting---Store-Sales-Forecasting'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/common/init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(repo_name, repo_owner, url, root, host, mlflow, dvc, patch_mlflow)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Create the repo if it wasn't created\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mrepo_api\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepoAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{repo_owner}/{repo_name}\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mrepo_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_repo_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/common/api/repo.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, repo, host, auth)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdagshub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_authenticator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/auth/tokens.py\u001b[0m in \u001b[0;36mget_authenticator\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \"\"\"\n\u001b[0;32m--> 374\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_get_token_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_authenticator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/auth/tokens.py\u001b[0m in \u001b[0;36mget_authenticator\u001b[0;34m(self, host, fail_if_no_token, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \"\"\"\n\u001b[1;32m    121\u001b[0m         \u001b[0mhost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhost\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_token_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfail_if_no_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDagshubAuthenticator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_storage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/auth/tokens.py\u001b[0m in \u001b[0;36mget_token_object\u001b[0;34m(self, host, fail_if_no_token, **kwargs)\u001b[0m\n\u001b[1;32m    185\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No valid tokens found for host '{host}'. Authenticating with OAuth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m                     \u001b[0mgood_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moauth_flow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m                     \u001b[0mgood_token_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgood_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/auth/oauth.py\u001b[0m in \u001b[0;36moauth_flow\u001b[0;34m(host, client_id, referrer)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moauth_flow_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflow_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Got token: {token}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/dagshub/auth/oauth.py\u001b[0m in \u001b[0;36moauth_flow_post\u001b[0;34m(flow_data, quiet)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \"\"\"\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mrich_console\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Waiting for authorization\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mquiet\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         res = httpx.post(\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;34mf\"{flow_data.oauth_url}/middleman\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"request_id\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mflow_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmiddle_man_request_id\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(url, content, data, files, json, params, headers, cookies, auth, proxy, follow_redirects, verify, timeout, trust_env)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[0;34m**\u001b[0m\u001b[0mParameters\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \"\"\"\n\u001b[0;32m--> 304\u001b[0;31m     return request(\n\u001b[0m\u001b[1;32m    305\u001b[0m         \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, params, content, data, files, json, headers, cookies, auth, proxy, timeout, follow_redirects, verify, trust_env)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0mtrust_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     ) as client:\n\u001b[0;32m--> 109\u001b[0;31m         return client.request(\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mextensions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         )\n\u001b[0;32m--> 825\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    train = pd.read_csv('train.csv')\n",
        "    with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    features = pd.read_csv('features.csv')\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b2ce14c526d0486f8fe4adb2e92cd2b8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_39702f16075c4d1cb565c4d79986ceba",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†¥\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†¥</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "39702f16075c4d1cb565c4d79986ceba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}