{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOqhP32itjnpaaZeBoQ/oNo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting/blob/lodia/model_exp_RandomForest_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kp9k9iGAdomk",
        "outputId": "29032924-c96c-4451-d3de-ab79b2574fed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.6.15)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WhHWeyVWggHH",
        "outputId": "12b86103-f23a-4cb3-9dc4-73955513f079"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "YUCNy6rBgh4b"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JlHjPODWgjbI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Coiehd_sgmD3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtn6HNQSgngU",
        "outputId": "a20cc891-d5a9-4872-9c08-453d013ec81f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 551MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_dgvfsVgplH",
        "outputId": "1e3f7e23-6e59-48fa-a339-0591cfe10a6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "path = \"/content/\"\n",
        "\n",
        "\n",
        "files_to_unzip = [\n",
        "    \"features.csv.zip\",\n",
        "    \"test.csv.zip\",\n",
        "    \"train.csv.zip\",\n",
        "    \"sampleSubmission.csv.zip\"\n",
        "]\n",
        "\n",
        "print(\"Checking and unzipping individual files...\")\n",
        "for file_name in files_to_unzip:\n",
        "    full_path = os.path.join(path, file_name)\n",
        "    if os.path.exists(full_path):\n",
        "        try:\n",
        "            with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
        "\n",
        "                zip_ref.extractall(path)\n",
        "            print(f\"Successfully unzipped: {file_name}\")\n",
        "\n",
        "        except zipfile.BadZipFile:\n",
        "            print(f\"Warning: {file_name} is not a valid zip file or already unzipped/corrupted.\")\n",
        "    else:\n",
        "        print(f\"Info: {file_name} not found, likely already unzipped or not present.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKJ3ftfOgrHj",
        "outputId": "55eced84-5a83-4096-e889-45b9402da82b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking and unzipping individual files...\n",
            "Successfully unzipped: features.csv.zip\n",
            "Successfully unzipped: test.csv.zip\n",
            "Successfully unzipped: train.csv.zip\n",
            "Successfully unzipped: sampleSubmission.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    train_df = pd.read_csv(path + \"train.csv\")\n",
        "    test_df = pd.read_csv(path + \"test.csv\")\n",
        "    features_df = pd.read_csv(path + \"features.csv\")\n",
        "    stores_df = pd.read_csv(path + \"stores.csv\")\n",
        "    print(\"All datasets loaded successfully!\")\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error loading file: {e}. Please check the 'path' variable.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjWU0yk4gt_s",
        "outputId": "d4c823d8-73b4-40ba-87ed-7dd9992b26ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All datasets loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, make_scorer # make_scorer for custom metric\n",
        "import warnings\n",
        "from datetime import timedelta\n",
        "\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "metadata": {
        "id": "FjshSiZ3gwUX"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weighted_mean_absolute_error(y_true, y_pred, is_holiday_flag):\n",
        "    weights = np.where(is_holiday_flag, 5, 1)\n",
        "    y_pred = np.maximum(0, y_pred) # Ensure predictions are non-negative\n",
        "    return np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n"
      ],
      "metadata": {
        "id": "LALRflyUg02e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 1. Data Loading and Merging ---\")\n",
        "try:\n",
        "    path = \"./\" # Adjust this path if your files are in a specific directory\n",
        "    train_df = pd.read_csv(path + \"train.csv\")\n",
        "    features_df = pd.read_csv(path + \"features.csv\")\n",
        "    stores_df = pd.read_csv(path + \"stores.csv\")\n",
        "    print(\"Raw datasets loaded successfully!\")\n",
        "\n",
        "    # Convert 'Date' columns to datetime objects for time-series operations\n",
        "    train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "    features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "    # Merge datasets: train data with features (like temperature, fuel price) and store details\n",
        "    train_merged = pd.merge(train_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "    train_merged = pd.merge(train_merged, stores_df, on=['Store'], how='left')\n",
        "    print(\"train_merged created successfully by combining train, features, and stores datasets.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading or merging dataframes: {e}. Please ensure original CSVs are accessible and path is correct.\")\n",
        "    exit()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wg7yR_JpMz7K",
        "outputId": "98bae230-db48-4277-acdc-eb9bf65924e5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 1. Data Loading and Merging ---\n",
            "Raw datasets loaded successfully!\n",
            "train_merged created successfully by combining train, features, and stores datasets.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Preprocessing & Feature Engineering ---\n",
        "print(\"\\n--- 2. Preprocessing & Feature Engineering (V9.0) ---\")\n",
        "\n",
        "# Handle Negative Weekly_Sales: Sales cannot be negative, so set any negative values to 0.\n",
        "train_merged['Weekly_Sales'] = train_merged['Weekly_Sales'].apply(lambda x: max(0, x))\n",
        "print(\"Handled negative Weekly_Sales by setting them to 0.\")\n",
        "\n",
        "# 2.1 Date-based Features (Enhanced with Cyclical Features)\n",
        "print(\"Creating date-based features (Year, Month, Week, DayOfWeek, DayOfYear, IsMonthStart, IsMonthEnd, TimeIdx, Month_sin, Month_cos)...\")\n",
        "train_merged['Year'] = train_merged['Date'].dt.year\n",
        "train_merged['Month'] = train_merged['Date'].dt.month\n",
        "train_merged['Week'] = train_merged['Date'].dt.isocalendar().week.astype(int)\n",
        "train_merged['DayOfWeek'] = train_merged['Date'].dt.dayofweek\n",
        "train_merged['DayOfYear'] = train_merged['Date'].dt.dayofyear\n",
        "train_merged['IsMonthStart'] = train_merged['Date'].dt.is_month_start.astype(int)\n",
        "train_merged['IsMonthEnd'] = train_merged['Date'].dt.is_month_end.astype(int)\n",
        "train_merged['TimeIdx'] = (train_merged['Date'] - train_merged['Date'].min()).dt.days # Days since first record\n",
        "\n",
        "# Cyclical features for Month (from teammate's code)\n",
        "train_merged['Month_sin'] = np.sin(2 * np.pi * train_merged['Month'] / 12)\n",
        "train_merged['Month_cos'] = np.cos(2 * np.pi * train_merged['Month'] / 12)\n",
        "\n",
        "print(\"Date-based and cyclical features created.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUUb4rbINZ_o",
        "outputId": "86683625-b691-4b9b-bdfb-98a25c1ed2ff"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 2. Preprocessing & Feature Engineering (V9.0) ---\n",
            "Handled negative Weekly_Sales by setting them to 0.\n",
            "Creating date-based features (Year, Month, Week, DayOfWeek, DayOfYear, IsMonthStart, IsMonthEnd, TimeIdx, Month_sin, Month_cos)...\n",
            "Date-based and cyclical features created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.2 Handle Missing Values\n",
        "print(\"Handling missing values (MarkDown, CPI, Unemployment)...\")\n",
        "markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n",
        "for col in markdown_cols:\n",
        "    train_merged[col] = train_merged[col].fillna(0)\n",
        "print(\"Filled NaN values in MarkDown columns with 0.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmzkf0nYNmYE",
        "outputId": "cbaefafb-5c88-429d-9720-891512c00c27"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Handling missing values (MarkDown, CPI, Unemployment)...\n",
            "Filled NaN values in MarkDown columns with 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CPI and Unemployment: Fill NaN using forward-fill then backward-fill within each store group,\n",
        "# then fill any remaining NaNs with the median.\n",
        "train_merged['CPI'] = train_merged.groupby('Store')['CPI'].ffill().bfill()\n",
        "train_merged['Unemployment'] = train_merged.groupby('Store')['Unemployment'].ffill().bfill()\n",
        "train_merged['CPI'] = train_merged['CPI'].fillna(train_merged['CPI'].median())\n",
        "train_merged['Unemployment'] = train_merged['Unemployment'].fillna(train_merged['Unemployment'].median())\n",
        "print(\"Filled NaN values in CPI and Unemployment columns.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYGq0kOkNr_q",
        "outputId": "3fa5a071-7c98-4872-dc9e-97178923701e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filled NaN values in CPI and Unemployment columns.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.3 Lagged and Rolling Mean Sales Features (Crucial for Time Series, Streamlined)\n",
        "print(\"Creating lagged and rolling mean sales features...\")\n",
        "# Ensure data is sorted by Store, Dept, and Date for correct chronological calculations\n",
        "train_merged = train_merged.sort_values(by=['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "# Lagged sales from previous week and previous year (52 weeks) - KEPT, as per V2.0 performance\n",
        "train_merged['Lag_Weekly_Sales'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).fillna(0)\n",
        "train_merged['Lag52_Weekly_Sales'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(52).fillna(0)\n",
        "\n",
        "# Rolling mean of sales over the past 4 weeks (excluding current week) - KEPT\n",
        "train_merged['RollingMean_4W_Sales'] = train_merged.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "    lambda x: x.shift(1).rolling(window=4, min_periods=1).mean()\n",
        ").fillna(0)\n",
        "\n",
        "print(\"Lagged (1 & 52-week) and RollingMean_4W_Sales features created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VqpMQ-pQNyHi",
        "outputId": "b2e7b320-dfaf-44be-cb3e-d2d8beab5e3f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating lagged and rolling mean sales features...\n",
            "Lagged (1 & 52-week) and RollingMean_4W_Sales features created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.4 Interaction Feature (Temperature * IsHoliday)\n",
        "print(\"Creating Temperature_IsHoliday interaction feature...\")\n",
        "train_merged['Temperature_IsHoliday'] = train_merged['Temperature'] * train_merged['IsHoliday']\n",
        "print(\"Temperature_IsHoliday created.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWTn9g6uOGQq",
        "outputId": "8a34390a-5a96-465d-81d3-b28142ca1c9d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating Temperature_IsHoliday interaction feature...\n",
            "Temperature_IsHoliday created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define features (X) and target (y) for the model\n",
        "X = train_merged.drop(columns=['Weekly_Sales', 'Date'])\n",
        "y = train_merged['Weekly_Sales']\n",
        "is_holiday_flags = train_merged['IsHoliday'] # Keep for WMAE calculation\n"
      ],
      "metadata": {
        "id": "oEftORhTOPGx"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. ColumnTransformer and Pipeline Setup ---\n",
        "print(\"\\n--- 3. Preprocessor and Pipeline Setup ---\")\n",
        "\n",
        "categorical_features = ['Type']\n",
        "numerical_features = ['Store', 'Dept', 'Size', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'IsHoliday'] + \\\n",
        "                     markdown_cols + \\\n",
        "                     ['Year', 'Month', 'Week', 'DayOfWeek', 'DayOfYear', 'IsMonthStart', 'IsMonthEnd', 'TimeIdx'] + \\\n",
        "                     ['Month_sin', 'Month_cos'] + \\\n",
        "                     ['Lag_Weekly_Sales', 'Lag52_Weekly_Sales', 'RollingMean_4W_Sales'] + \\\n",
        "                     ['Temperature_IsHoliday']\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features),\n",
        "        ('num', 'passthrough', numerical_features)\n",
        "    ],\n",
        "    remainder='drop' # Explicitly drop columns not specified\n",
        ")\n",
        "print(\"ColumnTransformer for preprocessing defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JpFY3RTOZSj",
        "outputId": "08e91725-111a-4d2f-8df4-c10ae0d97953"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 3. Preprocessor and Pipeline Setup ---\n",
            "ColumnTransformer for preprocessing defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- 4. Data Splitting (Chronological with Friday Alignment) ---\")\n",
        "# Ensure data is sorted by Date before splitting to maintain chronological order for global split\n",
        "train_merged = train_merged.sort_values(by='Date').reset_index(drop=True)\n",
        "\n",
        "total_rows = len(train_merged)\n",
        "min_date = train_merged['Date'].min()\n",
        "max_date = train_merged['Date'].max()\n",
        "total_days = (max_date - min_date).days\n",
        "\n",
        "# Calculate split dates based on total time span\n",
        "train_split_days = int(total_days * 0.6)\n",
        "val_split_days = int(total_days * 0.8)\n",
        "\n",
        "train_split_date = min_date + timedelta(days=train_split_days)\n",
        "val_split_date = min_date + timedelta(days=val_split_days)\n",
        "\n",
        "# Align split dates to Friday (weekday() == 4 for Friday)\n",
        "while train_split_date.weekday() != 4 and train_split_date <= max_date:\n",
        "    train_split_date += timedelta(days=1)\n",
        "while val_split_date.weekday() != 4 and val_split_date <= max_date:\n",
        "    val_split_date += timedelta(days=1)\n",
        "\n",
        "# Ensure split dates don't exceed max date or overlap incorrectly\n",
        "if train_split_date >= max_date: # Handle edge case for very small datasets\n",
        "    train_split_date = min_date + timedelta(days=int(total_days * 0.5))\n",
        "    if train_split_date.weekday() != 4: train_split_date += timedelta(days=(4 - train_split_date.weekday() + 7) % 7)\n",
        "\n",
        "if val_split_date <= train_split_date or val_split_date >= max_date:\n",
        "    val_split_date = train_split_date + timedelta(days=int(total_days * 0.2)) # Recalculate if issue\n",
        "    if val_split_date.weekday() != 4: val_split_date += timedelta(days=(4 - val_split_date.weekday() + 7) % 7)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "otMVcLISOcR_",
        "outputId": "caab5dfd-80ed-473f-d344-003e95e67978"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 4. Data Splitting (Chronological with Friday Alignment) ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = X[train_merged['Date'] < train_split_date]\n",
        "y_train = y[train_merged['Date'] < train_split_date]\n",
        "is_holiday_train = is_holiday_flags[train_merged['Date'] < train_split_date]\n",
        "\n",
        "X_val = X[(train_merged['Date'] >= train_split_date) & (train_merged['Date'] < val_split_date)]\n",
        "y_val = y[(train_merged['Date'] >= train_split_date) & (train_merged['Date'] < val_split_date)]\n",
        "is_holiday_val = is_holiday_flags[(train_merged['Date'] >= train_split_date) & (train_merged['Date'] < val_split_date)]\n",
        "\n",
        "X_local_test = X[train_merged['Date'] >= val_split_date]\n",
        "y_local_test = y[train_merged['Date'] >= val_split_date]\n",
        "is_holiday_local_test = is_holiday_flags[train_merged['Date'] >= val_split_date]\n"
      ],
      "metadata": {
        "id": "F1uhjVg7OzaL"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Data split into Training ({len(X_train)} rows), Validation ({len(X_val)} rows), Local Test ({len(X_local_test)} rows).\")\n",
        "if not train_merged.empty:\n",
        "    print(f\"Train dates: {train_merged['Date'].min().date()} to {train_split_date.date()} (approx.)\")\n",
        "    print(f\"Validation dates: {train_split_date.date()} to {val_split_date.date()} (approx.)\")\n",
        "    print(f\"Local Test dates: {val_split_date.date()} to {train_merged['Date'].max().date()} (approx.)\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5c0967hJO0wZ",
        "outputId": "20221e14-319b-49b6-8b3c-01e8717a5f6e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split into Training (252413 rows), Validation (83348 rows), Local Test (85809 rows).\n",
            "Train dates: 2010-02-05 to 2011-09-30 (approx.)\n",
            "Validation dates: 2011-09-30 to 2012-04-13 (approx.)\n",
            "Local Test dates: 2012-04-13 to 2012-10-26 (approx.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Hyperparameter Tuning with RandomizedSearchCV and TimeSeriesSplit ---\n",
        "print(\"\\n--- 5. Hyperparameter Tuning (RandomizedSearchCV with TimeSeriesSplit) ---\")\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit # Import TimeSeriesSplit\n",
        "\n",
        "# Define the full pipeline\n",
        "rf_pipeline_tuning = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                      ('regressor', RandomForestRegressor(random_state=42))])\n",
        "\n",
        "# Conservative parameter distribution for quick initial tuning\n",
        "param_distributions = {\n",
        "    'regressor__n_estimators': [100, 150],      # Fewer estimators for speed\n",
        "    'regressor__max_depth': [15, 25],           # Controlled depth\n",
        "    'regressor__min_samples_split': [5],        # Higher values lead to less complex, faster trees\n",
        "    'regressor__min_samples_leaf': [2],         # Higher values lead to less complex, faster trees\n",
        "    'regressor__max_features': [0.8],           # Fixed for simplicity and speed\n",
        "}\n",
        "\n",
        "# Use TimeSeriesSplit for CV (only on training data)\n",
        "# n_splits=2 means 2 validation folds will be created on the training data.\n",
        "# The splits will be chronological.\n",
        "tscv = TimeSeriesSplit(n_splits=2)\n",
        "\n",
        "# Use MAE as scoring metric for RandomizedSearchCV\n",
        "scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_pipeline_tuning,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=3,         # Only 3 iterations for ultra-fast check\n",
        "    cv=tscv,\n",
        "    scoring=scorer,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,        # Use all available CPU cores\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "print(f\"Starting Randomized Search for optimal Random Forest hyperparameters (n_iter={random_search.n_iter})...\")\n",
        "random_search.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "RQ0yC48FO3uL",
        "outputId": "368146fd-3a4b-40fb-e58b-d46b13f2b7f8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- 5. Hyperparameter Tuning (RandomizedSearchCV with TimeSeriesSplit) ---\n",
            "Starting Randomized Search for optimal Random Forest hyperparameters (n_iter=3)...\n",
            "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25-37160282.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Randomized Search for optimal Random Forest hyperparameters (n_iter={random_search.n_iter})...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m   1022\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1950\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1951\u001b[0;31m         evaluate_candidates(\n\u001b[0m\u001b[1;32m   1952\u001b[0m             ParameterSampler(\n\u001b[1;32m   1953\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    968\u001b[0m                     )\n\u001b[1;32m    969\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 970\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    971\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    972\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         )\n\u001b[0;32m---> 77\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2070\u001b[0m         \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1681\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1682\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_retrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1684\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mGeneratorExit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1798\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTASK_PENDING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m                 ):\n\u001b[0;32m-> 1800\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1801\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1802\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 6. Train Final Model with Best Hyperparameters on FULL Training Data ---\n",
        "print(\"\\n--- 6. Train Final Model with Best Hyperparameters on FULL Training Data ---\")\n",
        "best_params = random_search.best_params_\n",
        "print(f\"Best hyperparameters found after tuning: {best_params}\")\n",
        "\n",
        "# Create the final Random Forest Regressor with best parameters\n",
        "final_rf_regressor = RandomForestRegressor(random_state=42,\n",
        "                                           n_estimators=best_params['regressor__n_estimators'],\n",
        "                                           max_depth=best_params['regressor__max_depth'],\n",
        "                                           min_samples_split=best_params['regressor__min_samples_split'],\n",
        "                                           min_samples_leaf=best_params['regressor__min_samples_leaf'],\n",
        "                                           max_features=best_params['regressor__max_features'],\n",
        "                                           n_jobs=-1\n",
        "                                          )\n",
        "\n",
        "# Create the final pipeline\n",
        "final_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                  ('regressor', final_rf_regressor)])\n",
        "\n",
        "print(\"Training final model on FULL X_train with best hyperparameters...\")\n",
        "final_pipeline.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "HJLnGf-LPZ4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Final Model Evaluation ---\n",
        "print(\"\\n--- 7. Final Model Evaluation ---\")\n",
        "\n",
        "# Evaluate the final model on the Validation Set\n",
        "print(\"\\nEvaluating on Validation Set with final model...\")\n",
        "val_predictions_final = final_pipeline.predict(X_val)\n",
        "val_wmae_final = weighted_mean_absolute_error(y_val, val_predictions_final, is_holiday_val)\n",
        "print(f\"Validation WMAE (Final Model): {val_wmae_final:.4f}\")\n",
        "val_mae_final = mean_absolute_error(y_val, val_predictions_final)\n",
        "print(f\"Validation MAE (Final Model): {val_mae_final:.4f}\")\n",
        "\n",
        "# Evaluate the final model on the Local Test Set\n",
        "print(\"\\nEvaluating on Local Test Set with final model...\")\n",
        "local_test_predictions_final = final_pipeline.predict(X_local_test)\n",
        "local_test_wmae_final = weighted_mean_absolute_error(y_local_test, local_test_predictions_final, is_holiday_local_test)\n",
        "print(f\"Local Test WMAE (Final Model): {local_test_wmae_final:.4f}\")\n",
        "local_test_mae_final = mean_absolute_error(y_local_test, local_test_predictions_final)\n",
        "print(f\"Local Test MAE (Final Model): {local_test_mae_final:.4f}\")\n",
        "\n",
        "print(\"\\n--- Random Forest Model Version 9.0 Training and Evaluation Complete ---\")\n"
      ],
      "metadata": {
        "id": "-MY1ly6BPqou"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}