{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 8"
      ],
      "metadata": {
        "id": "SKl2IW5WKfgn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "L7YpKSlAKYmp",
        "outputId": "9dac1c80-c5e2-46af-c040-0d6aeb24279f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "id": "_uyGGHajKkEG",
        "outputId": "f2299018-2f73-4519-b980-a18253388bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 861MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prophet plotly mlflow dagshub xgboost -q"
      ],
      "metadata": {
        "id": "dvF738QKKtJh",
        "outputId": "13320298-9c08-4089-a4df-ccf83b3c24c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/24.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.6/24.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.6/24.7 MB\u001b[0m \u001b[31m182.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.1/24.7 MB\u001b[0m \u001b[31m214.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m212.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m102.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys"
      ],
      "metadata": {
        "id": "rYY9RrK9K5WT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"=== STEP 1: Loading and Basic Data Overview ===\")\n",
        "\n",
        "# Load all datasets (extract from zip files first)\n",
        "print(\"Extracting and loading datasets...\")\n",
        "\n",
        "# Extract and load stores.csv (assuming it's not zipped based on your code)\n",
        "stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "# Extract and load train.csv\n",
        "with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Extract and load test.csv\n",
        "with zipfile.ZipFile('test.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "test_df = pd.read_csv('test.csv')\n",
        "\n",
        "# Extract and load features.csv\n",
        "with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "features_df = pd.read_csv('features.csv')\n",
        "\n",
        "print(\"‚úÖ All datasets loaded successfully!\")\n",
        "print(f\"Train shape: {train_df.shape}\")\n",
        "print(f\"Test shape: {test_df.shape}\")\n",
        "print(f\"Features shape: {features_df.shape}\")\n",
        "print(f\"Stores shape: {stores_df.shape}\")\n",
        "\n",
        "print(\"\\n=== TRAIN DATASET ===\")\n",
        "print(\"Columns:\", train_df.columns.tolist())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nData types:\")\n",
        "print(train_df.dtypes)\n",
        "print(\"\\nBasic statistics:\")\n",
        "print(train_df.describe())\n",
        "\n",
        "print(\"\\n=== TEST DATASET ===\")\n",
        "print(\"Columns:\", test_df.columns.tolist())\n",
        "print(\"First 5 rows:\")\n",
        "print(test_df.head())\n",
        "\n",
        "print(\"\\n=== FEATURES DATASET ===\")\n",
        "print(\"Columns:\", features_df.columns.tolist())\n",
        "print(\"First 5 rows:\")\n",
        "print(features_df.head())\n",
        "\n",
        "print(\"\\n=== STORES DATASET ===\")\n",
        "print(\"Columns:\", stores_df.columns.tolist())\n",
        "print(\"First 5 rows:\")\n",
        "print(stores_df.head())\n",
        "print(\"Store types and sizes:\")\n",
        "print(stores_df.groupby('Type').agg({'Size': ['count', 'mean', 'min', 'max']}))\n"
      ],
      "metadata": {
        "id": "NjngU5dlKwc_",
        "outputId": "ea14c87a-3f0c-4520-e83c-7847472c2af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== STEP 1: Loading and Basic Data Overview ===\n",
            "Extracting and loading datasets...\n",
            "‚úÖ All datasets loaded successfully!\n",
            "Train shape: (421570, 5)\n",
            "Test shape: (115064, 4)\n",
            "Features shape: (8190, 12)\n",
            "Stores shape: (45, 3)\n",
            "\n",
            "=== TRAIN DATASET ===\n",
            "Columns: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']\n",
            "\n",
            "First 5 rows:\n",
            "   Store  Dept        Date  Weekly_Sales  IsHoliday\n",
            "0      1     1  2010-02-05      24924.50      False\n",
            "1      1     1  2010-02-12      46039.49       True\n",
            "2      1     1  2010-02-19      41595.55      False\n",
            "3      1     1  2010-02-26      19403.54      False\n",
            "4      1     1  2010-03-05      21827.90      False\n",
            "\n",
            "Data types:\n",
            "Store             int64\n",
            "Dept              int64\n",
            "Date             object\n",
            "Weekly_Sales    float64\n",
            "IsHoliday          bool\n",
            "dtype: object\n",
            "\n",
            "Basic statistics:\n",
            "               Store           Dept   Weekly_Sales\n",
            "count  421570.000000  421570.000000  421570.000000\n",
            "mean       22.200546      44.260317   15981.258123\n",
            "std        12.785297      30.492054   22711.183519\n",
            "min         1.000000       1.000000   -4988.940000\n",
            "25%        11.000000      18.000000    2079.650000\n",
            "50%        22.000000      37.000000    7612.030000\n",
            "75%        33.000000      74.000000   20205.852500\n",
            "max        45.000000      99.000000  693099.360000\n",
            "\n",
            "=== TEST DATASET ===\n",
            "Columns: ['Store', 'Dept', 'Date', 'IsHoliday']\n",
            "First 5 rows:\n",
            "   Store  Dept        Date  IsHoliday\n",
            "0      1     1  2012-11-02      False\n",
            "1      1     1  2012-11-09      False\n",
            "2      1     1  2012-11-16      False\n",
            "3      1     1  2012-11-23       True\n",
            "4      1     1  2012-11-30      False\n",
            "\n",
            "=== FEATURES DATASET ===\n",
            "Columns: ['Store', 'Date', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday']\n",
            "First 5 rows:\n",
            "   Store        Date  Temperature  Fuel_Price  MarkDown1  MarkDown2  \\\n",
            "0      1  2010-02-05        42.31       2.572        NaN        NaN   \n",
            "1      1  2010-02-12        38.51       2.548        NaN        NaN   \n",
            "2      1  2010-02-19        39.93       2.514        NaN        NaN   \n",
            "3      1  2010-02-26        46.63       2.561        NaN        NaN   \n",
            "4      1  2010-03-05        46.50       2.625        NaN        NaN   \n",
            "\n",
            "   MarkDown3  MarkDown4  MarkDown5         CPI  Unemployment  IsHoliday  \n",
            "0        NaN        NaN        NaN  211.096358         8.106      False  \n",
            "1        NaN        NaN        NaN  211.242170         8.106       True  \n",
            "2        NaN        NaN        NaN  211.289143         8.106      False  \n",
            "3        NaN        NaN        NaN  211.319643         8.106      False  \n",
            "4        NaN        NaN        NaN  211.350143         8.106      False  \n",
            "\n",
            "=== STORES DATASET ===\n",
            "Columns: ['Store', 'Type', 'Size']\n",
            "First 5 rows:\n",
            "   Store Type    Size\n",
            "0      1    A  151315\n",
            "1      2    A  202307\n",
            "2      3    B   37392\n",
            "3      4    A  205863\n",
            "4      5    B   34875\n",
            "Store types and sizes:\n",
            "      Size                              \n",
            "     count           mean    min     max\n",
            "Type                                    \n",
            "A       22  177247.727273  39690  219622\n",
            "B       17  101190.705882  34875  140167\n",
            "C        6   40541.666667  39690   42988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== STEP 2: Date Analysis & Missing Values ===\")\n",
        "\n",
        "# Convert dates to datetime\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "test_df['Date'] = pd.to_datetime(test_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "print(\"‚úÖ Dates converted to datetime format\")\n",
        "\n",
        "# Analyze date ranges\n",
        "print(f\"\\nTRAIN date range: {train_df['Date'].min()} to {train_df['Date'].max()}\")\n",
        "print(f\"TEST date range: {test_df['Date'].min()} to {test_df['Date'].max()}\")\n",
        "print(f\"FEATURES date range: {features_df['Date'].min()} to {features_df['Date'].max()}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\n=== Missing Values Analysis ===\")\n",
        "print(\"TRAIN missing values:\")\n",
        "print(train_df.isnull().sum())\n",
        "\n",
        "print(\"\\nTEST missing values:\")\n",
        "print(test_df.isnull().sum())\n",
        "\n",
        "print(\"\\nFEATURES missing values:\")\n",
        "print(features_df.isnull().sum())\n",
        "print(f\"MarkDown missing percentages:\")\n",
        "for col in ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']:\n",
        "    missing_pct = (features_df[col].isnull().sum() / len(features_df)) * 100\n",
        "    print(f\"  {col}: {missing_pct:.1f}%\")\n",
        "\n",
        "print(\"\\nSTORES missing values:\")\n",
        "print(stores_df.isnull().sum())\n",
        "\n",
        "# Analyze unique counts\n",
        "print(\"\\n=== Unique Value Counts ===\")\n",
        "print(f\"Unique stores in train: {train_df['Store'].nunique()}\")\n",
        "print(f\"Unique departments in train: {train_df['Dept'].nunique()}\")\n",
        "print(f\"Unique store-dept combinations in train: {train_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "\n",
        "print(f\"\\nUnique stores in test: {test_df['Store'].nunique()}\")\n",
        "print(f\"Unique departments in test: {test_df['Dept'].nunique()}\")\n",
        "print(f\"Unique store-dept combinations in test: {test_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Check holiday distribution\n",
        "print(\"\\n=== Holiday Analysis ===\")\n",
        "print(\"Train holiday weeks:\")\n",
        "print(train_df['IsHoliday'].value_counts())\n",
        "print(f\"Holiday percentage in train: {(train_df['IsHoliday'].sum() / len(train_df)) * 100:.2f}%\")\n",
        "\n",
        "print(\"\\nTest holiday weeks:\")\n",
        "print(test_df['IsHoliday'].value_counts())\n",
        "print(f\"Holiday percentage in test: {(test_df['IsHoliday'].sum() / len(test_df)) * 100:.2f}%\")\n",
        "\n",
        "# Sales statistics\n",
        "print(\"\\n=== Sales Analysis ===\")\n",
        "print(f\"Negative sales records: {(train_df['Weekly_Sales'] < 0).sum()}\")\n",
        "print(f\"Zero sales records: {(train_df['Weekly_Sales'] == 0).sum()}\")\n",
        "print(f\"Sales range: ${train_df['Weekly_Sales'].min():.2f} to ${train_df['Weekly_Sales'].max():.2f}\")\n",
        "\n",
        "# Check for consistent time series\n",
        "print(\"\\n=== Time Series Consistency Check ===\")\n",
        "# Group by store and dept to check date consistency\n",
        "sample_store_dept = train_df[(train_df['Store'] == 1) & (train_df['Dept'] == 1)].copy()\n",
        "sample_store_dept = sample_store_dept.sort_values('Date')\n",
        "print(f\"Sample store 1, dept 1 date range: {sample_store_dept['Date'].min()} to {sample_store_dept['Date'].max()}\")\n",
        "print(f\"Number of records for store 1, dept 1: {len(sample_store_dept)}\")\n",
        "print(f\"Expected weekly records for this period: {(sample_store_dept['Date'].max() - sample_store_dept['Date'].min()).days // 7 + 1}\")\n"
      ],
      "metadata": {
        "id": "2IjuV27_LfI9",
        "outputId": "e8893994-e65d-4972-9545-29b23aae4513",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "=== STEP 2: Date Analysis & Missing Values ===\n",
            "‚úÖ Dates converted to datetime format\n",
            "\n",
            "TRAIN date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "TEST date range: 2012-11-02 00:00:00 to 2013-07-26 00:00:00\n",
            "FEATURES date range: 2010-02-05 00:00:00 to 2013-07-26 00:00:00\n",
            "\n",
            "=== Missing Values Analysis ===\n",
            "TRAIN missing values:\n",
            "Store           0\n",
            "Dept            0\n",
            "Date            0\n",
            "Weekly_Sales    0\n",
            "IsHoliday       0\n",
            "dtype: int64\n",
            "\n",
            "TEST missing values:\n",
            "Store        0\n",
            "Dept         0\n",
            "Date         0\n",
            "IsHoliday    0\n",
            "dtype: int64\n",
            "\n",
            "FEATURES missing values:\n",
            "Store              0\n",
            "Date               0\n",
            "Temperature        0\n",
            "Fuel_Price         0\n",
            "MarkDown1       4158\n",
            "MarkDown2       5269\n",
            "MarkDown3       4577\n",
            "MarkDown4       4726\n",
            "MarkDown5       4140\n",
            "CPI              585\n",
            "Unemployment     585\n",
            "IsHoliday          0\n",
            "dtype: int64\n",
            "MarkDown missing percentages:\n",
            "  MarkDown1: 50.8%\n",
            "  MarkDown2: 64.3%\n",
            "  MarkDown3: 55.9%\n",
            "  MarkDown4: 57.7%\n",
            "  MarkDown5: 50.5%\n",
            "\n",
            "STORES missing values:\n",
            "Store    0\n",
            "Type     0\n",
            "Size     0\n",
            "dtype: int64\n",
            "\n",
            "=== Unique Value Counts ===\n",
            "Unique stores in train: 45\n",
            "Unique departments in train: 81\n",
            "Unique store-dept combinations in train: 3331\n",
            "\n",
            "Unique stores in test: 45\n",
            "Unique departments in test: 81\n",
            "Unique store-dept combinations in test: 3169\n",
            "\n",
            "=== Holiday Analysis ===\n",
            "Train holiday weeks:\n",
            "IsHoliday\n",
            "False    391909\n",
            "True      29661\n",
            "Name: count, dtype: int64\n",
            "Holiday percentage in train: 7.04%\n",
            "\n",
            "Test holiday weeks:\n",
            "IsHoliday\n",
            "False    106136\n",
            "True       8928\n",
            "Name: count, dtype: int64\n",
            "Holiday percentage in test: 7.76%\n",
            "\n",
            "=== Sales Analysis ===\n",
            "Negative sales records: 1285\n",
            "Zero sales records: 73\n",
            "Sales range: $-4988.94 to $693099.36\n",
            "\n",
            "=== Time Series Consistency Check ===\n",
            "Sample store 1, dept 1 date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Number of records for store 1, dept 1: 143\n",
            "Expected weekly records for this period: 143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== STEP 3: Data Merging & Pattern Analysis ===\")\n",
        "\n",
        "# Create a complete dataset by merging train with features and stores\n",
        "print(\"Merging datasets...\")\n",
        "\n",
        "# Merge train with features\n",
        "train_complete = train_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "print(f\"Train after merging with features: {train_complete.shape}\")\n",
        "\n",
        "# Merge with stores\n",
        "train_complete = train_complete.merge(stores_df, on='Store', how='left')\n",
        "print(f\"Train after merging with stores: {train_complete.shape}\")\n",
        "\n",
        "# Check for any merge issues\n",
        "print(f\"Records in train_complete: {len(train_complete)}\")\n",
        "print(f\"Records in original train: {len(train_df)}\")\n",
        "print(\"‚úÖ All train records preserved in merge\")\n",
        "\n",
        "# Same for test\n",
        "test_complete = test_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "test_complete = test_complete.merge(stores_df, on='Store', how='left')\n",
        "print(f\"Test complete shape: {test_complete.shape}\")\n",
        "\n",
        "print(\"\\n=== Holiday Consistency Check ===\")\n",
        "# Check if IsHoliday is consistent between datasets\n",
        "holiday_check = train_complete[['Date', 'IsHoliday', 'IsHoliday_feat']].drop_duplicates()\n",
        "inconsistent_holidays = holiday_check[holiday_check['IsHoliday'] != holiday_check['IsHoliday_feat']]\n",
        "print(f\"Inconsistent holiday flags: {len(inconsistent_holidays)}\")\n",
        "if len(inconsistent_holidays) > 0:\n",
        "    print(\"Sample inconsistencies:\")\n",
        "    print(inconsistent_holidays.head())\n",
        "\n",
        "print(\"\\n=== Store-Department Distribution Analysis ===\")\n",
        "print(\"Top 10 departments by total sales:\")\n",
        "dept_sales = train_complete.groupby('Dept')['Weekly_Sales'].agg(['sum', 'count', 'mean']).round(2)\n",
        "dept_sales = dept_sales.sort_values('sum', ascending=False)\n",
        "print(dept_sales.head(10))\n",
        "\n",
        "print(\"\\nTop 10 stores by total sales:\")\n",
        "store_sales = train_complete.groupby('Store')['Weekly_Sales'].agg(['sum', 'count', 'mean']).round(2)\n",
        "store_sales = store_sales.sort_values('sum', ascending=False)\n",
        "print(store_sales.head(10))\n",
        "\n",
        "print(\"\\nSales by store type:\")\n",
        "type_sales = train_complete.groupby('Type')['Weekly_Sales'].agg(['sum', 'count', 'mean', 'std']).round(2)\n",
        "print(type_sales)\n",
        "\n",
        "print(\"\\n=== Seasonal Pattern Analysis ===\")\n",
        "# Add time features for analysis\n",
        "train_complete['Year'] = train_complete['Date'].dt.year\n",
        "train_complete['Month'] = train_complete['Date'].dt.month\n",
        "train_complete['Week'] = train_complete['Date'].dt.isocalendar().week\n",
        "train_complete['DayOfYear'] = train_complete['Date'].dt.dayofyear\n",
        "\n",
        "print(\"Sales by year:\")\n",
        "yearly_sales = train_complete.groupby('Year')['Weekly_Sales'].agg(['sum', 'mean', 'count']).round(2)\n",
        "print(yearly_sales)\n",
        "\n",
        "print(\"\\nSales by month (average):\")\n",
        "monthly_sales = train_complete.groupby('Month')['Weekly_Sales'].mean().round(2)\n",
        "print(monthly_sales)\n",
        "\n",
        "print(\"\\n=== Feature Correlation with Sales ===\")\n",
        "# Correlation analysis\n",
        "numeric_features = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'Size']\n",
        "available_features = [col for col in numeric_features if col in train_complete.columns]\n",
        "\n",
        "print(\"Correlation with Weekly_Sales:\")\n",
        "for feature in available_features:\n",
        "    corr = train_complete[['Weekly_Sales', feature]].corr().iloc[0, 1]\n",
        "    print(f\"  {feature}: {corr:.3f}\")\n",
        "\n",
        "print(\"\\n=== MarkDown Analysis ===\")\n",
        "# Analyze markdown patterns\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "markdown_data = train_complete[train_complete[markdown_cols].notna().any(axis=1)]\n",
        "print(f\"Records with any markdown data: {len(markdown_data)} ({len(markdown_data)/len(train_complete)*100:.1f}%)\")\n",
        "\n",
        "if len(markdown_data) > 0:\n",
        "    print(\"\\nAverage sales with vs without markdowns:\")\n",
        "    with_markdown = train_complete[train_complete[markdown_cols].notna().any(axis=1)]['Weekly_Sales'].mean()\n",
        "    without_markdown = train_complete[train_complete[markdown_cols].isna().all(axis=1)]['Weekly_Sales'].mean()\n",
        "    print(f\"  With markdowns: ${with_markdown:.2f}\")\n",
        "    print(f\"  Without markdowns: ${without_markdown:.2f}\")\n",
        "    print(f\"  Difference: {((with_markdown - without_markdown) / without_markdown * 100):+.1f}%\")\n",
        "\n",
        "print(\"\\n=== Data Quality Summary ===\")\n",
        "print(f\"Training period: {train_df['Date'].min().strftime('%Y-%m-%d')} to {train_df['Date'].max().strftime('%Y-%m-%d')} ({(train_df['Date'].max() - train_df['Date'].min()).days} days)\")\n",
        "print(f\"Test period: {test_df['Date'].min().strftime('%Y-%m-%d')} to {test_df['Date'].max().strftime('%Y-%m-%d')} ({(test_df['Date'].max() - test_df['Date'].min()).days} days)\")\n",
        "print(f\"Total unique store-dept combinations in train: {train_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "print(f\"Total unique store-dept combinations in test: {test_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "print(f\"Missing combinations in test: {train_df[['Store', 'Dept']].drop_duplicates().shape[0] - test_df[['Store', 'Dept']].drop_duplicates().shape[0]}\")\n",
        "\n",
        "# Check for any time series gaps\n",
        "print(f\"\\nTime series completeness check:\")\n",
        "print(f\"Expected weekly periods in train: {((train_df['Date'].max() - train_df['Date'].min()).days // 7) + 1}\")\n",
        "print(f\"Unique dates in train: {train_df['Date'].nunique()}\")\n",
        "print(f\"Expected weekly periods in test: {((test_df['Date'].max() - test_df['Date'].min()).days // 7) + 1}\")\n",
        "print(f\"Unique dates in test: {test_df['Date'].nunique()}\")\n"
      ],
      "metadata": {
        "id": "Op3F_eVJLwbG",
        "outputId": "9e7f7a90-2d56-4cb6-b15f-1df68c84c36d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "=== STEP 3: Data Merging & Pattern Analysis ===\n",
            "Merging datasets...\n",
            "Train after merging with features: (421570, 15)\n",
            "Train after merging with stores: (421570, 17)\n",
            "Records in train_complete: 421570\n",
            "Records in original train: 421570\n",
            "‚úÖ All train records preserved in merge\n",
            "Test complete shape: (115064, 16)\n",
            "\n",
            "=== Holiday Consistency Check ===\n",
            "Inconsistent holiday flags: 0\n",
            "\n",
            "=== Store-Department Distribution Analysis ===\n",
            "Top 10 departments by total sales:\n",
            "               sum  count      mean\n",
            "Dept                               \n",
            "92    4.839433e+08   6435  75204.87\n",
            "95    4.493202e+08   6435  69824.42\n",
            "38    3.931181e+08   6435  61090.62\n",
            "72    3.057252e+08   6046  50566.52\n",
            "90    2.910685e+08   6435  45232.08\n",
            "40    2.889360e+08   6435  44900.70\n",
            "2     2.806112e+08   6435  43607.02\n",
            "91    2.167817e+08   6435  33687.91\n",
            "13    1.973216e+08   6435  30663.80\n",
            "8     1.942808e+08   6435  30191.26\n",
            "\n",
            "Top 10 stores by total sales:\n",
            "                sum  count      mean\n",
            "Store                               \n",
            "20     3.013978e+08  10214  29508.30\n",
            "4      2.995440e+08  10272  29161.21\n",
            "14     2.889999e+08  10040  28784.85\n",
            "13     2.865177e+08  10474  27355.14\n",
            "2      2.753824e+08  10238  26898.07\n",
            "10     2.716177e+08  10315  26332.30\n",
            "27     2.538559e+08  10225  24826.98\n",
            "6      2.237561e+08  10211  21913.24\n",
            "1      2.224028e+08  10244  21710.54\n",
            "39     2.074455e+08   9878  21000.76\n",
            "\n",
            "Sales by store type:\n",
            "               sum   count      mean       std\n",
            "Type                                          \n",
            "A     4.331015e+09  215478  20099.57  26423.46\n",
            "B     2.000701e+09  163495  12237.08  17203.67\n",
            "C     4.055035e+08   42597   9519.53  15985.35\n",
            "\n",
            "=== Seasonal Pattern Analysis ===\n",
            "Sales by year:\n",
            "               sum      mean   count\n",
            "Year                                \n",
            "2010  2.288886e+09  16270.28  140679\n",
            "2011  2.448200e+09  15954.07  153453\n",
            "2012  2.000133e+09  15694.95  127438\n",
            "\n",
            "Sales by month (average):\n",
            "Month\n",
            "1     14126.08\n",
            "2     16008.78\n",
            "3     15416.66\n",
            "4     15650.34\n",
            "5     15776.34\n",
            "6     16326.14\n",
            "7     15861.42\n",
            "8     16062.52\n",
            "9     15095.89\n",
            "10    15243.86\n",
            "11    17491.03\n",
            "12    19355.70\n",
            "Name: Weekly_Sales, dtype: float64\n",
            "\n",
            "=== Feature Correlation with Sales ===\n",
            "Correlation with Weekly_Sales:\n",
            "  Temperature: -0.002\n",
            "  Fuel_Price: -0.000\n",
            "  CPI: -0.021\n",
            "  Unemployment: -0.026\n",
            "  Size: 0.244\n",
            "\n",
            "=== MarkDown Analysis ===\n",
            "Records with any markdown data: 151432 (35.9%)\n",
            "\n",
            "Average sales with vs without markdowns:\n",
            "  With markdowns: $16177.02\n",
            "  Without markdowns: $15871.52\n",
            "  Difference: +1.9%\n",
            "\n",
            "=== Data Quality Summary ===\n",
            "Training period: 2010-02-05 to 2012-10-26 (994 days)\n",
            "Test period: 2012-11-02 to 2013-07-26 (266 days)\n",
            "Total unique store-dept combinations in train: 3331\n",
            "Total unique store-dept combinations in test: 3169\n",
            "Missing combinations in test: 162\n",
            "\n",
            "Time series completeness check:\n",
            "Expected weekly periods in train: 143\n",
            "Unique dates in train: 143\n",
            "Expected weekly periods in test: 39\n",
            "Unique dates in test: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"=== STEP 4: Feature Engineering Strategy & Testing Plan ===\")\n",
        "\n",
        "print(\"Based on the analysis, here's our feature engineering strategy:\\n\")\n",
        "\n",
        "print(\"üéØ KEY INSIGHTS DISCOVERED:\")\n",
        "print(\"1. Strong seasonality: Nov-Dec show 20%+ higher sales\")\n",
        "print(\"2. Store size has decent correlation (0.244) with sales\")\n",
        "print(\"3. Store Type A >> B >> C in sales volume\")\n",
        "print(\"4. Departments 92, 95, 38 are top performers\")\n",
        "print(\"5. MarkDown data available for 36% of records (post Nov 2011)\")\n",
        "print(\"6. 162 store-dept combinations missing in test set\")\n",
        "print(\"7. Perfect time series: 143 weeks train, 39 weeks test\")\n",
        "\n",
        "print(\"\\nüìä FEATURE ENGINEERING PLAN:\")\n",
        "\n",
        "print(\"\\n1. TIME-BASED FEATURES:\")\n",
        "print(\"   - Month, Quarter, Week of Year\")\n",
        "print(\"   - Days to/from major holidays (Christmas, Thanksgiving)\")\n",
        "print(\"   - Holiday season indicator (Nov-Dec)\")\n",
        "print(\"   - Back-to-school season (Aug-Sep)\")\n",
        "print(\"   - Tax season (Jan-Apr)\")\n",
        "\n",
        "print(\"\\n2. LAG FEATURES (Critical for time series):\")\n",
        "print(\"   - Sales lags: 1, 2, 4, 8, 12, 52 weeks\")\n",
        "print(\"   - Rolling means: 4, 8, 12, 26 weeks\")\n",
        "print(\"   - Rolling std: 4, 8, 12 weeks\")\n",
        "print(\"   - Year-over-year lag (52 weeks)\")\n",
        "\n",
        "print(\"\\n3. STORE/DEPT FEATURES:\")\n",
        "print(\"   - Store size percentile within type\")\n",
        "print(\"   - Department rank by historical sales\")\n",
        "print(\"   - Store performance rank\")\n",
        "print(\"   - Dept seasonal index (performance vs average)\")\n",
        "\n",
        "print(\"\\n4. MARKDOWN FEATURES:\")\n",
        "print(\"   - Total markdown amount (sum of all 5)\")\n",
        "print(\"   - Number of active markdowns\")\n",
        "print(\"   - Markdown intensity (markdown/avg_sales)\")\n",
        "print(\"   - Days since last markdown\")\n",
        "\n",
        "print(\"\\n5. ECONOMIC FEATURES:\")\n",
        "print(\"   - CPI change rate\")\n",
        "print(\"   - Unemployment change rate\")\n",
        "print(\"   - Fuel price change rate\")\n",
        "print(\"   - Temperature deviation from normal\")\n",
        "\n",
        "print(\"\\n6. HOLIDAY FEATURES:\")\n",
        "print(\"   - Pre-holiday indicators (1-3 weeks before)\")\n",
        "print(\"   - Post-holiday indicators (1-2 weeks after)\")\n",
        "print(\"   - Holiday type encoding\")\n",
        "print(\"   - Distance to next/previous holiday\")\n",
        "\n",
        "print(\"\\nüèóÔ∏è MODELING STRATEGY:\")\n",
        "\n",
        "print(\"\\n1. VALIDATION APPROACH:\")\n",
        "print(\"   - Time series split: Use last 12 weeks of train as validation\")\n",
        "print(\"   - Walk-forward validation for hyperparameter tuning\")\n",
        "print(\"   - WMAE metric with 5x holiday weighting\")\n",
        "\n",
        "print(\"\\n2. MODEL PIPELINE PRIORITY:\")\n",
        "print(\"   Phase 1 - Baseline Models:\")\n",
        "print(\"     - Naive forecasts (seasonal, drift)\")\n",
        "print(\"     - Linear regression with basic features\")\n",
        "print(\"     - Target: Beat naive baselines\")\n",
        "print()\n",
        "print(\"   Phase 2 - Tree-Based Models:\")\n",
        "print(\"     - LightGBM with full feature set\")\n",
        "print(\"     - XGBoost with optimized features\")\n",
        "print(\"     - Target: Strong performance with interpretability\")\n",
        "print()\n",
        "print(\"   Phase 3 - Classical Time Series:\")\n",
        "print(\"     - Prophet (handles holidays well)\")\n",
        "print(\"     - SARIMA (for pure time series patterns)\")\n",
        "print(\"     - Target: Capture seasonal patterns\")\n",
        "print()\n",
        "print(\"   Phase 4 - Deep Learning:\")\n",
        "print(\"     - N-BEATS (pure neural forecasting)\")\n",
        "print(\"     - DLinear (efficient linear approach)\")\n",
        "print(\"     - TFT (if computational resources allow)\")\n",
        "print(\"     - Target: Capture complex patterns\")\n",
        "\n",
        "print(\"\\n3. ENSEMBLE STRATEGY:\")\n",
        "print(\"   - Weight models based on validation performance\")\n",
        "print(\"   - Consider different models for different store types\")\n",
        "print(\"   - Holiday vs non-holiday specific models\")\n",
        "\n",
        "print(\"\\nüìã NEXT STEPS CHECKLIST:\")\n",
        "print(\"1. ‚úÖ Data exploration and understanding\")\n",
        "print(\"2. üîÑ Feature engineering implementation\")\n",
        "print(\"3. ‚è≥ Baseline model development\")\n",
        "print(\"4. ‚è≥ Tree-based model experiments\")\n",
        "print(\"5. ‚è≥ Time series model experiments\")\n",
        "print(\"6. ‚è≥ Deep learning model experiments\")\n",
        "print(\"7. ‚è≥ Model ensembling\")\n",
        "print(\"8. ‚è≥ Final inference and submission\")\n",
        "\n",
        "print(\"\\nüí° CRITICAL SUCCESS FACTORS:\")\n",
        "print(\"- Handle 162 missing store-dept combinations in test\")\n",
        "print(\"- Proper WMAE evaluation (5x holiday weight)\")\n",
        "print(\"- Robust cross-validation strategy\")\n",
        "print(\"- Feature engineering for seasonality\")\n",
        "print(\"- MarkDown feature engineering post Nov 2011\")\n",
        "\n",
        "print(\"\\nüöÄ READY TO START FEATURE ENGINEERING!\")\n",
        "print(\"Next: Implement feature engineering pipeline\")\n",
        "print(\"Focus: Time-based and lag features first\")\n",
        "\n",
        "# Save insights for reference\n",
        "insights = {\n",
        "    'train_period': f\"{train_df['Date'].min()} to {train_df['Date'].max()}\",\n",
        "    'test_period': f\"{test_df['Date'].min()} to {test_df['Date'].max()}\",\n",
        "    'top_depts': dept_sales.head(5).index.tolist(),\n",
        "    'top_stores': store_sales.head(5).index.tolist(),\n",
        "    'holiday_pct_train': (train_df['IsHoliday'].sum() / len(train_df)) * 100,\n",
        "    'holiday_pct_test': (test_df['IsHoliday'].sum() / len(test_df)) * 100,\n",
        "    'markdown_availability': len(markdown_data) / len(train_complete) * 100,\n",
        "    'missing_combinations': train_df[['Store', 'Dept']].drop_duplicates().shape[0] - test_df[['Store', 'Dept']].drop_duplicates().shape[0]\n",
        "}\n",
        "\n",
        "print(f\"\\nüìä Key metrics saved for reference:\")\n",
        "for key, value in insights.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚ú® EXPLORATION COMPLETE - READY FOR FEATURE ENGINEERING ‚ú®\")\n"
      ],
      "metadata": {
        "id": "9zGL2MCtMUQL",
        "outputId": "37db9851-5334-489c-fd5f-58e4c19e5982",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "=== STEP 4: Feature Engineering Strategy & Testing Plan ===\n",
            "Based on the analysis, here's our feature engineering strategy:\n",
            "\n",
            "üéØ KEY INSIGHTS DISCOVERED:\n",
            "1. Strong seasonality: Nov-Dec show 20%+ higher sales\n",
            "2. Store size has decent correlation (0.244) with sales\n",
            "3. Store Type A >> B >> C in sales volume\n",
            "4. Departments 92, 95, 38 are top performers\n",
            "5. MarkDown data available for 36% of records (post Nov 2011)\n",
            "6. 162 store-dept combinations missing in test set\n",
            "7. Perfect time series: 143 weeks train, 39 weeks test\n",
            "\n",
            "üìä FEATURE ENGINEERING PLAN:\n",
            "\n",
            "1. TIME-BASED FEATURES:\n",
            "   - Month, Quarter, Week of Year\n",
            "   - Days to/from major holidays (Christmas, Thanksgiving)\n",
            "   - Holiday season indicator (Nov-Dec)\n",
            "   - Back-to-school season (Aug-Sep)\n",
            "   - Tax season (Jan-Apr)\n",
            "\n",
            "2. LAG FEATURES (Critical for time series):\n",
            "   - Sales lags: 1, 2, 4, 8, 12, 52 weeks\n",
            "   - Rolling means: 4, 8, 12, 26 weeks\n",
            "   - Rolling std: 4, 8, 12 weeks\n",
            "   - Year-over-year lag (52 weeks)\n",
            "\n",
            "3. STORE/DEPT FEATURES:\n",
            "   - Store size percentile within type\n",
            "   - Department rank by historical sales\n",
            "   - Store performance rank\n",
            "   - Dept seasonal index (performance vs average)\n",
            "\n",
            "4. MARKDOWN FEATURES:\n",
            "   - Total markdown amount (sum of all 5)\n",
            "   - Number of active markdowns\n",
            "   - Markdown intensity (markdown/avg_sales)\n",
            "   - Days since last markdown\n",
            "\n",
            "5. ECONOMIC FEATURES:\n",
            "   - CPI change rate\n",
            "   - Unemployment change rate\n",
            "   - Fuel price change rate\n",
            "   - Temperature deviation from normal\n",
            "\n",
            "6. HOLIDAY FEATURES:\n",
            "   - Pre-holiday indicators (1-3 weeks before)\n",
            "   - Post-holiday indicators (1-2 weeks after)\n",
            "   - Holiday type encoding\n",
            "   - Distance to next/previous holiday\n",
            "\n",
            "üèóÔ∏è MODELING STRATEGY:\n",
            "\n",
            "1. VALIDATION APPROACH:\n",
            "   - Time series split: Use last 12 weeks of train as validation\n",
            "   - Walk-forward validation for hyperparameter tuning\n",
            "   - WMAE metric with 5x holiday weighting\n",
            "\n",
            "2. MODEL PIPELINE PRIORITY:\n",
            "   Phase 1 - Baseline Models:\n",
            "     - Naive forecasts (seasonal, drift)\n",
            "     - Linear regression with basic features\n",
            "     - Target: Beat naive baselines\n",
            "\n",
            "   Phase 2 - Tree-Based Models:\n",
            "     - LightGBM with full feature set\n",
            "     - XGBoost with optimized features\n",
            "     - Target: Strong performance with interpretability\n",
            "\n",
            "   Phase 3 - Classical Time Series:\n",
            "     - Prophet (handles holidays well)\n",
            "     - SARIMA (for pure time series patterns)\n",
            "     - Target: Capture seasonal patterns\n",
            "\n",
            "   Phase 4 - Deep Learning:\n",
            "     - N-BEATS (pure neural forecasting)\n",
            "     - DLinear (efficient linear approach)\n",
            "     - TFT (if computational resources allow)\n",
            "     - Target: Capture complex patterns\n",
            "\n",
            "3. ENSEMBLE STRATEGY:\n",
            "   - Weight models based on validation performance\n",
            "   - Consider different models for different store types\n",
            "   - Holiday vs non-holiday specific models\n",
            "\n",
            "üìã NEXT STEPS CHECKLIST:\n",
            "1. ‚úÖ Data exploration and understanding\n",
            "2. üîÑ Feature engineering implementation\n",
            "3. ‚è≥ Baseline model development\n",
            "4. ‚è≥ Tree-based model experiments\n",
            "5. ‚è≥ Time series model experiments\n",
            "6. ‚è≥ Deep learning model experiments\n",
            "7. ‚è≥ Model ensembling\n",
            "8. ‚è≥ Final inference and submission\n",
            "\n",
            "üí° CRITICAL SUCCESS FACTORS:\n",
            "- Handle 162 missing store-dept combinations in test\n",
            "- Proper WMAE evaluation (5x holiday weight)\n",
            "- Robust cross-validation strategy\n",
            "- Feature engineering for seasonality\n",
            "- MarkDown feature engineering post Nov 2011\n",
            "\n",
            "üöÄ READY TO START FEATURE ENGINEERING!\n",
            "Next: Implement feature engineering pipeline\n",
            "Focus: Time-based and lag features first\n",
            "\n",
            "üìä Key metrics saved for reference:\n",
            "   train_period: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "   test_period: 2012-11-02 00:00:00 to 2013-07-26 00:00:00\n",
            "   top_depts: [92, 95, 38, 72, 90]\n",
            "   top_stores: [20, 4, 14, 13, 2]\n",
            "   holiday_pct_train: 7.035842208885831\n",
            "   holiday_pct_test: 7.759160119585622\n",
            "   markdown_availability: 35.92096211779776\n",
            "   missing_combinations: 162\n",
            "\n",
            "============================================================\n",
            "‚ú® EXPLORATION COMPLETE - READY FOR FEATURE ENGINEERING ‚ú®\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "import zipfile\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üöÄ WALMART SALES FORECASTING - FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load and prepare data (same as exploration)\n",
        "print(\"üì• Loading datasets...\")\n",
        "\n",
        "# Load stores.csv\n",
        "stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "# Extract and load train.csv\n",
        "with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "train_df = pd.read_csv('train.csv')\n",
        "\n",
        "# Extract and load features.csv\n",
        "with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "features_df = pd.read_csv('features.csv')\n",
        "\n",
        "print(\"‚úÖ Data loaded successfully!\")\n",
        "\n",
        "# Convert dates and merge\n",
        "train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "# Create complete training dataset\n",
        "train_complete = train_df.merge(features_df, on=['Store', 'Date'], how='left', suffixes=('', '_feat'))\n",
        "train_complete = train_complete.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "print(f\"Complete training data shape: {train_complete.shape}\")\n",
        "\n",
        "print(\"\\n‚è∞ TIME SERIES SPLIT STRATEGY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define time series split - CRITICAL: No data leakage!\n",
        "train_end_date = train_complete['Date'].max()\n",
        "train_start_date = train_complete['Date'].min()\n",
        "\n",
        "# Use last 12 weeks (3 months) as validation set\n",
        "validation_weeks = 12\n",
        "val_start_date = train_end_date - timedelta(weeks=validation_weeks-1)\n",
        "\n",
        "print(f\"Training period: {train_start_date} to {val_start_date - timedelta(days=7)}\")\n",
        "print(f\"Validation period: {val_start_date} to {train_end_date}\")\n",
        "print(f\"Total training weeks: {(train_end_date - train_start_date).days // 7 + 1}\")\n",
        "print(f\"Validation weeks: {validation_weeks}\")\n",
        "\n",
        "# Split data chronologically\n",
        "train_data = train_complete[train_complete['Date'] < val_start_date].copy()\n",
        "val_data = train_complete[train_complete['Date'] >= val_start_date].copy()\n",
        "\n",
        "print(f\"\\nTrain data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "print(f\"No data leakage: ‚úÖ\")\n",
        "\n",
        "print(\"\\nüîß FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def add_time_features(df):\n",
        "    \"\"\"Add time-based features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Basic time features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Quarter'] = df['Date'].dt.quarter\n",
        "    df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "    df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "\n",
        "    # Seasonal features\n",
        "    df['IsHolidaySeason'] = ((df['Month'] == 11) | (df['Month'] == 12)).astype(int)\n",
        "    df['IsBackToSchool'] = ((df['Month'] == 8) | (df['Month'] == 9)).astype(int)\n",
        "    df['IsTaxSeason'] = ((df['Month'] >= 1) & (df['Month'] <= 4)).astype(int)\n",
        "\n",
        "    # Holiday distance features (simplified for now)\n",
        "    # Christmas dates for each year\n",
        "    christmas_dates = {\n",
        "        2010: pd.to_datetime('2010-12-31'),\n",
        "        2011: pd.to_datetime('2011-12-30'),\n",
        "        2012: pd.to_datetime('2012-12-28'),\n",
        "    }\n",
        "\n",
        "    # Thanksgiving dates for each year\n",
        "    thanksgiving_dates = {\n",
        "        2010: pd.to_datetime('2010-11-26'),\n",
        "        2011: pd.to_datetime('2011-11-25'),\n",
        "        2012: pd.to_datetime('2012-11-23'),\n",
        "    }\n",
        "\n",
        "    # Calculate days to Christmas\n",
        "    df['DaysToChristmas'] = 0\n",
        "    df['DaysToThanksgiving'] = 0\n",
        "\n",
        "    for year in christmas_dates.keys():\n",
        "        year_mask = df['Year'] == year\n",
        "        if year_mask.any():\n",
        "            df.loc[year_mask, 'DaysToChristmas'] = (christmas_dates[year] - df.loc[year_mask, 'Date']).dt.days\n",
        "            df.loc[year_mask, 'DaysToThanksgiving'] = (thanksgiving_dates[year] - df.loc[year_mask, 'Date']).dt.days\n",
        "\n",
        "    # Pre/Post holiday indicators\n",
        "    df['PreChristmas'] = ((df['DaysToChristmas'] > 0) & (df['DaysToChristmas'] <= 21)).astype(int)\n",
        "    df['PostChristmas'] = ((df['DaysToChristmas'] < 0) & (df['DaysToChristmas'] >= -14)).astype(int)\n",
        "    df['PreThanksgiving'] = ((df['DaysToThanksgiving'] > 0) & (df['DaysToThanksgiving'] <= 14)).astype(int)\n",
        "    df['PostThanksgiving'] = ((df['DaysToThanksgiving'] < 0) & (df['DaysToThanksgiving'] >= -7)).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_lag_features(df, target_col='Weekly_Sales'):\n",
        "    \"\"\"Add lag features - ONLY using data available at prediction time\"\"\"\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "    # Lag features (previous weeks)\n",
        "    lag_periods = [1, 2, 4, 8, 12, 52]\n",
        "\n",
        "    for lag in lag_periods:\n",
        "        df[f'{target_col}_lag_{lag}'] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "    # Rolling features (windows ending BEFORE current week)\n",
        "    rolling_windows = [4, 8, 12, 26]\n",
        "\n",
        "    for window in rolling_windows:\n",
        "        # Rolling mean\n",
        "        df[f'{target_col}_rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).mean()\n",
        "        )\n",
        "\n",
        "        # Rolling std\n",
        "        df[f'{target_col}_rolling_std_{window}'] = df.groupby(['Store', 'Dept'])[target_col].transform(\n",
        "            lambda x: x.shift(1).rolling(window=window, min_periods=1).std()\n",
        "        )\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_store_dept_features(df):\n",
        "    \"\"\"Add store and department specific features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Store size percentile within type\n",
        "    df['SizePercentileInType'] = df.groupby('Type')['Size'].transform(lambda x: x.rank(pct=True))\n",
        "\n",
        "    # Historical performance features (calculated only on available data)\n",
        "    # Note: In real implementation, these should be calculated only on data before each prediction\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_markdown_features(df):\n",
        "    \"\"\"Add markdown related features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "    # Total markdown amount\n",
        "    df['TotalMarkDown'] = df[markdown_cols].sum(axis=1, skipna=True)\n",
        "\n",
        "    # Number of active markdowns\n",
        "    df['NumActiveMarkdowns'] = df[markdown_cols].count(axis=1)\n",
        "\n",
        "    # Has any markdown\n",
        "    df['HasMarkdown'] = (df['NumActiveMarkdowns'] > 0).astype(int)\n",
        "\n",
        "    # Fill NaN markdowns with 0 for calculations\n",
        "    for col in markdown_cols:\n",
        "        df[col] = df[col].fillna(0)\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_economic_features(df):\n",
        "    \"\"\"Add economic indicator features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Sort by date for proper shift calculations\n",
        "    df = df.sort_values(['Store', 'Date'])\n",
        "\n",
        "    # Change rates (week-over-week)\n",
        "    economic_vars = ['CPI', 'Unemployment', 'Fuel_Price', 'Temperature']\n",
        "\n",
        "    for var in economic_vars:\n",
        "        df[f'{var}_change'] = df.groupby('Store')[var].pct_change()\n",
        "        df[f'{var}_change'] = df[f'{var}_change'].fillna(0)\n",
        "\n",
        "    # Temperature deviation from store average\n",
        "    df['TempDeviation'] = df.groupby('Store')['Temperature'].transform(lambda x: x - x.mean())\n",
        "\n",
        "    return df\n",
        "\n",
        "print(\"üîÑ Applying feature engineering to training data...\")\n",
        "\n",
        "# Apply feature engineering pipeline\n",
        "train_features = add_time_features(train_data)\n",
        "train_features = add_lag_features(train_features)\n",
        "train_features = add_store_dept_features(train_features)\n",
        "train_features = add_markdown_features(train_features)\n",
        "train_features = add_economic_features(train_features)\n",
        "\n",
        "print(f\"‚úÖ Training features created: {train_features.shape}\")\n",
        "\n",
        "print(\"üîÑ Applying same features to validation data...\")\n",
        "\n",
        "# Apply same transformations to validation data\n",
        "val_features = add_time_features(val_data)\n",
        "val_features = add_lag_features(val_features)  # This will use train data for lags\n",
        "val_features = add_store_dept_features(val_features)\n",
        "val_features = add_markdown_features(val_features)\n",
        "val_features = add_economic_features(val_features)\n",
        "\n",
        "print(f\"‚úÖ Validation features created: {val_features.shape}\")\n",
        "\n",
        "print(\"\\nüìä FEATURE SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get feature columns (excluding original columns)\n",
        "original_cols = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n",
        "                'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n",
        "                'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday_feat', 'Type', 'Size']\n",
        "\n",
        "feature_cols = [col for col in train_features.columns if col not in original_cols]\n",
        "\n",
        "print(f\"Total features created: {len(feature_cols)}\")\n",
        "print(f\"Original columns: {len(original_cols)}\")\n",
        "print(f\"Total columns: {train_features.shape[1]}\")\n",
        "\n",
        "print(\"\\nFeature categories:\")\n",
        "time_features = [col for col in feature_cols if any(x in col.lower() for x in ['year', 'month', 'quarter', 'week', 'day', 'season', 'holiday', 'christmas', 'thanksgiving'])]\n",
        "lag_features = [col for col in feature_cols if 'lag' in col or 'rolling' in col]\n",
        "markdown_features = [col for col in feature_cols if 'markdown' in col.lower()]\n",
        "economic_features = [col for col in feature_cols if 'change' in col or 'deviation' in col]\n",
        "store_features = [col for col in feature_cols if 'percentile' in col.lower() or 'size' in col.lower()]\n",
        "\n",
        "print(f\"  Time features: {len(time_features)}\")\n",
        "print(f\"  Lag features: {len(lag_features)}\")\n",
        "print(f\"  Markdown features: {len(markdown_features)}\")\n",
        "print(f\"  Economic features: {len(economic_features)}\")\n",
        "print(f\"  Store features: {len(store_features)}\")\n",
        "\n",
        "print(\"\\nüéØ NEXT STEPS\")\n",
        "print(\"=\" * 50)\n",
        "print(\"1. ‚úÖ Feature engineering pipeline complete\")\n",
        "print(\"2. üîÑ Ready for baseline model development\")\n",
        "print(\"3. ‚è≥ Implement WMAE metric\")\n",
        "print(\"4. ‚è≥ Build naive forecasting baselines\")\n",
        "print(\"5. ‚è≥ Develop tree-based models\")\n",
        "\n",
        "# Save processed data for modeling\n",
        "print(\"\\nüíæ Saving processed data...\")\n",
        "train_features.to_csv('train_features.csv', index=False)\n",
        "val_features.to_csv('val_features.csv', index=False)\n",
        "print(\"‚úÖ Data saved to train_features.csv and val_features.csv\")\n",
        "\n",
        "print(\"\\nüéâ FEATURE ENGINEERING COMPLETE!\")\n",
        "print(\"Ready to start modeling experiments!\")"
      ],
      "metadata": {
        "id": "tEXKPkyOMsCn",
        "outputId": "f2f302ee-646e-4e6c-877b-997166db233d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ WALMART SALES FORECASTING - FEATURE ENGINEERING PIPELINE\n",
            "======================================================================\n",
            "üì• Loading datasets...\n",
            "‚úÖ Data loaded successfully!\n",
            "Complete training data shape: (421570, 17)\n",
            "\n",
            "‚è∞ TIME SERIES SPLIT STRATEGY\n",
            "==================================================\n",
            "Training period: 2010-02-05 00:00:00 to 2012-08-03 00:00:00\n",
            "Validation period: 2012-08-10 00:00:00 to 2012-10-26 00:00:00\n",
            "Total training weeks: 143\n",
            "Validation weeks: 12\n",
            "\n",
            "Train data shape: (386007, 17)\n",
            "Validation data shape: (35563, 17)\n",
            "No data leakage: ‚úÖ\n",
            "\n",
            "üîß FEATURE ENGINEERING PIPELINE\n",
            "==================================================\n",
            "üîÑ Applying feature engineering to training data...\n",
            "‚úÖ Training features created: (386007, 54)\n",
            "üîÑ Applying same features to validation data...\n",
            "‚úÖ Validation features created: (35563, 54)\n",
            "\n",
            "üìä FEATURE SUMMARY\n",
            "==================================================\n",
            "Total features created: 37\n",
            "Original columns: 17\n",
            "Total columns: 54\n",
            "\n",
            "Feature categories:\n",
            "  Time features: 27\n",
            "  Lag features: 14\n",
            "  Markdown features: 3\n",
            "  Economic features: 4\n",
            "  Store features: 1\n",
            "\n",
            "üéØ NEXT STEPS\n",
            "==================================================\n",
            "1. ‚úÖ Feature engineering pipeline complete\n",
            "2. üîÑ Ready for baseline model development\n",
            "3. ‚è≥ Implement WMAE metric\n",
            "4. ‚è≥ Build naive forecasting baselines\n",
            "5. ‚è≥ Develop tree-based models\n",
            "\n",
            "üíæ Saving processed data...\n",
            "‚úÖ Data saved to train_features.csv and val_features.csv\n",
            "\n",
            "üéâ FEATURE ENGINEERING COMPLETE!\n",
            "Ready to start modeling experiments!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üéØ WALMART SALES FORECASTING - LIGHTGBM BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load processed data\n",
        "print(\"üì• Loading processed datasets...\")\n",
        "train_features = pd.read_csv('train_features.csv')\n",
        "val_features = pd.read_csv('val_features.csv')\n",
        "\n",
        "# Convert dates back to datetime\n",
        "train_features['Date'] = pd.to_datetime(train_features['Date'])\n",
        "val_features['Date'] = pd.to_datetime(val_features['Date'])\n",
        "\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Validation features shape: {val_features.shape}\")\n",
        "\n",
        "print(\"\\nüìä WMAE METRIC IMPLEMENTATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def wmae(y_true, y_pred, weights):\n",
        "    \"\"\"Weighted Mean Absolute Error - Holiday weeks have 5x weight\"\"\"\n",
        "    absolute_errors = np.abs(y_true - y_pred)\n",
        "    weighted_errors = absolute_errors * weights\n",
        "    return np.sum(weighted_errors) / np.sum(weights)\n",
        "\n",
        "def calculate_weights(is_holiday):\n",
        "    \"\"\"Calculate weights: 5 for holiday weeks, 1 for normal weeks\"\"\"\n",
        "    return np.where(is_holiday, 5.0, 1.0)\n",
        "\n",
        "print(\"‚úÖ WMAE implementation ready\")\n",
        "\n",
        "print(\"\\nüîß PREPARING DATA FOR LIGHTGBM\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare features (exclude target and identifiers)\n",
        "exclude_cols = ['Date', 'Weekly_Sales', 'Store', 'Dept']\n",
        "feature_cols = [col for col in train_features.columns if col not in exclude_cols]\n",
        "\n",
        "# Handle categorical variables\n",
        "categorical_cols = ['Type']\n",
        "for col in categorical_cols:\n",
        "    if col in feature_cols:\n",
        "        le = LabelEncoder()\n",
        "        # Fit on combined data to ensure consistent encoding\n",
        "        combined_values = pd.concat([train_features[col], val_features[col]]).astype(str)\n",
        "        le.fit(combined_values)\n",
        "\n",
        "        train_features[col + '_encoded'] = le.transform(train_features[col].astype(str))\n",
        "        val_features[col + '_encoded'] = le.transform(val_features[col].astype(str))\n",
        "\n",
        "        feature_cols.remove(col)\n",
        "        feature_cols.append(col + '_encoded')\n",
        "\n",
        "print(f\"Total features for LightGBM: {len(feature_cols)}\")\n",
        "\n",
        "# Check for missing values\n",
        "print(\"\\nMissing value analysis:\")\n",
        "train_missing = train_features[feature_cols].isnull().sum()\n",
        "val_missing = val_features[feature_cols].isnull().sum()\n",
        "\n",
        "print(f\"Features with missing values in train: {(train_missing > 0).sum()}\")\n",
        "print(f\"Features with missing values in validation: {(val_missing > 0).sum()}\")\n",
        "\n",
        "# LightGBM can handle missing values, so we don't need to drop rows\n",
        "print(f\"Using all training records: {len(train_features)}\")\n",
        "print(f\"Using all validation records: {len(val_features)}\")\n",
        "\n",
        "print(\"\\nüîß NAIVE BASELINE FOR COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def naive_seasonal_simple(train_df, val_df):\n",
        "    \"\"\"Simple seasonal naive with fallbacks\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Create a mapping of store-dept to recent sales for faster lookup\n",
        "    recent_sales = {}\n",
        "    for store in train_df['Store'].unique():\n",
        "        for dept in train_df['Dept'].unique():\n",
        "            store_dept_data = train_df[(train_df['Store'] == store) & (train_df['Dept'] == dept)]\n",
        "            if len(store_dept_data) > 0:\n",
        "                recent_sales[(store, dept)] = store_dept_data['Weekly_Sales'].iloc[-4:].mean()  # Last 4 weeks avg\n",
        "\n",
        "    overall_mean = train_df['Weekly_Sales'].mean()\n",
        "\n",
        "    for _, row in val_df.iterrows():\n",
        "        store, dept = row['Store'], row['Dept']\n",
        "\n",
        "        if (store, dept) in recent_sales:\n",
        "            pred = recent_sales[(store, dept)]\n",
        "        else:\n",
        "            pred = overall_mean\n",
        "\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Run naive baseline\n",
        "print(\"üîÑ Running naive seasonal baseline...\")\n",
        "naive_pred = naive_seasonal_simple(train_features, val_features)\n",
        "val_weights = calculate_weights(val_features['IsHoliday'].values)\n",
        "naive_wmae = wmae(val_features['Weekly_Sales'].values, naive_pred, val_weights)\n",
        "\n",
        "print(f\"Naive Seasonal WMAE: {naive_wmae:,.2f}\")\n",
        "\n",
        "print(\"\\nüîß LIGHTGBM MODEL TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# LightGBM parameters\n",
        "lgb_params = {\n",
        "    'objective': 'regression',\n",
        "    'metric': 'mae',\n",
        "    'boosting_type': 'gbdt',\n",
        "    'num_leaves': 63,\n",
        "    'learning_rate': 0.05,\n",
        "    'feature_fraction': 0.8,\n",
        "    'bagging_fraction': 0.8,\n",
        "    'bagging_freq': 5,\n",
        "    'min_data_in_leaf': 50,\n",
        "    'lambda_l1': 0.1,\n",
        "    'lambda_l2': 0.1,\n",
        "    'verbose': -1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "print(\"üîÑ Training LightGBM model...\")\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "print(f\"Training samples: {len(train_features)}\")\n",
        "print(f\"Validation samples: {len(val_features)}\")\n",
        "\n",
        "# Create LightGBM datasets\n",
        "train_lgb = lgb.Dataset(\n",
        "    train_features[feature_cols],\n",
        "    label=train_features['Weekly_Sales'],\n",
        "    categorical_feature=['Type_encoded'] if 'Type_encoded' in feature_cols else None\n",
        ")\n",
        "\n",
        "val_lgb = lgb.Dataset(\n",
        "    val_features[feature_cols],\n",
        "    label=val_features['Weekly_Sales'],\n",
        "    reference=train_lgb,\n",
        "    categorical_feature=['Type_encoded'] if 'Type_encoded' in feature_cols else None\n",
        ")\n",
        "\n",
        "# Train model with early stopping\n",
        "lgb_model = lgb.train(\n",
        "    lgb_params,\n",
        "    train_lgb,\n",
        "    valid_sets=[train_lgb, val_lgb],\n",
        "    valid_names=['train', 'valid'],\n",
        "    num_boost_round=500,\n",
        "    callbacks=[\n",
        "        lgb.early_stopping(50),\n",
        "        lgb.log_evaluation(50)\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training completed. Best iteration: {lgb_model.best_iteration}\")\n",
        "\n",
        "print(\"\\nüìä MODEL EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "lgb_pred = lgb_model.predict(val_features[feature_cols])\n",
        "\n",
        "# Calculate WMAE\n",
        "lgb_wmae = wmae(val_features['Weekly_Sales'].values, lgb_pred, val_weights)\n",
        "\n",
        "print(f\"LightGBM WMAE: {lgb_wmae:,.2f}\")\n",
        "print(f\"Naive WMAE: {naive_wmae:,.2f}\")\n",
        "print(f\"Improvement: {((naive_wmae - lgb_wmae) / naive_wmae * 100):+.1f}%\")\n",
        "\n",
        "print(\"\\nüìà FEATURE IMPORTANCE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get feature importance\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': lgb_model.feature_importance(importance_type='gain')\n",
        "})\n",
        "importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 most important features:\")\n",
        "top_features = importance_df.head(15)\n",
        "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {row['feature']:<25} {row['importance']:>10,.0f}\")\n",
        "\n",
        "print(\"\\nüìä PREDICTION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyze predictions by holiday vs non-holiday\n",
        "holiday_mask = val_features['IsHoliday'] == True\n",
        "non_holiday_mask = val_features['IsHoliday'] == False\n",
        "\n",
        "if holiday_mask.sum() > 0:\n",
        "    holiday_mae = np.mean(np.abs(val_features.loc[holiday_mask, 'Weekly_Sales'] - lgb_pred[holiday_mask]))\n",
        "    print(f\"Holiday weeks MAE: {holiday_mae:,.2f}\")\n",
        "\n",
        "if non_holiday_mask.sum() > 0:\n",
        "    non_holiday_mae = np.mean(np.abs(val_features.loc[non_holiday_mask, 'Weekly_Sales'] - lgb_pred[non_holiday_mask]))\n",
        "    print(f\"Non-holiday weeks MAE: {non_holiday_mae:,.2f}\")\n",
        "\n",
        "# Analyze by store type\n",
        "for store_type in val_features['Type'].unique():\n",
        "    type_mask = val_features['Type'] == store_type\n",
        "    if type_mask.sum() > 0:\n",
        "        type_mae = np.mean(np.abs(val_features.loc[type_mask, 'Weekly_Sales'] - lgb_pred[type_mask]))\n",
        "        print(f\"Store Type {store_type} MAE: {type_mae:,.2f}\")\n",
        "\n",
        "print(\"\\nüíæ SAVING RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'Model': ['Naive Seasonal', 'LightGBM'],\n",
        "    'WMAE': [naive_wmae, lgb_wmae],\n",
        "    'Records': [len(val_features), len(val_features)]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('lightgbm_baseline_results.csv', index=False)\n",
        "\n",
        "# Save feature importance\n",
        "importance_df.to_csv('lightgbm_feature_importance.csv', index=False)\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Store': val_features['Store'],\n",
        "    'Dept': val_features['Dept'],\n",
        "    'Date': val_features['Date'],\n",
        "    'Actual': val_features['Weekly_Sales'],\n",
        "    'IsHoliday': val_features['IsHoliday'],\n",
        "    'Type': val_features['Type'],\n",
        "    'Naive_Pred': naive_pred,\n",
        "    'LightGBM_Pred': lgb_pred,\n",
        "    'Naive_Error': np.abs(val_features['Weekly_Sales'] - naive_pred),\n",
        "    'LightGBM_Error': np.abs(val_features['Weekly_Sales'] - lgb_pred)\n",
        "})\n",
        "predictions_df.to_csv('lightgbm_predictions.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Results saved:\")\n",
        "print(\"  - lightgbm_baseline_results.csv\")\n",
        "print(\"  - lightgbm_feature_importance.csv\")\n",
        "print(\"  - lightgbm_predictions.csv\")\n",
        "\n",
        "print(f\"\\nüéØ BASELINE ESTABLISHED!\")\n",
        "print(f\"üèÜ Target to beat: {lgb_wmae:,.2f} WMAE\")\n",
        "print(f\"üìà Next steps: Hyperparameter tuning, feature engineering, ensemble methods\")\n",
        "\n",
        "print(\"\\nüéâ LIGHTGBM BASELINE COMPLETE! üöÄ\")"
      ],
      "metadata": {
        "id": "R-xFstGgOSX_",
        "outputId": "86cc7031-ee5b-400e-8dd4-9d29138abec6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ WALMART SALES FORECASTING - LIGHTGBM BASELINE\n",
            "============================================================\n",
            "üì• Loading processed datasets...\n",
            "Train features shape: (386007, 54)\n",
            "Validation features shape: (35563, 54)\n",
            "\n",
            "üìä WMAE METRIC IMPLEMENTATION\n",
            "==================================================\n",
            "‚úÖ WMAE implementation ready\n",
            "\n",
            "üîß PREPARING DATA FOR LIGHTGBM\n",
            "==================================================\n",
            "Total features for LightGBM: 50\n",
            "\n",
            "Missing value analysis:\n",
            "Features with missing values in train: 14\n",
            "Features with missing values in validation: 14\n",
            "Using all training records: 386007\n",
            "Using all validation records: 35563\n",
            "\n",
            "üîß NAIVE BASELINE FOR COMPARISON\n",
            "==================================================\n",
            "üîÑ Running naive seasonal baseline...\n",
            "Naive Seasonal WMAE: 2,142.17\n",
            "\n",
            "üîß LIGHTGBM MODEL TRAINING\n",
            "==================================================\n",
            "üîÑ Training LightGBM model...\n",
            "Features: 50\n",
            "Training samples: 386007\n",
            "Validation samples: 35563\n",
            "Training until validation scores don't improve for 50 rounds\n",
            "[50]\ttrain's l1: 2226.57\tvalid's l1: 3015.24\n",
            "[100]\ttrain's l1: 1639.14\tvalid's l1: 2542.67\n",
            "Early stopping, best iteration is:\n",
            "[97]\ttrain's l1: 1646.11\tvalid's l1: 2525.53\n",
            "‚úÖ Training completed. Best iteration: 97\n",
            "\n",
            "üìä MODEL EVALUATION\n",
            "==================================================\n",
            "LightGBM WMAE: 2,418.70\n",
            "Naive WMAE: 2,142.17\n",
            "Improvement: -12.9%\n",
            "\n",
            "üìà FEATURE IMPORTANCE\n",
            "==================================================\n",
            "Top 15 most important features:\n",
            " 1. Weekly_Sales_lag_1        1,091,696,802,388,736\n",
            " 2. Weekly_Sales_rolling_mean_4 352,929,871,381,248\n",
            " 3. Weekly_Sales_rolling_mean_8 34,824,903,481,728\n",
            " 4. Weekly_Sales_lag_52       23,359,783,364,608\n",
            " 5. Weekly_Sales_lag_4        15,001,051,912,448\n",
            " 6. Weekly_Sales_lag_2        12,735,409,119,872\n",
            " 7. DayOfYear                 7,467,751,315,968\n",
            " 8. WeekOfYear                5,513,277,996,160\n",
            " 9. Weekly_Sales_rolling_mean_26 4,472,375,339,520\n",
            "10. MarkDown3                 3,987,509,339,776\n",
            "11. Weekly_Sales_rolling_std_4 3,291,346,155,392\n",
            "12. PreChristmas              3,192,087,571,200\n",
            "13. Weekly_Sales_rolling_std_26 2,868,926,479,872\n",
            "14. Weekly_Sales_rolling_mean_12 2,727,190,658,688\n",
            "15. Month                     2,323,440,023,808\n",
            "\n",
            "üìä PREDICTION ANALYSIS\n",
            "==================================================\n",
            "Holiday weeks MAE: 2,098.49\n",
            "Non-holiday weeks MAE: 2,564.38\n",
            "Store Type A MAE: 3,113.31\n",
            "Store Type B MAE: 2,009.44\n",
            "Store Type C MAE: 1,558.74\n",
            "\n",
            "üíæ SAVING RESULTS\n",
            "==================================================\n",
            "‚úÖ Results saved:\n",
            "  - lightgbm_baseline_results.csv\n",
            "  - lightgbm_feature_importance.csv\n",
            "  - lightgbm_predictions.csv\n",
            "\n",
            "üéØ BASELINE ESTABLISHED!\n",
            "üèÜ Target to beat: 2,418.70 WMAE\n",
            "üìà Next steps: Hyperparameter tuning, feature engineering, ensemble methods\n",
            "\n",
            "üéâ LIGHTGBM BASELINE COMPLETE! üöÄ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"üéØ WALMART SALES FORECASTING - XGBOOST BASELINE\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load processed data\n",
        "print(\"üì• Loading processed datasets...\")\n",
        "train_features = pd.read_csv('train_features.csv')\n",
        "val_features = pd.read_csv('val_features.csv')\n",
        "\n",
        "# Convert dates back to datetime\n",
        "train_features['Date'] = pd.to_datetime(train_features['Date'])\n",
        "val_features['Date'] = pd.to_datetime(val_features['Date'])\n",
        "\n",
        "print(f\"Train features shape: {train_features.shape}\")\n",
        "print(f\"Validation features shape: {val_features.shape}\")\n",
        "\n",
        "print(\"\\nüìä WMAE METRIC IMPLEMENTATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def wmae(y_true, y_pred, weights):\n",
        "    \"\"\"Weighted Mean Absolute Error - Holiday weeks have 5x weight\"\"\"\n",
        "    absolute_errors = np.abs(y_true - y_pred)\n",
        "    weighted_errors = absolute_errors * weights\n",
        "    return np.sum(weighted_errors) / np.sum(weights)\n",
        "\n",
        "def calculate_weights(is_holiday):\n",
        "    \"\"\"Calculate weights: 5 for holiday weeks, 1 for normal weeks\"\"\"\n",
        "    return np.where(is_holiday, 5.0, 1.0)\n",
        "\n",
        "# Custom WMAE objective for XGBoost\n",
        "def wmae_objective(y_pred, y_true):\n",
        "    \"\"\"Custom WMAE objective function for XGBoost\"\"\"\n",
        "    # XGBoost passes DMatrix, need to get labels and weights\n",
        "    labels = y_true.get_label()\n",
        "    weights = y_true.get_weight()\n",
        "\n",
        "    # Calculate gradient (derivative of WMAE)\n",
        "    residual = y_pred - labels\n",
        "    grad = weights * np.sign(residual)\n",
        "\n",
        "    # Calculate hessian (second derivative)\n",
        "    # For MAE, hessian is 0, but XGBoost needs non-zero values\n",
        "    hess = weights * 0.01  # Small positive value\n",
        "\n",
        "    return grad, hess\n",
        "\n",
        "def wmae_eval(y_pred, y_true):\n",
        "    \"\"\"Custom WMAE evaluation function for XGBoost\"\"\"\n",
        "    labels = y_true.get_label()\n",
        "    weights = y_true.get_weight()\n",
        "\n",
        "    if weights is None:\n",
        "        weights = np.ones(len(labels))\n",
        "\n",
        "    absolute_errors = np.abs(labels - y_pred)\n",
        "    weighted_errors = absolute_errors * weights\n",
        "    wmae_score = np.sum(weighted_errors) / np.sum(weights)\n",
        "\n",
        "    return 'wmae', wmae_score\n",
        "\n",
        "print(\"‚úÖ WMAE implementation ready\")\n",
        "\n",
        "print(\"\\nüîß PREPARING DATA FOR XGBOOST\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Prepare features (exclude target and identifiers)\n",
        "exclude_cols = ['Date', 'Weekly_Sales', 'Store', 'Dept']\n",
        "feature_cols = [col for col in train_features.columns if col not in exclude_cols]\n",
        "\n",
        "# Handle categorical variables\n",
        "categorical_cols = ['Type']\n",
        "for col in categorical_cols:\n",
        "    if col in feature_cols:\n",
        "        le = LabelEncoder()\n",
        "        # Fit on combined data to ensure consistent encoding\n",
        "        combined_values = pd.concat([train_features[col], val_features[col]]).astype(str)\n",
        "        le.fit(combined_values)\n",
        "\n",
        "        train_features[col + '_encoded'] = le.transform(train_features[col].astype(str))\n",
        "        val_features[col + '_encoded'] = le.transform(val_features[col].astype(str))\n",
        "\n",
        "        feature_cols.remove(col)\n",
        "        feature_cols.append(col + '_encoded')\n",
        "\n",
        "print(f\"Total features for XGBoost: {len(feature_cols)}\")\n",
        "\n",
        "# Analyze missing values\n",
        "print(\"\\nMissing value analysis:\")\n",
        "train_missing = train_features[feature_cols].isnull().sum()\n",
        "val_missing = val_features[feature_cols].isnull().sum()\n",
        "\n",
        "print(f\"Features with missing values in train: {(train_missing > 0).sum()}\")\n",
        "print(f\"Features with missing values in validation: {(val_missing > 0).sum()}\")\n",
        "\n",
        "# XGBoost handles missing values natively, but let's see which features have them\n",
        "if (train_missing > 0).sum() > 0:\n",
        "    print(\"\\nFeatures with missing values (train):\")\n",
        "    missing_features = train_missing[train_missing > 0].sort_values(ascending=False)\n",
        "    for feat, count in missing_features.head(10).items():\n",
        "        pct = count / len(train_features) * 100\n",
        "        print(f\"  {feat}: {count:,} ({pct:.1f}%)\")\n",
        "\n",
        "print(f\"\\nUsing all training records: {len(train_features)}\")\n",
        "print(f\"Using all validation records: {len(val_features)}\")\n",
        "\n",
        "print(\"\\nüîß NAIVE BASELINE FOR COMPARISON\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "def naive_seasonal_simple(train_df, val_df):\n",
        "    \"\"\"Simple seasonal naive with fallbacks\"\"\"\n",
        "    predictions = []\n",
        "\n",
        "    # Create a mapping of store-dept to recent sales for faster lookup\n",
        "    recent_sales = {}\n",
        "    for store in train_df['Store'].unique():\n",
        "        for dept in train_df['Dept'].unique():\n",
        "            store_dept_data = train_df[(train_df['Store'] == store) & (train_df['Dept'] == dept)]\n",
        "            if len(store_dept_data) > 0:\n",
        "                recent_sales[(store, dept)] = store_dept_data['Weekly_Sales'].iloc[-4:].mean()  # Last 4 weeks avg\n",
        "\n",
        "    overall_mean = train_df['Weekly_Sales'].mean()\n",
        "\n",
        "    for _, row in val_df.iterrows():\n",
        "        store, dept = row['Store'], row['Dept']\n",
        "\n",
        "        if (store, dept) in recent_sales:\n",
        "            pred = recent_sales[(store, dept)]\n",
        "        else:\n",
        "            pred = overall_mean\n",
        "\n",
        "        predictions.append(pred)\n",
        "\n",
        "    return np.array(predictions)\n",
        "\n",
        "# Run naive baseline\n",
        "print(\"üîÑ Running naive seasonal baseline...\")\n",
        "naive_pred = naive_seasonal_simple(train_features, val_features)\n",
        "val_weights = calculate_weights(val_features['IsHoliday'].values)\n",
        "naive_wmae = wmae(val_features['Weekly_Sales'].values, naive_pred, val_weights)\n",
        "\n",
        "print(f\"Naive Seasonal WMAE: {naive_wmae:,.2f}\")\n",
        "\n",
        "print(\"\\nüîß XGBOOST MODEL TRAINING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# XGBoost parameters - more conservative than LightGBM\n",
        "xgb_params = {\n",
        "    'objective': 'reg:absoluteerror',  # MAE objective\n",
        "    'eval_metric': 'mae',\n",
        "    'max_depth': 6,\n",
        "    'learning_rate': 0.05,\n",
        "    'subsample': 0.8,\n",
        "    'colsample_bytree': 0.8,\n",
        "    'min_child_weight': 5,\n",
        "    'reg_alpha': 0.1,  # L1 regularization\n",
        "    'reg_lambda': 0.1,  # L2 regularization\n",
        "    'random_state': 42,\n",
        "    'tree_method': 'hist',  # Faster for large datasets\n",
        "    'verbosity': 0\n",
        "}\n",
        "\n",
        "print(\"üîÑ Training XGBoost model...\")\n",
        "print(f\"Features: {len(feature_cols)}\")\n",
        "print(f\"Training samples: {len(train_features)}\")\n",
        "print(f\"Validation samples: {len(val_features)}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "X_train = train_features[feature_cols]\n",
        "y_train = train_features['Weekly_Sales']\n",
        "X_val = val_features[feature_cols]\n",
        "y_val = val_features['Weekly_Sales']\n",
        "\n",
        "# Calculate weights for WMAE\n",
        "train_weights = calculate_weights(train_features['IsHoliday'].values)\n",
        "\n",
        "# Create DMatrix objects\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train, weight=train_weights)\n",
        "dval = xgb.DMatrix(X_val, label=y_val, weight=val_weights)\n",
        "\n",
        "# Train model with early stopping\n",
        "print(\"Training with weighted MAE objective...\")\n",
        "xgb_model = xgb.train(\n",
        "    xgb_params,\n",
        "    dtrain,\n",
        "    num_boost_round=500,\n",
        "    evals=[(dtrain, 'train'), (dval, 'eval')],\n",
        "    early_stopping_rounds=50,\n",
        "    verbose_eval=50,\n",
        "    feval=wmae_eval  # Custom WMAE evaluation\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Training completed. Best iteration: {xgb_model.best_iteration}\")\n",
        "\n",
        "print(\"\\nüìä MODEL EVALUATION\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Make predictions\n",
        "xgb_pred = xgb_model.predict(dval)\n",
        "\n",
        "# Calculate WMAE\n",
        "xgb_wmae = wmae(val_features['Weekly_Sales'].values, xgb_pred, val_weights)\n",
        "\n",
        "print(f\"XGBoost WMAE: {xgb_wmae:,.2f}\")\n",
        "print(f\"Naive WMAE: {naive_wmae:,.2f}\")\n",
        "print(f\"Improvement: {((naive_wmae - xgb_wmae) / naive_wmae * 100):+.1f}%\")\n",
        "\n",
        "print(\"\\nüìà FEATURE IMPORTANCE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get feature importance\n",
        "importance_dict = xgb_model.get_score(importance_type='gain')\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': list(importance_dict.keys()),\n",
        "    'importance': list(importance_dict.values())\n",
        "})\n",
        "importance_df = importance_df.sort_values('importance', ascending=False)\n",
        "\n",
        "print(\"Top 15 most important features:\")\n",
        "top_features = importance_df.head(15)\n",
        "for i, (_, row) in enumerate(top_features.iterrows(), 1):\n",
        "    print(f\"{i:2d}. {row['feature']:<25} {row['importance']:>10,.0f}\")\n",
        "\n",
        "print(\"\\nüìä PREDICTION ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Analyze predictions by holiday vs non-holiday\n",
        "holiday_mask = val_features['IsHoliday'] == True\n",
        "non_holiday_mask = val_features['IsHoliday'] == False\n",
        "\n",
        "if holiday_mask.sum() > 0:\n",
        "    holiday_mae = np.mean(np.abs(val_features.loc[holiday_mask, 'Weekly_Sales'] - xgb_pred[holiday_mask]))\n",
        "    print(f\"Holiday weeks MAE: {holiday_mae:,.2f}\")\n",
        "\n",
        "if non_holiday_mask.sum() > 0:\n",
        "    non_holiday_mae = np.mean(np.abs(val_features.loc[non_holiday_mask, 'Weekly_Sales'] - xgb_pred[non_holiday_mask]))\n",
        "    print(f\"Non-holiday weeks MAE: {non_holiday_mae:,.2f}\")\n",
        "\n",
        "# Analyze by store type\n",
        "for store_type in val_features['Type'].unique():\n",
        "    type_mask = val_features['Type'] == store_type\n",
        "    if type_mask.sum() > 0:\n",
        "        type_mae = np.mean(np.abs(val_features.loc[type_mask, 'Weekly_Sales'] - xgb_pred[type_mask]))\n",
        "        print(f\"Store Type {store_type} MAE: {type_mae:,.2f}\")\n",
        "\n",
        "print(\"\\nüíæ SAVING RESULTS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Save results\n",
        "results = {\n",
        "    'Model': ['Naive Seasonal', 'XGBoost'],\n",
        "    'WMAE': [naive_wmae, xgb_wmae],\n",
        "    'Records': [len(val_features), len(val_features)]\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('xgboost_baseline_results.csv', index=False)\n",
        "\n",
        "# Save feature importance\n",
        "importance_df.to_csv('xgboost_feature_importance.csv', index=False)\n",
        "\n",
        "# Save predictions\n",
        "predictions_df = pd.DataFrame({\n",
        "    'Store': val_features['Store'],\n",
        "    'Dept': val_features['Dept'],\n",
        "    'Date': val_features['Date'],\n",
        "    'Actual': val_features['Weekly_Sales'],\n",
        "    'IsHoliday': val_features['IsHoliday'],\n",
        "    'Type': val_features['Type'],\n",
        "    'Naive_Pred': naive_pred,\n",
        "    'XGBoost_Pred': xgb_pred,\n",
        "    'Naive_Error': np.abs(val_features['Weekly_Sales'] - naive_pred),\n",
        "    'XGBoost_Error': np.abs(val_features['Weekly_Sales'] - xgb_pred)\n",
        "})\n",
        "predictions_df.to_csv('xgboost_predictions.csv', index=False)\n",
        "\n",
        "print(\"‚úÖ Results saved:\")\n",
        "print(\"  - xgboost_baseline_results.csv\")\n",
        "print(\"  - xgboost_feature_importance.csv\")\n",
        "print(\"  - xgboost_predictions.csv\")\n",
        "\n",
        "print(f\"\\nüéØ BASELINE ESTABLISHED!\")\n",
        "print(f\"üèÜ Target to beat: {xgb_wmae:,.2f} WMAE\")\n",
        "\n",
        "# Compare with LightGBM if available\n",
        "try:\n",
        "    lgb_results = pd.read_csv('lightgbm_baseline_results.csv')\n",
        "    lgb_wmae = lgb_results[lgb_results['Model'] == 'LightGBM']['WMAE'].iloc[0]\n",
        "    print(f\"\\nüîç MODEL COMPARISON:\")\n",
        "    print(f\"XGBoost WMAE: {xgb_wmae:,.2f}\")\n",
        "    print(f\"LightGBM WMAE: {lgb_wmae:,.2f}\")\n",
        "    if xgb_wmae < lgb_wmae:\n",
        "        improvement = ((lgb_wmae - xgb_wmae) / lgb_wmae * 100)\n",
        "        print(f\"üèÜ XGBoost wins by {improvement:+.1f}%\")\n",
        "    else:\n",
        "        improvement = ((xgb_wmae - lgb_wmae) / xgb_wmae * 100)\n",
        "        print(f\"üèÜ LightGBM wins by {improvement:+.1f}%\")\n",
        "except:\n",
        "    print(\"üìù Note: Run LightGBM baseline to compare models\")\n",
        "\n",
        "print(f\"\\nüìà Next steps: Hyperparameter tuning, feature engineering, ensemble methods\")\n",
        "print(\"\\nüéâ XGBOOST BASELINE COMPLETE! üöÄ\")"
      ],
      "metadata": {
        "id": "_YeqWiCNRQc3",
        "outputId": "116bbe78-cf97-437c-93bf-e5be8d56cb9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ WALMART SALES FORECASTING - XGBOOST BASELINE\n",
            "============================================================\n",
            "üì• Loading processed datasets...\n",
            "Train features shape: (386007, 54)\n",
            "Validation features shape: (35563, 54)\n",
            "\n",
            "üìä WMAE METRIC IMPLEMENTATION\n",
            "==================================================\n",
            "‚úÖ WMAE implementation ready\n",
            "\n",
            "üîß PREPARING DATA FOR XGBOOST\n",
            "==================================================\n",
            "Total features for XGBoost: 50\n",
            "\n",
            "Missing value analysis:\n",
            "Features with missing values in train: 14\n",
            "Features with missing values in validation: 14\n",
            "\n",
            "Features with missing values (train):\n",
            "  Weekly_Sales_lag_52: 159,891 (41.4%)\n",
            "  Weekly_Sales_lag_12: 38,529 (10.0%)\n",
            "  Weekly_Sales_lag_8: 25,919 (6.7%)\n",
            "  Weekly_Sales_lag_4: 13,109 (3.4%)\n",
            "  Weekly_Sales_lag_2: 6,609 (1.7%)\n",
            "  Weekly_Sales_rolling_std_4: 6,609 (1.7%)\n",
            "  Weekly_Sales_rolling_std_26: 6,609 (1.7%)\n",
            "  Weekly_Sales_rolling_std_8: 6,609 (1.7%)\n",
            "  Weekly_Sales_rolling_std_12: 6,609 (1.7%)\n",
            "  Weekly_Sales_lag_1: 3,322 (0.9%)\n",
            "\n",
            "Using all training records: 386007\n",
            "Using all validation records: 35563\n",
            "\n",
            "üîß NAIVE BASELINE FOR COMPARISON\n",
            "==================================================\n",
            "üîÑ Running naive seasonal baseline...\n",
            "Naive Seasonal WMAE: 2,142.17\n",
            "\n",
            "üîß XGBOOST MODEL TRAINING\n",
            "==================================================\n",
            "üîÑ Training XGBoost model...\n",
            "Features: 50\n",
            "Training samples: 386007\n",
            "Validation samples: 35563\n",
            "Training with weighted MAE objective...\n",
            "[0]\ttrain-mae:13176.01628\ttrain-wmae:13176.01660\teval-mae:12848.59733\teval-wmae:12848.59570\n",
            "[50]\ttrain-mae:3076.88216\ttrain-wmae:3076.88184\teval-mae:3311.85673\teval-wmae:3311.85693\n",
            "[100]\ttrain-mae:2032.74990\ttrain-wmae:2032.74963\teval-mae:2468.36316\teval-wmae:2468.36304\n",
            "[150]\ttrain-mae:1910.60419\ttrain-wmae:1910.60425\teval-mae:2484.92475\teval-wmae:2484.92456\n",
            "[166]\ttrain-mae:1891.71532\ttrain-wmae:1891.71533\teval-mae:2491.50717\teval-wmae:2491.50708\n",
            "‚úÖ Training completed. Best iteration: 116\n",
            "\n",
            "üìä MODEL EVALUATION\n",
            "==================================================\n",
            "XGBoost WMAE: 2,491.51\n",
            "Naive WMAE: 2,142.17\n",
            "Improvement: -16.3%\n",
            "\n",
            "üìà FEATURE IMPORTANCE\n",
            "==================================================\n",
            "Top 15 most important features:\n",
            " 1. Weekly_Sales_lag_1            10,548\n",
            " 2. Weekly_Sales_rolling_mean_8      3,620\n",
            " 3. Weekly_Sales_rolling_mean_4      3,569\n",
            " 4. Weekly_Sales_rolling_mean_12        496\n",
            " 5. Weekly_Sales_rolling_mean_26        483\n",
            " 6. Weekly_Sales_lag_52              248\n",
            " 7. WeekOfYear                       219\n",
            " 8. DayOfYear                        211\n",
            " 9. Weekly_Sales_lag_2               193\n",
            "10. PostThanksgiving                 185\n",
            "11. DaysToChristmas                  177\n",
            "12. Month                            168\n",
            "13. Weekly_Sales_lag_4               148\n",
            "14. PreChristmas                     136\n",
            "15. DaysToThanksgiving               117\n",
            "\n",
            "üìä PREDICTION ANALYSIS\n",
            "==================================================\n",
            "Holiday weeks MAE: 1,973.34\n",
            "Non-holiday weeks MAE: 2,727.25\n",
            "Store Type A MAE: 3,344.50\n",
            "Store Type B MAE: 2,112.89\n",
            "Store Type C MAE: 1,375.72\n",
            "\n",
            "üíæ SAVING RESULTS\n",
            "==================================================\n",
            "‚úÖ Results saved:\n",
            "  - xgboost_baseline_results.csv\n",
            "  - xgboost_feature_importance.csv\n",
            "  - xgboost_predictions.csv\n",
            "\n",
            "üéØ BASELINE ESTABLISHED!\n",
            "üèÜ Target to beat: 2,491.51 WMAE\n",
            "\n",
            "üîç MODEL COMPARISON:\n",
            "XGBoost WMAE: 2,491.51\n",
            "LightGBM WMAE: 2,418.70\n",
            "üèÜ LightGBM wins by +2.9%\n",
            "\n",
            "üìà Next steps: Hyperparameter tuning, feature engineering, ensemble methods\n",
            "\n",
            "üéâ XGBOOST BASELINE COMPLETE! üöÄ\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}