{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:55:39.092755Z","iopub.execute_input":"2025-07-17T16:55:39.093013Z","iopub.status.idle":"2025-07-17T16:55:41.815776Z","shell.execute_reply.started":"2025-07-17T16:55:39.092992Z","shell.execute_reply":"2025-07-17T16:55:41.815012Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"! pip install pytorch-forecasting pytorch-lightning pandas numpy scikit-learn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:55:41.817482Z","iopub.execute_input":"2025-07-17T16:55:41.817808Z","iopub.status.idle":"2025-07-17T16:57:04.571919Z","shell.execute_reply.started":"2025-07-17T16:55:41.817789Z","shell.execute_reply":"2025-07-17T16:57:04.571216Z"}},"outputs":[{"name":"stdout","text":"Collecting pytorch-forecasting\n  Downloading pytorch_forecasting-1.4.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: torch!=2.0.1,<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (2.6.0+cu124)\nCollecting lightning<3.0.0,>=2.0.0 (from pytorch-forecasting)\n  Downloading lightning-2.5.2-py3-none-any.whl.metadata (38 kB)\nRequirement already satisfied: scipy<2.0,>=1.8 in /usr/local/lib/python3.11/dist-packages (from pytorch-forecasting) (1.15.3)\nRequirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\nRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\nRequirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.5.1)\nRequirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.3)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (25.0)\nRequirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.14.0)\nRequirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy) (2.4.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.12.13)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.18.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (1.3.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy) (2024.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy) (2024.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=2.0.1,<3.0.0,>=2.0.0->pytorch-forecasting) (3.0.2)\nRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\nDownloading pytorch_forecasting-1.4.0-py3-none-any.whl (260 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.9/260.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading lightning-2.5.2-py3-none-any.whl (821 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.1/821.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, lightning, pytorch-forecasting\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\nSuccessfully installed lightning-2.5.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-forecasting-1.4.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install  neuralforecast --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:04.573137Z","iopub.execute_input":"2025-07-17T16:57:04.573462Z","iopub.status.idle":"2025-07-17T16:57:12.267917Z","shell.execute_reply.started":"2025-07-17T16:57:04.573429Z","shell.execute_reply":"2025-07-17T16:57:12.267179Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.8/285.8 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import warnings\nimport numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, FunctionTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom neuralforecast import NeuralForecast\nfrom neuralforecast.models import TFT\nfrom statsmodels.tools.sm_exceptions import ValueWarning # Ensure this is handled if it's still an issue\nimport zipfile\nimport os\n\n# Suppress warnings for cleaner output in Kaggle notebooks\nwarnings.filterwarnings(\"ignore\", category=ValueWarning)\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"display.max_rows\", 100)\npd.set_option(\"display.max_columns\", None) # Display all columns\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:12.269122Z","iopub.execute_input":"2025-07-17T16:57:12.270044Z","iopub.status.idle":"2025-07-17T16:57:38.269941Z","shell.execute_reply.started":"2025-07-17T16:57:12.270002Z","shell.execute_reply":"2025-07-17T16:57:38.269334Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"KAGGLE_DATA_PATH = \"/kaggle/input/walmart-recruiting-store-sales-forecasting/\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.270647Z","iopub.execute_input":"2025-07-17T16:57:38.270898Z","iopub.status.idle":"2025-07-17T16:57:38.274825Z","shell.execute_reply.started":"2025-07-17T16:57:38.270869Z","shell.execute_reply":"2025-07-17T16:57:38.274072Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def calculate_wmae(y_true, y_pred, is_holiday_flag, holiday_weight=5.0):\n    \n    abs_errors = np.abs(y_true - y_pred)\n    weights = np.where(is_holiday_flag.astype(bool), holiday_weight, 1.0)\n    wmae = np.sum(weights * abs_errors) / np.sum(weights)\n    return wmae","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.275831Z","iopub.execute_input":"2025-07-17T16:57:38.276198Z","iopub.status.idle":"2025-07-17T16:57:38.591498Z","shell.execute_reply.started":"2025-07-17T16:57:38.276168Z","shell.execute_reply":"2025-07-17T16:57:38.590734Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class DateFeatureCreator(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        if \"Date\" not in X.columns:\n            raise ValueError(\"DateFeatureCreator requires 'Date' column in input X.\")\n            \n        # Ensure 'Date' is datetime type before operations\n        if not pd.api.types.is_datetime64_any_dtype(X['Date']):\n            X['Date'] = pd.to_datetime(X['Date'])\n\n        # Using to_period('W') and then converting to integer week number\n        # rank(method=\"dense\") ensures consecutive integers for weeks\n        X[\"week\"] = (X[\"Date\"].dt.to_period(\"W\").rank(method=\"dense\").astype(int) - 1)\n        \n        # Cyclical features for different periodicities\n        X[\"sin_13\"] = np.sin(2 * np.pi * X[\"week\"] / 13) # Roughly quarterly seasonality\n        X[\"cos_13\"] = np.cos(2 * np.pi * X[\"week\"] / 13)\n        X[\"sin_23\"] = np.sin(2 * np.pi * X[\"week\"] / 23) # A different, less common periodicity\n        X[\"cos_23\"] = np.cos(2 * np.pi * X[\"week\"] / 23)\n        \n        # Drop the original 'Date' column as its information is now in cyclical features\n        X = X.drop(columns=[\"Date\"], errors='ignore')\n        return X\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.593644Z","iopub.execute_input":"2025-07-17T16:57:38.594033Z","iopub.status.idle":"2025-07-17T16:57:38.612956Z","shell.execute_reply.started":"2025-07-17T16:57:38.594013Z","shell.execute_reply":"2025-07-17T16:57:38.612329Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class ColumnDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, columns):\n        self.columns = columns\n\n    def fit(self, X, y=None):\n        return self\n        \n    def transform(self, X):\n        return X.drop(columns=self.columns, errors=\"ignore\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.614055Z","iopub.execute_input":"2025-07-17T16:57:38.614325Z","iopub.status.idle":"2025-07-17T16:57:38.628709Z","shell.execute_reply.started":"2025-07-17T16:57:38.614303Z","shell.execute_reply":"2025-07-17T16:57:38.628099Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class ColumnTransformerWithNames(ColumnTransformer):\n    \"\"\"\n    A wrapper around ColumnTransformer to retain column names and return a DataFrame.\n    Handles OneHotEncoder output specifically.\n    \"\"\"\n    def __init__(self, transformers, remainder='drop'): \n        super().__init__(transformers=transformers, remainder=remainder) \n        self.output_columns_ = None\n\n    def fit(self, X, y=None):\n        super().fit(X, y)\n        self.output_columns_ = self._get_feature_names_out_internal(X)\n        return self\n\n    def _get_feature_names_out_internal(self, X):\n        column_names = []\n        for name, transformer, columns in self.transformers_:\n            if transformer == 'drop':\n                continue\n            elif transformer == 'passthrough':\n                # Ensure passthrough columns are correctly identified from original X\n                if isinstance(columns, str):\n                    column_names.append(columns)\n                else:\n                    column_names.extend(list(columns))\n            else:\n                if hasattr(transformer, 'get_feature_names_out'):\n                    if isinstance(columns, str): \n                        col_names = [columns]\n                    else:\n                        col_names = list(columns)\n                    column_names.extend(list(transformer.get_feature_names_out(col_names)))\n                else:\n                    if isinstance(columns, str): # Fallback for transformers without get_feature_names_out\n                        column_names.append(columns)\n                    else:\n                        column_names.extend(list(columns))\n        return column_names\n\n    def transform(self, X):\n        transformed_array = super().transform(X)\n        if self.output_columns_ is None:\n             raise RuntimeError(\"ColumnTransformerWithNames must be fitted before transform.\")\n        \n        # Convert to dense array if it's a sparse matrix (older sklearn default for OHE)\n        if hasattr(transformed_array, 'toarray'):\n            transformed_array = transformed_array.toarray()\n\n        # Ensure that the index is preserved from the input X\n        # This is CRITICAL for maintaining alignment with y\n        return pd.DataFrame(transformed_array, index=X.index, columns=self.output_columns_)\n\n    def fit_transform(self, X, y=None):\n        transformed_array = super().fit_transform(X, y)\n        self.output_columns_ = self._get_feature_names_out_internal(X)\n        \n        # Convert to dense array if it's a sparse matrix (older sklearn default for OHE)\n        if hasattr(transformed_array, 'toarray'):\n            transformed_array = transformed_array.toarray()\n\n        # Ensure that the index is preserved from the input X\n        # This is CRITICAL for maintaining alignment with y\n        return pd.DataFrame(transformed_array, index=X.index, columns=self.output_columns_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.629533Z","iopub.execute_input":"2025-07-17T16:57:38.629733Z","iopub.status.idle":"2025-07-17T16:57:38.645934Z","shell.execute_reply.started":"2025-07-17T16:57:38.629717Z","shell.execute_reply":"2025-07-17T16:57:38.645373Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"class MultiIndexKeeper(BaseEstimator, TransformerMixin):\n    def __init__(self, index_cols=[\"Date\", \"Store\", \"Dept\"]):\n        self.index_cols = index_cols\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        \n        if 'Date' in X.columns and not pd.api.types.is_datetime64_any_dtype(X['Date']):\n            X['Date'] = pd.to_datetime(X['Date'])\n            \n        missing_cols = [col for col in self.index_cols if col not in X.columns]\n        if missing_cols:\n            raise ValueError(f\"MultiIndexKeeper: Missing columns in input X: {missing_cols}\")\n            \n        # IMPORTANT: When setting index, ensure 'Date', 'Store', 'Dept' columns\n        # are not dropped, as they are needed later by NeuralForecast as covariates.\n        # This is already handled by drop=False.\n        X.set_index(self.index_cols, drop=False, inplace=True)\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.646656Z","iopub.execute_input":"2025-07-17T16:57:38.646873Z","iopub.status.idle":"2025-07-17T16:57:38.665331Z","shell.execute_reply.started":"2025-07-17T16:57:38.646858Z","shell.execute_reply":"2025-07-17T16:57:38.664584Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class TFTRegressor(BaseEstimator, RegressorMixin):\n    def __init__(self, input_chunk_length=52, output_chunk_length=39, epochs=25, batch_size=32, random_seed=42):\n        self.input_chunk_length = input_chunk_length\n        self.output_chunk_length = output_chunk_length\n        self.epochs = epochs\n        self.batch_size = batch_size\n        self.random_seed = random_seed\n        self.nf_ = None\n        self.model_ = None\n        self.trained_df_ = None # Store the DataFrame used for training\n\n    def fit(self, X, y):\n        # Ensure y has a name for proper merging if it doesn't already\n        if y.name is None:\n            y.name = 'y' \n\n        y_multiindexed = pd.Series(y.values, index=X.index, name='y')\n\n        df = X.copy() \n        df['y'] = y_multiindexed \n\n        if not pd.api.types.is_datetime64_any_dtype(df.index.get_level_values('Date')):\n            raise ValueError(\"MultiIndex 'Date' level is not datetime type. Ensure MultiIndexKeeper makes it datetime.\")\n        df['ds'] = df.index.get_level_values('Date') \n        \n        df[\"unique_id\"] = df[\"Store\"].astype(str) + \"_\" + df[\"Dept\"].astype(str) \n\n        # Store the prepared DataFrame for use in predict\n        self.trained_df_ = df.copy() # Store the full df (including y and features)\n\n        # --- DEBUGGING STEP: Check for NaNs immediately before NeuralForecast fit ---\n        nan_check = df.isnull().sum()\n        cols_with_nans = nan_check[nan_check > 0].index.tolist()\n        if cols_with_nans:\n            print(f\"DEBUG: Found NaNs in the following columns before NeuralForecast fit: {cols_with_nans}\")\n            # Print head including index for better context\n            print(df.loc[df[cols_with_nans[0]].isnull(), cols_with_nans].head())\n            raise ValueError(f\"Found missing values in {cols_with_nans}.\")\n        # --- END DEBUGGING STEP ---\n\n        self.model_ = TFT(\n            h=self.output_chunk_length,\n            input_size=self.input_chunk_length,\n            batch_size=self.batch_size,\n            random_seed=self.random_seed,\n        )\n\n        self.nf_ = NeuralForecast(models=[self.model_], freq=\"W-FRI\")\n\n        self.nf_.fit(df=df)\n        return self\n\n    def predict(self, X):\n        # X here is the transformed X_val from the pipeline, with MultiIndex ('Date', 'Store', 'Dept')\n        \n        # 1. Prepare future covariates from X (which is X_val after preprocessing)\n        df_future_covariates_raw = X.copy() # This X contains all features for the validation period\n        \n        if not pd.api.types.is_datetime64_any_dtype(df_future_covariates_raw.index.get_level_values('Date')):\n            raise ValueError(\"MultiIndex 'Date' level is not datetime type in predict. Ensure MultiIndexKeeper makes it datetime.\")\n        \n        df_future_covariates_raw['ds'] = df_future_covariates_raw.index.get_level_values('Date')\n        df_future_covariates_raw[\"unique_id\"] = df_future_covariates_raw[\"Store\"].astype(str) + \"_\" + df_future_covariates_raw[\"Dept\"].astype(str)\n\n        # Identify all covariate columns (all columns in self.trained_df_ except 'ds', 'unique_id', 'y')\n        covariate_cols = [col for col in self.trained_df_.columns if col not in ['ds', 'unique_id', 'y']]\n\n        # Select only the relevant future covariate columns and the required 'ds', 'unique_id'\n        df_future_covariates_selected = df_future_covariates_raw[['ds', 'unique_id'] + covariate_cols].copy()\n        \n        # 2. Generate the full expected future dataframe for all series for the forecast horizon\n        expected_future_df_template = self.nf_.make_future_dataframe(self.trained_df_)\n        \n        # 3. Merge the generated template with our actual future covariates (X_val)\n        futr_df_complete = pd.merge(\n            expected_future_df_template,\n            df_future_covariates_selected,\n            on=['unique_id', 'ds'],\n            how='left'\n        )\n        \n        nan_check_futr = futr_df_complete.isnull().sum()\n        cols_with_nans_futr = nan_check_futr[nan_check_futr > 0].index.tolist()\n        if cols_with_nans_futr:\n            print(f\"DEBUG: Found NaNs in futr_df_complete for columns: {cols_with_nans_futr}. Filling with 0.\")\n            futr_df_complete[cols_with_nans_futr] = futr_df_complete[cols_with_nans_futr].fillna(0)\n\n        # 4. Perform the prediction\n        # Ensure that `df` has all required unique_ids and `futr_df` aligns.\n        # This is the point where the model generates predictions.\n        forecast_df = self.nf_.predict(df=self.trained_df_, futr_df=futr_df_complete) \n        \n        forecast_df = forecast_df.rename(columns={'TFT': 'yhat'})\n\n        if 'unique_id' in forecast_df.columns:\n            forecast_df[['Store', 'Dept']] = forecast_df['unique_id'].str.split('_', expand=True)\n            forecast_df['Store'] = forecast_df['Store'].astype(float).astype(int)\n            forecast_df['Dept'] = forecast_df['Dept'].astype(float).astype(int)\n\n        forecast_df['Date'] = pd.to_datetime(forecast_df['ds']) \n        \n        # Ensure the index columns are correct before setting the index\n        # Also, make sure that 'Store' and 'Dept' are properly integer type before setting multi-index\n        forecast_df_indexed = forecast_df.set_index(['Date', 'Store', 'Dept'])[['yhat']]\n\n        # This is where NaNs can be introduced if forecast_df_indexed doesn't cover all X.index\n        final_predictions = forecast_df_indexed.reindex(X.index)\n\n        y_pred = final_predictions['yhat'].values.flatten() \n\n        # FIX: Fill any NaNs in the final predictions array with 0 before evaluation\n        if np.isnan(y_pred).any():\n            print(\"DEBUG: Found NaNs in final y_pred after reindex. Filling with 0.\")\n            y_pred = np.nan_to_num(y_pred, nan=0.0)\n\n        y_pred[y_pred < 0] = 0\n\n        return y_pred","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.666212Z","iopub.execute_input":"2025-07-17T16:57:38.666477Z","iopub.status.idle":"2025-07-17T16:57:38.682980Z","shell.execute_reply.started":"2025-07-17T16:57:38.666459Z","shell.execute_reply":"2025-07-17T16:57:38.682335Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def main():\n    print(\"🚀 Starting Walmart Sales Forecasting with Temporal Fusion Transformer on Kaggle\")\n    print(\"=\" * 80)\n\n    try:\n        # --- 1. Data Loading (Kaggle specific path) ---\n        print(\"📊 Loading datasets from Kaggle input path...\")\n        \n        train_zip_path = os.path.join(KAGGLE_DATA_PATH, 'train.csv.zip')\n        features_zip_path = os.path.join(KAGGLE_DATA_PATH, 'features.csv.zip')\n        stores_csv_path = os.path.join(KAGGLE_DATA_PATH, 'stores.csv') \n\n        # --- UNZIP THE FILES ---\n        print(\"   📂 Unzipping necessary data files...\")\n        with zipfile.ZipFile(train_zip_path, 'r') as zip_ref:\n            zip_ref.extractall('.') \n        print(f\"      - Extracted: {train_zip_path}\")\n        \n        with zipfile.ZipFile(features_zip_path, 'r') as zip_ref:\n            zip_ref.extractall('.') \n        print(f\"      - Extracted: {features_zip_path}\")\n\n        # Now, load the unzipped CSVs (they will be in the current directory, '/kaggle/working/')\n        train_df = pd.read_csv('train.csv')\n        features_df = pd.read_csv('features.csv')\n        stores_df = pd.read_csv(stores_csv_path) \n\n        # Convert Date columns to datetime early for consistency\n        train_df['Date'] = pd.to_datetime(train_df['Date'])\n        features_df['Date'] = pd.to_datetime(features_df['Date'])\n\n        print(f\"   📈 Train data: {train_df.shape}\")\n        print(f\"   📊 Features data: {features_df.shape}\")\n        print(f\"   🏪 Stores data: {stores_df.shape}\")\n\n        # --- 2. Data Merging and Initial Cleaning ---\n        print(\"\\n🧹 Merging data and initial cleaning...\")\n        merged_df = pd.merge(train_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n        train_full = pd.merge(merged_df, stores_df, on=['Store'], how='left')\n\n        # Fill NaN in MarkDown columns with 0, assuming no markdown if not specified\n        markdown_cols = [f'MarkDown{i}' for i in range(1, 6)]\n        for col in markdown_cols:\n            if col in train_full.columns:\n                train_full[col] = train_full[col].fillna(0)\n\n        # Remove rows with negative Weekly_Sales\n        initial_rows = len(train_full)\n        train_full = train_full[train_full['Weekly_Sales'] > 0]\n        print(f\"   🗑️ Removed {initial_rows - len(train_full)} rows with negative Weekly_Sales.\")\n\n        # --- Define column lists here, BEFORE their use ---\n        numerical_cols = ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment'] + [f'MarkDown{i}' for i in range(1, 6)]\n        categorical_ohe_cols = [\"Type\", \"IsHoliday\"] \n        passthrough_cols = [\"Store\", \"Dept\"] \n        # --- End of column definitions ---\n\n\n        # --- NEW FIX: Ensure continuous series and fill NaNs for NeuralForecast ---\n        print(\"   Filling missing dates and sales for time series continuity...\")\n        \n        # Create a full set of (Store, Dept, Date) unique combinations\n        unique_store_dept_dates = train_full[['Store', 'Dept', 'Date']].drop_duplicates()\n\n        # Generate all expected dates for each (Store, Dept)\n        df_list = []\n        for (store, dept), group in unique_store_dept_dates.groupby(['Store', 'Dept']):\n            series_min_date = group['Date'].min()\n            series_max_date = group['Date'].max()\n            full_series_dates = pd.date_range(start=series_min_date, end=series_max_date, freq='W-FRI')\n            \n            temp_df = pd.DataFrame({\n                'Store': store,\n                'Dept': dept,\n                'Date': full_series_dates\n            })\n            df_list.append(temp_df)\n        \n        complete_series_df = pd.concat(df_list, ignore_index=True)\n        \n        # Merge the complete series dates with the original train_full data\n        train_full_cleaned = pd.merge(\n            complete_series_df,\n            train_full,\n            on=['Store', 'Dept', 'Date'],\n            how='left'\n        )\n        \n        # Now, fill NaNs in 'Weekly_Sales' with 0 (as sales cannot be truly missing here)\n        nan_sales_before_fill = train_full_cleaned['Weekly_Sales'].isnull().sum()\n        train_full_cleaned['Weekly_Sales'] = train_full_cleaned['Weekly_Sales'].fillna(0)\n        print(f\"   Filled {nan_sales_before_fill} NaN Weekly_Sales values with 0 for series continuity.\")\n\n        # Re-merge the original features_df and stores_df to fill in associated data\n        train_full_cleaned = pd.merge(train_full_cleaned, features_df.drop(columns=['IsHoliday'], errors='ignore'), on=['Store', 'Date'], how='left', suffixes=('', '_feats'))\n        train_full_cleaned = pd.merge(train_full_cleaned, stores_df, on=['Store'], how='left', suffixes=('', '_stores'))\n\n        # Combine IsHoliday if it was duplicated, prioritize original if available, otherwise features_df\n        if 'IsHoliday_feats' in train_full_cleaned.columns:\n            train_full_cleaned['IsHoliday'] = train_full_cleaned['IsHoliday'].fillna(train_full_cleaned['IsHoliday_feats'])\n            train_full_cleaned = train_full_cleaned.drop(columns=['IsHoliday_feats'])\n        \n        # Now fill NaNs in features columns (numerical and categorical where appropriate)\n        # Using groupby transform for numerical, then a general fill for any remaining\n        for col in numerical_cols: \n            if col in train_full_cleaned.columns:\n                train_full_cleaned[col] = train_full_cleaned.groupby(['Store', 'Dept'])[col].transform(lambda x: x.fillna(x.mean()))\n                train_full_cleaned[col] = train_full_cleaned[col].fillna(train_full_cleaned[col].mean()) # Fill any remaining with global mean\n\n        # Ensure no negative sales after any filling process (though filling with 0 should prevent this)\n        train_full_cleaned['Weekly_Sales'][train_full_cleaned['Weekly_Sales'] < 0] = 0\n\n        # Sort by date, store, and department for time series consistency (important for NeuralForecast)\n        train_full = train_full_cleaned.sort_values(by=['Date', 'Store', 'Dept']).reset_index(drop=True)\n\n        print(f\"   ✅ Merged and cleaned data: {train_full.shape}\")\n        print(f\"   📅 Date range: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n        print(f\"   Sanity check: NaNs in Weekly_Sales after cleaning: {train_full['Weekly_Sales'].isnull().sum()}\")\n\n\n        # --- 3. Data Splitting (80/20 Time-based) ---\n        print(\"\\n📅 Step 1: Creating temporal split (80/20)...\")\n        \n        df_sorted = train_full.sort_values('Date').reset_index(drop=True)\n\n        unique_dates = sorted(df_sorted['Date'].unique())\n        total_weeks = len(unique_dates)\n        train_ratio = 0.8 \n        train_weeks = int(total_weeks * train_ratio)\n\n        if train_weeks < 1:\n            train_weeks = 1\n        if train_weeks >= total_weeks:\n            train_weeks = total_weeks - 1 \n\n        split_date = unique_dates[train_weeks - 1] \n\n        X_train = df_sorted[df_sorted['Date'] <= split_date].drop(columns=['Weekly_Sales']).copy()\n        y_train = df_sorted[df_sorted['Date'] <= split_date]['Weekly_Sales'].copy()\n        X_val = df_sorted[df_sorted['Date'] > split_date].drop(columns=['Weekly_Sales']).copy()\n        y_val = df_sorted[df_sorted['Date'] > split_date]['Weekly_Sales'].copy()\n\n        print(f\"   📊 Split date: {split_date}\")\n        print(f\"   📈 Train: {len(X_train):,} records ({X_train['Date'].min()} to {X_train['Date'].max()})\")\n        print(f\"   📉 Val: {len(X_val):,} records ({X_val['Date'].min()} to {X_val['Date'].max()})\")\n\n        # --- 4. Pipeline Definition ---\n        print(\"\\n⚙️ Step 2: Defining preprocessing and TFT pipeline...\")\n\n        numerical_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='mean'))\n        ])\n\n        categorical_transformer = Pipeline(steps=[\n            ('imputer', SimpleImputer(strategy='most_frequent')),\n            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n        ])\n\n        preprocessor = ColumnTransformerWithNames(transformers=[\n            ('num', numerical_transformer, numerical_cols),\n            ('cat', categorical_transformer, categorical_ohe_cols),\n            ('pass', 'passthrough', passthrough_cols) \n        ], remainder='drop') \n\n        pipeline = Pipeline([\n            (\"multi_index_keeper\", MultiIndexKeeper(index_cols=[\"Date\", \"Store\", \"Dept\"])),\n            (\"date_feature_creator\", DateFeatureCreator()), \n            (\"preprocessor\", preprocessor), \n            (\"tft_regressor\", TFTRegressor(input_chunk_length=52, output_chunk_length=39, epochs=25, batch_size=32, random_seed=42))\n        ])\n\n        print(\"   ✅ Pipeline defined.\")\n\n        # --- 5. Model Training ---\n        print(\"\\n🧠 Step 3: Training TFT model...\")\n        pipeline.fit(X_train, y_train)\n        print(\"   ✅ TFT Model Training Complete!\")\n\n        # --- 6. Model Evaluation ---\n        print(\"\\n📊 Step 4: Evaluating model on validation set...\")\n        y_pred_val = pipeline.predict(X_val)\n\n        y_pred_val[y_pred_val < 0] = 0\n\n        is_holiday_val = X_val['IsHoliday'].values.astype(bool)\n\n        if len(y_val) > 0:\n            val_mae = mean_absolute_error(y_val, y_pred_val)\n            val_rmse = np.sqrt(mean_squared_error(y_val, y_pred_val))\n            val_wmae = calculate_wmae(y_val, y_pred_val, is_holiday_val)\n        else:\n            val_mae, val_rmse, val_wmae = 0, 0, 0\n            print(\"   ⚠️ Warning: No data points for evaluation. Metrics set to 0.\")\n\n        holiday_mask_val = is_holiday_val.astype(bool)\n        holiday_mae_val = mean_absolute_error(y_val[holiday_mask_val], y_pred_val[holiday_mask_val]) if holiday_mask_val.any() else np.nan\n        non_holiday_mae_val = mean_absolute_error(y_val[~holiday_mask_val], y_pred_val[~holiday_mask_val]) if (~holiday_mask_val).any() else np.nan\n\n        print(\"\\n\" + \"=\" * 60)\n        print(\"🎯 EXPERIMENT TFT RESULTS SUMMARY\")\n        print(\"=\" * 60)\n\n        print(\"\\n📊 Validation Metrics:\")\n        print(f\"   WMAE (Competition Metric): ${val_wmae:,.2f}\")\n        print(f\"   MAE: ${val_mae:,.2f}\")\n        print(f\"   RMSE: ${val_rmse:,.2f}\")\n\n        print(\"\\n📊 Holiday Breakdown:\")\n        print(f\"   Holiday MAE: ${holiday_mae_val:,.2f} ({int(holiday_mask_val.sum()):,} samples)\")\n        print(f\"   Non-Holiday MAE: ${non_holiday_mae_val:,.2f} ({int((~holiday_mask_val).sum()):,} samples)\")\n\n        print(\"\\n🎉 Experiment TFT: Complete!\")\n\n    except Exception as e:\n        print(f\"❌ Experiment failed: {e}\")\n        raise","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.683826Z","iopub.execute_input":"2025-07-17T16:57:38.684123Z","iopub.status.idle":"2025-07-17T16:57:38.709093Z","shell.execute_reply.started":"2025-07-17T16:57:38.684100Z","shell.execute_reply":"2025-07-17T16:57:38.708512Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T16:57:38.709805Z","iopub.execute_input":"2025-07-17T16:57:38.710085Z","iopub.status.idle":"2025-07-17T17:01:51.784738Z","shell.execute_reply.started":"2025-07-17T16:57:38.710064Z","shell.execute_reply":"2025-07-17T17:01:51.784027Z"}},"outputs":[{"name":"stdout","text":"🚀 Starting Walmart Sales Forecasting with Temporal Fusion Transformer on Kaggle\n================================================================================\n📊 Loading datasets from Kaggle input path...\n   📂 Unzipping necessary data files...\n      - Extracted: /kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n      - Extracted: /kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n   📈 Train data: (421570, 5)\n   📊 Features data: (8190, 12)\n   🏪 Stores data: (45, 3)\n\n🧹 Merging data and initial cleaning...\n   🗑️ Removed 1358 rows with negative Weekly_Sales.\n   Filling missing dates and sales for time series continuity...\n   Filled 26507 NaN Weekly_Sales values with 0 for series continuity.\n   ✅ Merged and cleaned data: (446719, 27)\n   📅 Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n   Sanity check: NaNs in Weekly_Sales after cleaning: 0\n\n📅 Step 1: Creating temporal split (80/20)...\n   📊 Split date: 2012-04-06 00:00:00\n   📈 Train: 357,562 records (2010-02-05 00:00:00 to 2012-04-06 00:00:00)\n   📉 Val: 89,157 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n\n⚙️ Step 2: Defining preprocessing and TFT pipeline...\n   ✅ Pipeline defined.\n\n🧠 Step 3: Training TFT model...\n","output_type":"stream"},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\nINFO: ----------------------------------------------------------------------------------------------------\ndistributed_backend=nccl\nAll distributed processes registered. Starting with 2 processes\n----------------------------------------------------------------------------------------------------\n\n2025-07-17 16:57:58.839500: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752771479.222730     116 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752771479.346429     116 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebfe56291d7d402295c9fb805223edcf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validation: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"INFO: `Trainer.fit` stopped: `max_steps=1000` reached.\n","output_type":"stream"},{"name":"stdout","text":"   ✅ TFT Model Training Complete!\n\n📊 Step 4: Evaluating model on validation set...\nDEBUG: Found NaNs in futr_df_complete for columns: ['Temperature', 'Fuel_Price', 'CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'Type_A', 'Type_B', 'Type_C', 'IsHoliday_False', 'IsHoliday_True', 'Store', 'Dept']. Filling with 0.\n","output_type":"stream"},{"name":"stderr","text":"INFO: GPU available: True (cuda), used: True\nINFO: TPU available: False, using: 0 TPU cores\nINFO: HPU available: False, using: 0 HPUs\n2025-07-17 17:01:47.064320: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752771707.082298      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752771707.087766      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Predicting: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8971b5b2d2c49b5952233c0ec06dab1"}},"metadata":{}},{"name":"stdout","text":"DEBUG: Found NaNs in final y_pred after reindex. Filling with 0.\n\n============================================================\n🎯 EXPERIMENT TFT RESULTS SUMMARY\n============================================================\n\n📊 Validation Metrics:\n   WMAE (Competition Metric): $1,421.02\n   MAE: $1,585.18\n   RMSE: $3,415.59\n\n📊 Holiday Breakdown:\n   Holiday MAE: $868.07 (6,617 samples)\n   Non-Holiday MAE: $1,642.66 (82,540 samples)\n\n🎉 Experiment TFT: Complete!\n","output_type":"stream"}],"execution_count":13}]}