{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting/blob/main/feature_importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMZqRTtCY31E",
        "outputId": "49a61d9b-4bd4-4f7c-a394-36d329d36de7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m117.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "foFWtundZCTx"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CqdtFSAaZDlt"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oyi1Zq8cZFIW"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWwWkCerZGX1",
        "outputId": "2a1afd62-1cc9-4fbb-8852-3ce81c893fd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 1.02GB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrKcnvgKZH_b",
        "outputId": "54de41d5-de1d-4662-996d-bdcef0605d72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ],
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qBXkuKkCZJvL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "import subprocess\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08iOjzo_ZYhR",
        "outputId": "3764aa05-0bf5-4c76-ad94-3e0c00248447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install prophet plotly mlflow dagshub xgboost -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mT-I9HzZZRg",
        "outputId": "a31a3b31-0e33-454f-a072-dc24454045f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… All libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "print(\"âœ… All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kDwu85PQZ9Xw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "import dagshub\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import warnings\n",
        "import os\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n"
      ],
      "metadata": {
        "id": "UZbD0Q8jVBU8"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartPreprocessingPipeline:\n",
        "    def __init__(self, train_date_split='2012-04-13', remove_outliers=True, enable_lag_features=False):\n",
        "        self.train_date_split = pd.to_datetime(train_date_split)\n",
        "        self.remove_outliers = remove_outliers\n",
        "        self.enable_lag_features = enable_lag_features\n",
        "        self.weekly_sales_outlier_thresholds = {\n",
        "            'A': {'lower': -20000, 'upper': 75000},\n",
        "            'B': {'lower': -10000, 'upper': 40000},\n",
        "            'C': {'lower': -5000, 'upper': 20000}\n",
        "        }\n",
        "        self.outliers_removed_count = 0\n",
        "\n",
        "    def fit_transform(self, train_df, features_df, stores_df):\n",
        "        \"\"\"\n",
        "        Main method to merge, clean, engineer features, and prepare data.\n",
        "        MarkDown features are now INCLUDED.\n",
        "        \"\"\"\n",
        "        print(\"ğŸ“Š Starting data preprocessing...\")\n",
        "        df = self._merge_data(train_df, features_df, stores_df)\n",
        "        df = self._clean_data(df)\n",
        "        df = self._feature_engineer_dates(df)\n",
        "        df = self._feature_engineer_holidays(df)\n",
        "        df = self._feature_engineer_store_type(df)\n",
        "        df = self._handle_markdown_nans(df) # NEW: Handle NaNs in MarkDowns\n",
        "        if self.remove_outliers:\n",
        "            df = self._remove_outliers(df)\n",
        "        if not self.enable_lag_features:\n",
        "            df = self._remove_lag_features(df) # This will no longer remove MarkDowns\n",
        "        df = self._handle_missing_values(df) # General NaN handling\n",
        "        df = self._remove_redundant_features(df)\n",
        "        print(\"âœ… Data preprocessing complete.\")\n",
        "        return df\n",
        "\n",
        "    def _merge_data(self, train_df, features_df, stores_df):\n",
        "        print(\"   Merging datasets...\")\n",
        "        df = pd.merge(train_df, stores_df, on='Store', how='left')\n",
        "        df = pd.merge(df, features_df, on=['Store', 'Date'], how='left')\n",
        "        print(f\"   Merged data shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def _clean_data(self, df):\n",
        "        print(\"   Cleaning merged data (handling IsHoliday duplicates)...\")\n",
        "        if 'IsHoliday_x' in df.columns and 'IsHoliday_y' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday_x'] | df['IsHoliday_y']\n",
        "            df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(bool)\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_dates(self, df):\n",
        "        print(\"   Creating date features...\")\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int)\n",
        "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "        df['IsMonthStart'] = (df['Date'].dt.day == 1).astype(int)\n",
        "        df['IsMonthEnd'] = (df['Date'].dt.is_month_end).astype(int)\n",
        "\n",
        "        min_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - min_date).dt.days\n",
        "        df['WeeksFromStart'] = (df['DaysFromStart'] // 7).astype(int)\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_holidays(self, df):\n",
        "        print(\"   Creating holiday features...\")\n",
        "        holidays = {\n",
        "            'SuperBowl': [datetime(y, 2, d) for y in range(2010, 2013) for d in [7, 8, 9, 10, 11, 12, 13]],\n",
        "            'LaborDay': [datetime(y, 9, d) for y in range(2010, 2013) for d in [1, 2, 3, 4, 5, 6, 7]],\n",
        "            'Thanksgiving': [datetime(y, 11, d) for y in range(2010, 2013) for d in [22, 23, 24, 25, 26, 27, 28]],\n",
        "            'Christmas': [datetime(y, 12, d) for y in range(2010, 2013) for d in [24, 25, 26, 27, 28, 29, 30, 31]]\n",
        "        }\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].isin(holidays['SuperBowl']).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].isin(holidays['LaborDay']).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].isin(holidays['Thanksgiving']).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].isin(holidays['Christmas']).astype(int)\n",
        "\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] | df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12, 1, 2, 9]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8]).astype(int)\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_store_type(self, df):\n",
        "        print(\"   Encoding store Type...\")\n",
        "        df['Type_Encoded'] = df['Type'].astype('category').cat.codes\n",
        "        df = pd.get_dummies(df, columns=['Type'], prefix='Type', drop_first=False)\n",
        "        return df\n",
        "\n",
        "    def _handle_markdown_nans(self, df):\n",
        "        \"\"\"\n",
        "        Handles NaN values in MarkDown columns.\n",
        "        MarkDowns are often NaN when no markdown was applied.\n",
        "        Filling with 0 is a common strategy.\n",
        "        \"\"\"\n",
        "        print(\"   Handling NaNs in MarkDown columns (filling with 0)...\")\n",
        "        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n",
        "        for col in markdown_cols:\n",
        "            df[col] = df[col].fillna(0)\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df):\n",
        "        print(\"   Removing outliers from Weekly_Sales based on store type...\")\n",
        "        initial_rows = len(df)\n",
        "        df_cleaned = pd.DataFrame()\n",
        "        for store_type in self.weekly_sales_outlier_thresholds.keys():\n",
        "            if f'Type_{store_type}' in df.columns:\n",
        "                subset = df[df[f'Type_{store_type}'] == 1].copy()\n",
        "                lower = self.weekly_sales_outlier_thresholds[store_type]['lower']\n",
        "                upper = self.weekly_sales_outlier_thresholds[store_type]['upper']\n",
        "                subset = subset[(subset['Weekly_Sales'] >= lower) & (subset['Weekly_Sales'] <= upper)]\n",
        "                df_cleaned = pd.concat([df_cleaned, subset])\n",
        "            else:\n",
        "                pass # If Type_A, Type_B, or Type_C don't exist\n",
        "\n",
        "        if not df_cleaned.empty:\n",
        "            self.outliers_removed_count = initial_rows - len(df_cleaned)\n",
        "            if self.outliers_removed_count > 0:\n",
        "                print(f\"   ğŸ—‘ï¸ Removed {self.outliers_removed_count} outliers.\")\n",
        "            return df_cleaned\n",
        "        else:\n",
        "            print(\"   Warning: Outlier removal resulted in an empty DataFrame. Returning original DataFrame.\")\n",
        "            return df\n",
        "\n",
        "    def _remove_lag_features(self, df):\n",
        "        # This method is kept for consistency with previous pipelines,\n",
        "        # but since MarkDowns are now included, it won't remove them.\n",
        "        # If any other specific 'lagged_sales' features were created, they'd be removed here.\n",
        "        return df\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        print(\"   Handling remaining missing values (filling numerical NaNs with 0)...\")\n",
        "        numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "        df[numerical_cols] = df[numerical_cols].fillna(0)\n",
        "        return df\n",
        "\n",
        "    def _remove_redundant_features(self, df):\n",
        "        print(\"   Removing redundant features...\")\n",
        "        cols_to_drop = [\n",
        "            'Year', # Often redundant with other temporal features\n",
        "            'Day',  # Covered by DayOfWeek, Month\n",
        "            # 'Size' is kept as it's a useful feature\n",
        "        ]\n",
        "        return df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n"
      ],
      "metadata": {
        "id": "GCSFHyejVDlN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution for Feature Importance Analysis ---\n",
        "def main():\n",
        "    print(\"ğŸš€ Starting Feature Importance Analysis with MarkDowns...\")\n",
        "\n",
        "    # Load data (assuming data is unzipped in the current directory)\n",
        "    print(\"Loading raw data...\")\n",
        "    # Ensure raw data files (train.csv, features.csv, stores.csv) are in the same directory\n",
        "    # or provide the correct paths.\n",
        "    try:\n",
        "        train_raw = pd.read_csv('train.csv')\n",
        "        features_raw = pd.read_csv('features.csv')\n",
        "        stores_raw = pd.read_csv('stores.csv')\n",
        "        print(\"Data loaded successfully.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Data files not found. Attempting to unzip...\")\n",
        "        try:\n",
        "            with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "            with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "            with zipfile.ZipFile('stores.csv.zip', 'r') as zip_ref: # Assuming stores.csv might also be zipped\n",
        "                zip_ref.extractall('.')\n",
        "            train_raw = pd.read_csv('train.csv')\n",
        "            features_raw = pd.read_csv('features.csv')\n",
        "            stores_raw = pd.read_csv('stores.csv')\n",
        "            print(\"Data unzipped and loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or unzipping data: {e}\")\n",
        "            print(\"Please ensure 'train.csv', 'features.csv', 'stores.csv' (or their .zip files) are in the current directory.\")\n",
        "            return\n",
        "\n",
        "\n",
        "    # Initialize and run the preprocessing pipeline\n",
        "    # MarkDowns are now included by default as _remove_markdowns is not called in fit_transform\n",
        "    pipeline = WalmartPreprocessingPipeline(\n",
        "        remove_outliers=True,\n",
        "        enable_lag_features=False # Lag features are still off, but MarkDowns are separate\n",
        "    )\n",
        "    processed_df = pipeline.fit_transform(train_raw, features_raw, stores_raw)\n",
        "    print(f\"\\nProcessed data shape: {processed_df.shape}\")\n",
        "    print(f\"Columns in processed data: {processed_df.columns.tolist()}\")\n",
        "\n",
        "    # --- Feature Importance Analysis (using Correlation) ---\n",
        "    print(\"\\nğŸ“Š Analyzing Feature Importance based on Correlation with Weekly_Sales...\")\n",
        "\n",
        "    # Define target variable\n",
        "    target_col = 'Weekly_Sales'\n",
        "\n",
        "    # Exclude non-feature columns (like 'Date', 'Store', 'Dept' if not treated as numerical features for correlation)\n",
        "    # For correlation, we typically want numerical features.\n",
        "    # Store and Dept are categorical IDs, their direct correlation might not be meaningful\n",
        "    # but their encoded versions (if any) or one-hot versions could be.\n",
        "    # For this simple correlation, let's include all numerical-like features.\n",
        "\n",
        "    # Ensure 'Date' is not in features for correlation\n",
        "    features_for_correlation = [col for col in processed_df.columns if col not in [target_col, 'Date']]\n",
        "\n",
        "    # Calculate correlations\n",
        "    correlations = processed_df[features_for_correlation + [target_col]].corr()[target_col].drop(target_col)\n",
        "\n",
        "    # Convert to absolute values for importance (magnitude of correlation)\n",
        "    abs_correlations = correlations.abs().sort_values(ascending=False)\n",
        "\n",
        "    print(\"\\n--- Feature Correlations with Weekly_Sales (Absolute Value, Sorted) ---\")\n",
        "    print(abs_correlations)\n",
        "\n",
        "    # Define a threshold to categorize features\n",
        "    # This threshold is subjective; adjust based on domain knowledge or further analysis\n",
        "    correlation_threshold = 0.06 # Example threshold: features with abs correlation >= 0.05 are 'important'\n",
        "\n",
        "    important_features = abs_correlations[abs_correlations >= correlation_threshold].index.tolist()\n",
        "    less_important_features = abs_correlations[abs_correlations < correlation_threshold].index.tolist()\n",
        "\n",
        "    print(f\"\\n--- Categorized Feature Importance (Threshold: {correlation_threshold}) ---\")\n",
        "    print(\"\\nâœ… Important Features (Absolute Correlation >= {}):\".format(correlation_threshold))\n",
        "    if important_features:\n",
        "        for feature in important_features:\n",
        "            print(f\"   - {feature}: {abs_correlations[feature]:.4f}\")\n",
        "    else:\n",
        "        print(\"   No features met the importance threshold.\")\n",
        "\n",
        "    print(\"\\nâŒ Less Important Features (Absolute Correlation < {}):\".format(correlation_threshold))\n",
        "    if less_important_features:\n",
        "        for feature in less_important_features:\n",
        "            print(f\"   - {feature}: {abs_correlations[feature]:.4f}\")\n",
        "    else:\n",
        "        print(\"   All features met the importance threshold.\")\n",
        "\n",
        "    print(\"\\nğŸ‰ Feature Importance Analysis Completed!\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Unzip data if necessary (assuming they are in the same directory)\n",
        "    # This block is for convenience in environments like Colab\n",
        "    try:\n",
        "        # Check if raw files exist, if not, try unzipping\n",
        "        if not os.path.exists('train.csv') and os.path.exists('train.csv.zip'):\n",
        "            with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "        if not os.path.exists('features.csv') and os.path.exists('features.csv.zip'):\n",
        "            with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "        if not os.path.exists('stores.csv') and os.path.exists('stores.csv.zip'):\n",
        "            with zipfile.ZipFile('stores.csv.zip', 'r') as zip_ref:\n",
        "                zip_ref.extractall('.')\n",
        "    except Exception as e:\n",
        "        print(f\"Error during initial zip extraction check: {e}\")\n",
        "        print(\"Please ensure data files (or their zips) are in the current directory.\")\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "NnwLHak5VNkD"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "995OzZ7qVPAa",
        "outputId": "e52a13d6-ae34-4738-fd80-4e1ac8dfd841"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Feature Importance Analysis with MarkDowns...\n",
            "Loading raw data...\n",
            "Data loaded successfully.\n",
            "ğŸ“Š Starting data preprocessing...\n",
            "   Merging datasets...\n",
            "   Merged data shape: (421570, 17)\n",
            "   Cleaning merged data (handling IsHoliday duplicates)...\n",
            "   Creating date features...\n",
            "   Creating holiday features...\n",
            "   Encoding store Type...\n",
            "   Handling NaNs in MarkDown columns (filling with 0)...\n",
            "   Removing outliers from Weekly_Sales based on store type...\n",
            "   ğŸ—‘ï¸ Removed 28279 outliers.\n",
            "   Handling remaining missing values (filling numerical NaNs with 0)...\n",
            "   Removing redundant features...\n",
            "âœ… Data preprocessing complete.\n",
            "\n",
            "Processed data shape: (393291, 34)\n",
            "Columns in processed data: ['Store', 'Dept', 'Date', 'Weekly_Sales', 'Size', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'IsHoliday', 'Month', 'DayOfWeek', 'WeekOfYear', 'IsWeekend', 'IsMonthStart', 'IsMonthEnd', 'DaysFromStart', 'WeeksFromStart', 'IsSuperBowlWeek', 'IsLaborDayWeek', 'IsThanksgivingWeek', 'IsChristmasWeek', 'IsMajorHoliday', 'IsHolidayMonth', 'IsBackToSchool', 'Type_Encoded', 'Type_A', 'Type_B', 'Type_C']\n",
            "\n",
            "ğŸ“Š Analyzing Feature Importance based on Correlation with Weekly_Sales...\n",
            "\n",
            "--- Feature Correlations with Weekly_Sales (Absolute Value, Sorted) ---\n",
            "Size                  0.328173\n",
            "Type_Encoded          0.293024\n",
            "Type_A                0.279791\n",
            "Type_C                0.180398\n",
            "Type_B                0.179979\n",
            "Store                 0.087495\n",
            "MarkDown5             0.058338\n",
            "MarkDown1             0.054127\n",
            "MarkDown4             0.039362\n",
            "Unemployment          0.037553\n",
            "Dept                  0.036625\n",
            "MarkDown2             0.024680\n",
            "MarkDown3             0.023365\n",
            "Month                 0.022890\n",
            "WeekOfYear            0.022491\n",
            "IsThanksgivingWeek    0.015576\n",
            "IsMajorHoliday        0.012057\n",
            "IsChristmasWeek       0.011620\n",
            "Fuel_Price            0.007285\n",
            "IsMonthEnd            0.005368\n",
            "IsHoliday             0.005168\n",
            "DaysFromStart         0.003908\n",
            "WeeksFromStart        0.003908\n",
            "IsHolidayMonth        0.003227\n",
            "IsBackToSchool        0.002930\n",
            "Temperature           0.002527\n",
            "IsLaborDayWeek        0.001342\n",
            "CPI                   0.000976\n",
            "IsSuperBowlWeek       0.000538\n",
            "IsMonthStart          0.000297\n",
            "DayOfWeek                  NaN\n",
            "IsWeekend                  NaN\n",
            "Name: Weekly_Sales, dtype: float64\n",
            "\n",
            "--- Categorized Feature Importance (Threshold: 0.06) ---\n",
            "\n",
            "âœ… Important Features (Absolute Correlation >= 0.06):\n",
            "   - Size: 0.3282\n",
            "   - Type_Encoded: 0.2930\n",
            "   - Type_A: 0.2798\n",
            "   - Type_C: 0.1804\n",
            "   - Type_B: 0.1800\n",
            "   - Store: 0.0875\n",
            "\n",
            "âŒ Less Important Features (Absolute Correlation < 0.06):\n",
            "   - MarkDown5: 0.0583\n",
            "   - MarkDown1: 0.0541\n",
            "   - MarkDown4: 0.0394\n",
            "   - Unemployment: 0.0376\n",
            "   - Dept: 0.0366\n",
            "   - MarkDown2: 0.0247\n",
            "   - MarkDown3: 0.0234\n",
            "   - Month: 0.0229\n",
            "   - WeekOfYear: 0.0225\n",
            "   - IsThanksgivingWeek: 0.0156\n",
            "   - IsMajorHoliday: 0.0121\n",
            "   - IsChristmasWeek: 0.0116\n",
            "   - Fuel_Price: 0.0073\n",
            "   - IsMonthEnd: 0.0054\n",
            "   - IsHoliday: 0.0052\n",
            "   - DaysFromStart: 0.0039\n",
            "   - WeeksFromStart: 0.0039\n",
            "   - IsHolidayMonth: 0.0032\n",
            "   - IsBackToSchool: 0.0029\n",
            "   - Temperature: 0.0025\n",
            "   - IsLaborDayWeek: 0.0013\n",
            "   - CPI: 0.0010\n",
            "   - IsSuperBowlWeek: 0.0005\n",
            "   - IsMonthStart: 0.0003\n",
            "\n",
            "ğŸ‰ Feature Importance Analysis Completed!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}