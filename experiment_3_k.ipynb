{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# experiment_3_k"
      ],
      "metadata": {
        "id": "WVxt12dyFod1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT-AULFVEUq3",
        "outputId": "11f4a677-9ef6-4866-d593-a0e0b734f873"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "u8i0qbdAFuSr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "aq126gljFwch"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "_-L7Yg7MFx2q"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVXtMB9GFzFV",
        "outputId": "6201aa5a-2623-4cc7-d27f-9eba9fc62f2b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 553MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DqUuM5MtF3Vs",
        "outputId": "809a3818-f5dd-4ea5-a332-79a16ef8b853"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Setup and MLflow/DagsHub Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install prophet plotly mlflow dagshub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kZ2CZjfaF584",
        "outputId": "d40fae43-8e21-463a-98f2-6a5575904de5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: prophet in /usr/local/lib/python3.11/dist-packages (1.1.7)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (5.24.1)\n",
            "Collecting mlflow\n",
            "  Downloading mlflow-3.1.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting dagshub\n",
            "  Downloading dagshub-0.5.10-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: cmdstanpy>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (1.2.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from prophet) (3.10.0)\n",
            "Requirement already satisfied: pandas>=1.0.4 in /usr/local/lib/python3.11/dist-packages (from prophet) (2.2.2)\n",
            "Requirement already satisfied: holidays<1,>=0.25 in /usr/local/lib/python3.11/dist-packages (from prophet) (0.75)\n",
            "Requirement already satisfied: tqdm>=4.36.1 in /usr/local/lib/python3.11/dist-packages (from prophet) (4.67.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.11/dist-packages (from prophet) (6.5.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly) (9.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly) (24.2)\n",
            "Collecting mlflow-skinny==3.1.0 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.1.0-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.1)\n",
            "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: pyarrow<21,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.41)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (3.1.1)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading databricks_sdk-0.57.0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.115.12)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (3.1.44)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (8.7.0)\n",
            "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_api-1.34.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_sdk-1.34.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (2.11.7)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (2.32.3)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (4.14.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==3.1.0->mlflow) (0.34.3)\n",
            "Collecting appdirs>=1.4.4 (from dagshub)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (13.9.4)\n",
            "Collecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-3.5.3-py2.py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting treelib>=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.7.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Collecting boto3 (from dagshub)\n",
            "  Downloading boto3-1.38.43-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting semver (from dagshub)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n",
            "  Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.1.3)\n",
            "Requirement already satisfied: stanio<2.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from cmdstanpy>=1.0.4->prophet) (0.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.2.1)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.4.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (4.0.12)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->prophet) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.4->prophet) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.3)\n",
            "Collecting botocore<1.39.0,>=1.38.43 (from boto3->dagshub)\n",
            "  Downloading botocore-1.38.43-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->dagshub)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==3.1.0->mlflow) (0.46.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.1.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.1.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Collecting opentelemetry-semantic-conventions==0.55b1 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.1.0->mlflow)\n",
            "  Downloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.1.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.1.0->mlflow) (3.4.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.1.0->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.1.0-py3-none-any.whl (24.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dagshub-0.5.10-py3-none-any.whl (260 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading dagshub_annotation_converter-0.1.10-py3-none-any.whl (33 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading treelib-1.7.1-py3-none-any.whl (19 kB)\n",
            "Downloading boto3-1.38.43-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.38.43-py3-none-any.whl (13.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.57.0-py3-none-any.whl (733 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m29.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_api-1.34.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_sdk-1.34.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_semantic_conventions-0.55b1-py3-none-any.whl (196 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading gql-3.5.3-py2.py3-none-any.whl (74 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, gunicorn, graphql-core, dacite, backoff, typing-inspect, opentelemetry-api, graphql-relay, gql, docker, botocore, alembic, s3transfer, opentelemetry-semantic-conventions, graphene, dataclasses-json, databricks-sdk, dagshub-annotation-converter, opentelemetry-sdk, boto3, mlflow-skinny, dagshub, mlflow\n",
            "Successfully installed alembic-1.16.2 appdirs-1.4.4 backoff-2.2.1 boto3-1.38.43 botocore-1.38.43 dacite-1.6.0 dagshub-0.5.10 dagshub-annotation-converter-0.1.10 databricks-sdk-0.57.0 dataclasses-json-0.6.7 docker-7.1.0 gql-3.5.3 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 jmespath-1.0.1 marshmallow-3.26.1 mlflow-3.1.0 mlflow-skinny-3.1.0 mypy-extensions-1.1.0 opentelemetry-api-1.34.1 opentelemetry-sdk-1.34.1 opentelemetry-semantic-conventions-0.55b1 pathvalidate-3.3.1 s3transfer-0.13.0 semver-3.0.4 treelib-1.7.1 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import MLflow and DagsHub\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots"
      ],
      "metadata": {
        "id": "UU5e--WEF9lD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner=\"konstantine25b\", repo_name=\"Walmart-Recruiting---Store-Sales-Forecasting\", mlflow=True)\n",
        "\n",
        "# Set MLflow experiment\n",
        "experiment_name = \"Walmart_Prophet_Date_Features\"\n",
        "mlflow.set_experiment(experiment_name)\n",
        "\n",
        "print(\"âœ… MLflow and DagsHub initialized!\")\n",
        "print(f\"ğŸ”¬ Experiment: {experiment_name}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "927ee7e1e5844070886503bc2d9684cf",
            "7cd4b3c93b2748b68a0d6a287eb01747"
          ]
        },
        "id": "mUuELXRvG9Oe",
        "outputId": "3d9804aa-01c8-4fe5-a2e0-f49040a3c4ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1mâ—â—â— AUTHORIZATION REQUIRED â—â—â—\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">â—â—â— AUTHORIZATION REQUIRED â—â—â—</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "927ee7e1e5844070886503bc2d9684cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=b2aad0dc-1e32-45d1-b226-6b590404526b&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=f6765cb66121ea77d6738d9cfe8fee03acb408b423ebf84be940d3b710fd39fb\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as konstantine25b\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as konstantine25b\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/25 08:02:26 INFO mlflow.tracking.fluent: Experiment with name 'Walmart_Prophet_Date_Features' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… MLflow and DagsHub initialized!\n",
            "ğŸ”¬ Experiment: Walmart_Prophet_Date_Features\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Data Loading with MLflow Tracking\n",
        "with mlflow.start_run(run_name=\"Data_Loading_Stores_Train_Only\") as run:\n",
        "\n",
        "    print(f\"ğŸ”„ Starting data loading run: {run.info.run_id}\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"data_sources\", \"stores.csv, train.csv\")\n",
        "    mlflow.log_param(\"features_csv_used\", \"No\")\n",
        "    mlflow.log_param(\"approach\", \"prophet_with_date_features\")\n",
        "    mlflow.log_param(\"branch\", \"simplified_stores_train\")\n",
        "\n",
        "    # Load data (only stores and train)\n",
        "    stores = pd.read_csv('stores.csv')\n",
        "\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    train = pd.read_csv('train.csv')\n",
        "\n",
        "    # Log data metrics\n",
        "    mlflow.log_metric(\"train_samples\", len(train))\n",
        "    mlflow.log_metric(\"stores_count\", len(stores))\n",
        "    mlflow.log_metric(\"unique_stores_in_train\", train['Store'].nunique())\n",
        "    mlflow.log_metric(\"unique_departments\", train['Dept'].nunique())\n",
        "    mlflow.log_metric(\"date_range_days\", (pd.to_datetime(train['Date'].max()) - pd.to_datetime(train['Date'].min())).days)\n",
        "\n",
        "    # Basic data quality checks\n",
        "    mlflow.log_metric(\"train_missing_values\", train.isnull().sum().sum())\n",
        "    mlflow.log_metric(\"stores_missing_values\", stores.isnull().sum().sum())\n",
        "    mlflow.log_metric(\"negative_sales_count\", (train['Weekly_Sales'] < 0).sum())\n",
        "\n",
        "    print(\"âœ… Data loaded - Using only stores.csv and train.csv\")\n",
        "    print(f\"ğŸ“Š Train shape: {train.shape}\")\n",
        "    print(f\"ğŸª Stores shape: {stores.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrNzlhMkHCl7",
        "outputId": "720ff7a9-5d87-4b1f-e00a-920bf4ad9291"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Starting data loading run: c7b5171952eb49c9b00421fc6b62405f\n",
            "âœ… Data loaded - Using only stores.csv and train.csv\n",
            "ğŸ“Š Train shape: (421570, 5)\n",
            "ğŸª Stores shape: (45, 3)\n",
            "ğŸƒ View run Data_Loading_Stores_Train_Only at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/c7b5171952eb49c9b00421fc6b62405f\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Fixed Feature Engineering for 2-Year Dataset\n",
        "with mlflow.start_run(run_name=\"Feature_Engineering_Date_Lagging_Fixed\") as run:\n",
        "\n",
        "    print(f\"ğŸ”„ Starting feature engineering run: {run.info.run_id}\")\n",
        "\n",
        "    # Log parameters\n",
        "    mlflow.log_param(\"feature_engineering_type\", \"comprehensive_date_and_lagging_2year_data\")\n",
        "    mlflow.log_param(\"data_period\", \"2010-02-05_to_2012-11-01\")\n",
        "    mlflow.log_param(\"lag_periods\", \"1,2,3,4,8,12\")  # Removed 52-week lag\n",
        "    mlflow.log_param(\"rolling_windows\", \"4,8,12,26\")  # Max 26 weeks (6 months)\n",
        "    mlflow.log_param(\"ewm_alphas\", \"0.1,0.3,0.5\")\n",
        "\n",
        "    def create_comprehensive_date_features(df):\n",
        "        \"\"\"Create extensive date features for retail forecasting\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Basic date components\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "        # Retail-specific time features\n",
        "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "        df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "        df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "        df['IsQuarterStart'] = df['Date'].dt.is_quarter_start.astype(int)\n",
        "        df['IsQuarterEnd'] = df['Date'].dt.is_quarter_end.astype(int)\n",
        "        df['IsYearStart'] = df['Date'].dt.is_year_start.astype(int)\n",
        "        df['IsYearEnd'] = df['Date'].dt.is_year_end.astype(int)\n",
        "\n",
        "        # Cyclical encodings\n",
        "        df['MonthSin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['MonthCos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['DayOfWeekSin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeekCos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfYearSin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "        df['DayOfYearCos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "        df['WeekOfYearSin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "        df['WeekOfYearCos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "\n",
        "        # Time since reference\n",
        "        reference_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        df['MonthsFromStart'] = ((df['Date'].dt.year - reference_date.year) * 12 +\n",
        "                                df['Date'].dt.month - reference_date.month)\n",
        "\n",
        "        # Walmart-specific holiday features (based on provided dates)\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "\n",
        "        # Retail calendar features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)  # Nov, Dec\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)    # Aug, Sep\n",
        "        df['IsSummerSeason'] = df['Month'].isin([6, 7, 8]).astype(int) # Jun, Jul, Aug\n",
        "        df['IsSpringSeaso'] = df['Month'].isin([3, 4, 5]).astype(int)  # Mar, Apr, May\n",
        "\n",
        "        # Week patterns\n",
        "        df['IsFirstWeekOfMonth'] = (df['Day'] <= 7).astype(int)\n",
        "        df['IsLastWeekOfMonth'] = (df['Date'].dt.days_in_month - df['Day'] < 7).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lagging_features_2year(df, target_col='Weekly_Sales'):\n",
        "        \"\"\"\n",
        "        Create lagging features appropriate for 2-year dataset\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # Reasonable lag features for 2-year data\n",
        "        lags = [1, 2, 3, 4, 8, 12]  # Up to 12 weeks (3 months)\n",
        "\n",
        "        for lag in lags:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "        # Rolling window features (max 26 weeks = 6 months)\n",
        "        windows = [4, 8, 12, 26]\n",
        "        for window in windows:\n",
        "            # Rolling mean\n",
        "            df[f'{target_col}_rolling_mean_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            # Rolling std\n",
        "            df[f'{target_col}_rolling_std_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .std()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            # Rolling min/max\n",
        "            df[f'{target_col}_rolling_min_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .min()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            df[f'{target_col}_rolling_max_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .max()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Exponential weighted features\n",
        "        alphas = [0.1, 0.3, 0.5]\n",
        "        for alpha in alphas:\n",
        "            df[f'{target_col}_ewm_{alpha}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .ewm(alpha=alpha)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Short-term differences only (no 52-week seasonal diff)\n",
        "        df[f'{target_col}_trend_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(1)   # Week-over-week\n",
        "        )\n",
        "\n",
        "        df[f'{target_col}_monthly_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(4)   # Month-over-month (4 weeks)\n",
        "        )\n",
        "\n",
        "        df[f'{target_col}_quarterly_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(12)  # Quarter-over-quarter (12 weeks)\n",
        "        )\n",
        "\n",
        "        # Year-over-year features (for data we have)\n",
        "        # Only calculate if we have at least 1 year of data\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "        df['YearWeek'] = df['Date'].dt.year.astype(str) + '_' + df['WeekOfYear'].astype(str).str.zfill(2)\n",
        "\n",
        "        # Same week previous year (only for 2011 and 2012 data)\n",
        "        df[f'{target_col}_same_week_prev_year'] = (\n",
        "            df.groupby(['Store', 'Dept', 'WeekOfYear'])[target_col].shift(1)\n",
        "        )\n",
        "\n",
        "        # Holiday interaction features\n",
        "        df[f'{target_col}_pre_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(-1).fillna(0).astype(int)\n",
        "        df[f'{target_col}_post_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply feature engineering\n",
        "    initial_cols = len(train.columns)\n",
        "\n",
        "    train_merged = train.merge(stores, on='Store', how='left')\n",
        "    mlflow.log_metric(\"after_store_merge_cols\", len(train_merged.columns))\n",
        "\n",
        "    train_with_features = create_comprehensive_date_features(train_merged)\n",
        "    date_features_added = len(train_with_features.columns) - len(train_merged.columns)\n",
        "    mlflow.log_metric(\"date_features_added\", date_features_added)\n",
        "\n",
        "    train_final = create_lagging_features_2year(train_with_features)\n",
        "    lag_features_added = len(train_final.columns) - len(train_with_features.columns)\n",
        "    mlflow.log_metric(\"lag_features_added\", lag_features_added)\n",
        "\n",
        "    total_features_added = len(train_final.columns) - initial_cols\n",
        "    mlflow.log_metric(\"total_features_added\", total_features_added)\n",
        "    mlflow.log_metric(\"final_dataset_cols\", len(train_final.columns))\n",
        "    mlflow.log_metric(\"final_dataset_rows\", len(train_final))\n",
        "\n",
        "    # Data quality after feature engineering\n",
        "    mlflow.log_metric(\"missing_values_after_features\", train_final.isnull().sum().sum())\n",
        "    mlflow.log_metric(\"infinite_values\", np.isinf(train_final.select_dtypes(include=[np.number])).sum().sum())\n",
        "\n",
        "    # Log data coverage analysis\n",
        "    date_range = pd.to_datetime(train_final['Date'].max()) - pd.to_datetime(train_final['Date'].min())\n",
        "    mlflow.log_metric(\"total_weeks_in_data\", date_range.days // 7)\n",
        "    mlflow.log_metric(\"years_of_data\", date_range.days / 365.25)\n",
        "\n",
        "    # Holiday coverage analysis\n",
        "    holiday_weeks = train_final['IsHoliday'].sum()\n",
        "    major_holiday_weeks = train_final['IsMajorHoliday'].sum()\n",
        "    mlflow.log_metric(\"holiday_weeks_total\", holiday_weeks)\n",
        "    mlflow.log_metric(\"major_holiday_weeks\", major_holiday_weeks)\n",
        "\n",
        "    print(f\"âœ… Feature engineering completed for 2-year dataset!\")\n",
        "    print(f\"ğŸ“Š Final dataset shape: {train_final.shape}\")\n",
        "    print(f\"ğŸ“‹ Features added: {total_features_added}\")\n",
        "    print(f\"ğŸ“… Data period: {train_final['Date'].min()} to {train_final['Date'].max()}\")\n",
        "    print(f\"ğŸ¯ Holiday weeks identified: {holiday_weeks} total, {major_holiday_weeks} major holidays\")\n",
        "\n",
        "    # Show feature summary\n",
        "    feature_categories = {\n",
        "        'Date Features': [col for col in train_final.columns if any(x in col for x in ['Year', 'Month', 'Day', 'Week', 'Quarter', 'Sin', 'Cos'])],\n",
        "        'Holiday Features': [col for col in train_final.columns if any(x in col for x in ['Holiday', 'SuperBowl', 'Labor', 'Thanksgiving', 'Christmas'])],\n",
        "        'Lag Features': [col for col in train_final.columns if 'lag_' in col],\n",
        "        'Rolling Features': [col for col in train_final.columns if 'rolling_' in col],\n",
        "        'EWM Features': [col for col in train_final.columns if 'ewm_' in col],\n",
        "        'Diff Features': [col for col in train_final.columns if 'diff' in col]\n",
        "    }\n",
        "\n",
        "    for category, features in feature_categories.items():\n",
        "        mlflow.log_metric(f\"{category.lower().replace(' ', '_')}_count\", len(features))\n",
        "        if features:\n",
        "            print(f\"\\n{category}: {len(features)} features\")\n",
        "            print(f\"  Examples: {features[:3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6KefazkHJMh",
        "outputId": "c344a923-a080-4dc9-ff71-7300fac6f9ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”„ Starting feature engineering run: cec4a44589a94079ae5a64e8662006ae\n",
            "âœ… Feature engineering completed for 2-year dataset!\n",
            "ğŸ“Š Final dataset shape: (421570, 75)\n",
            "ğŸ“‹ Features added: 70\n",
            "ğŸ“… Data period: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "ğŸ¯ Holiday weeks identified: 29661 total, 29661 major holidays\n",
            "\n",
            "Date Features: 65 features\n",
            "  Examples: ['Weekly_Sales', 'Year', 'Month']\n",
            "\n",
            "Holiday Features: 7 features\n",
            "  Examples: ['IsHoliday', 'IsSuperBowlWeek', 'IsLaborDayWeek']\n",
            "\n",
            "Lag Features: 6 features\n",
            "  Examples: ['Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_3']\n",
            "\n",
            "Rolling Features: 16 features\n",
            "  Examples: ['Weekly_Sales_rolling_mean_4', 'Weekly_Sales_rolling_std_4', 'Weekly_Sales_rolling_min_4']\n",
            "\n",
            "EWM Features: 3 features\n",
            "  Examples: ['Weekly_Sales_ewm_0.1', 'Weekly_Sales_ewm_0.3', 'Weekly_Sales_ewm_0.5']\n",
            "\n",
            "Diff Features: 3 features\n",
            "  Examples: ['Weekly_Sales_trend_diff', 'Weekly_Sales_monthly_diff', 'Weekly_Sales_quarterly_diff']\n",
            "ğŸƒ View run Feature_Engineering_Date_Lagging_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/cec4a44589a94079ae5a64e8662006ae\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: NaN Analysis and Data Quality Check\n",
        "with mlflow.start_run(run_name=\"Data_Quality_NaN_Analysis\") as run:\n",
        "\n",
        "    print(f\"ğŸ” Starting NaN analysis run: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"analysis_type\", \"comprehensive_nan_check\")\n",
        "    mlflow.log_param(\"dataset_shape\", f\"{train_final.shape[0]}x{train_final.shape[1]}\")\n",
        "\n",
        "    # 1. Overall NaN Summary\n",
        "    total_cells = train_final.shape[0] * train_final.shape[1]\n",
        "    total_nans = train_final.isnull().sum().sum()\n",
        "    nan_percentage = (total_nans / total_cells) * 100\n",
        "\n",
        "    mlflow.log_metric(\"total_cells\", total_cells)\n",
        "    mlflow.log_metric(\"total_nans\", total_nans)\n",
        "    mlflow.log_metric(\"nan_percentage\", nan_percentage)\n",
        "\n",
        "    print(f\"ğŸ“Š Overall NaN Summary:\")\n",
        "    print(f\"   Total cells: {total_cells:,}\")\n",
        "    print(f\"   Total NaNs: {total_nans:,}\")\n",
        "    print(f\"   NaN percentage: {nan_percentage:.2f}%\")\n",
        "\n",
        "    # 2. NaN by Column\n",
        "    nan_by_column = train_final.isnull().sum()\n",
        "    nan_columns = nan_by_column[nan_by_column > 0].sort_values(ascending=False)\n",
        "\n",
        "    mlflow.log_metric(\"columns_with_nans\", len(nan_columns))\n",
        "    mlflow.log_metric(\"columns_without_nans\", len(train_final.columns) - len(nan_columns))\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Columns with NaNs: {len(nan_columns)} out of {len(train_final.columns)}\")\n",
        "\n",
        "    if len(nan_columns) > 0:\n",
        "        print(f\"\\nğŸ” Detailed NaN Analysis by Column:\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "        nan_analysis = pd.DataFrame({\n",
        "            'Column': nan_columns.index,\n",
        "            'NaN_Count': nan_columns.values,\n",
        "            'NaN_Percentage': (nan_columns.values / len(train_final)) * 100,\n",
        "            'Data_Type': [str(train_final[col].dtype) for col in nan_columns.index]\n",
        "        }).reset_index(drop=True)\n",
        "\n",
        "        print(nan_analysis.to_string(index=False))\n",
        "\n",
        "        # Log top NaN columns\n",
        "        top_nan_columns = nan_analysis.head(10)\n",
        "        for i, row in top_nan_columns.iterrows():\n",
        "            mlflow.log_metric(f\"nan_count_{row['Column']}\", row['NaN_Count'])\n",
        "            mlflow.log_metric(f\"nan_pct_{row['Column']}\", row['NaN_Percentage'])\n",
        "\n",
        "        # Save detailed analysis\n",
        "        nan_analysis.to_csv(\"nan_analysis_detailed.csv\", index=False)\n",
        "        mlflow.log_artifact(\"nan_analysis_detailed.csv\")\n",
        "\n",
        "    # 3. NaN by Feature Category\n",
        "    feature_categories = {\n",
        "        'Original Features': ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday', 'Type', 'Size'],\n",
        "        'Date Features': [col for col in train_final.columns if any(x in col for x in ['Year', 'Month', 'Day', 'Week', 'Quarter', 'Sin', 'Cos', 'DaysFrom', 'MonthsFrom'])],\n",
        "        'Holiday Features': [col for col in train_final.columns if any(x in col for x in ['Holiday', 'SuperBowl', 'Labor', 'Thanksgiving', 'Christmas']) and col != 'IsHoliday'],\n",
        "        'Lag Features': [col for col in train_final.columns if 'lag_' in col],\n",
        "        'Rolling Features': [col for col in train_final.columns if 'rolling_' in col],\n",
        "        'EWM Features': [col for col in train_final.columns if 'ewm_' in col],\n",
        "        'Diff Features': [col for col in train_final.columns if 'diff' in col or 'same_week_prev_year' in col]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ NaN Analysis by Feature Category:\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for category, features in feature_categories.items():\n",
        "        category_features = [col for col in features if col in train_final.columns]\n",
        "        if category_features:\n",
        "            category_nans = train_final[category_features].isnull().sum().sum()\n",
        "            category_cells = len(train_final) * len(category_features)\n",
        "            category_nan_pct = (category_nans / category_cells) * 100 if category_cells > 0 else 0\n",
        "\n",
        "            mlflow.log_metric(f\"{category.lower().replace(' ', '_')}_nans\", category_nans)\n",
        "            mlflow.log_metric(f\"{category.lower().replace(' ', '_')}_nan_pct\", category_nan_pct)\n",
        "\n",
        "            print(f\"{category}:\")\n",
        "            print(f\"   Features: {len(category_features)}\")\n",
        "            print(f\"   NaNs: {category_nans:,} ({category_nan_pct:.2f}%)\")\n",
        "\n",
        "            # Show which specific features have NaNs\n",
        "            cat_nan_features = train_final[category_features].isnull().sum()\n",
        "            cat_nan_features = cat_nan_features[cat_nan_features > 0]\n",
        "            if len(cat_nan_features) > 0:\n",
        "                print(f\"   Features with NaNs: {list(cat_nan_features.index)}\")\n",
        "            print()\n",
        "\n",
        "    # 4. NaN Pattern Analysis\n",
        "    print(f\"\\nğŸ” NaN Pattern Analysis:\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "    # Check if NaNs follow expected patterns for lagging features\n",
        "    lag_features = [col for col in train_final.columns if 'lag_' in col]\n",
        "    if lag_features:\n",
        "        print(f\"Lag Features NaN Analysis:\")\n",
        "        for lag_col in lag_features:\n",
        "            lag_num = int(lag_col.split('_lag_')[1])\n",
        "            expected_nans_per_group = lag_num\n",
        "\n",
        "            # Count NaNs by Store-Dept group\n",
        "            group_nan_counts = train_final.groupby(['Store', 'Dept'])[lag_col].apply(lambda x: x.isnull().sum())\n",
        "            actual_nans = train_final[lag_col].isnull().sum()\n",
        "\n",
        "            print(f\"   {lag_col}: {actual_nans:,} NaNs\")\n",
        "            mlflow.log_metric(f\"nans_{lag_col}\", actual_nans)\n",
        "\n",
        "    # Check rolling features\n",
        "    rolling_features = [col for col in train_final.columns if 'rolling_' in col]\n",
        "    if rolling_features:\n",
        "        print(f\"\\nRolling Features NaN Analysis:\")\n",
        "        for roll_col in rolling_features[:3]:  # Show first 3\n",
        "            nans = train_final[roll_col].isnull().sum()\n",
        "            print(f\"   {roll_col}: {nans:,} NaNs\")\n",
        "\n",
        "    # Check diff features\n",
        "    diff_features = [col for col in train_final.columns if 'diff' in col]\n",
        "    if diff_features:\n",
        "        print(f\"\\nDiff Features NaN Analysis:\")\n",
        "        for diff_col in diff_features:\n",
        "            nans = train_final[diff_col].isnull().sum()\n",
        "            print(f\"   {diff_col}: {nans:,} NaNs\")\n",
        "\n",
        "    # 5. Sample of rows with most NaNs\n",
        "    print(f\"\\nğŸ¯ Sample Analysis:\")\n",
        "    print(\"=\"*30)\n",
        "\n",
        "    # Count NaNs per row\n",
        "    nans_per_row = train_final.isnull().sum(axis=1)\n",
        "    max_nans_per_row = nans_per_row.max()\n",
        "    rows_with_max_nans = (nans_per_row == max_nans_per_row).sum()\n",
        "\n",
        "    mlflow.log_metric(\"max_nans_per_row\", max_nans_per_row)\n",
        "    mlflow.log_metric(\"rows_with_max_nans\", rows_with_max_nans)\n",
        "\n",
        "    print(f\"Maximum NaNs in single row: {max_nans_per_row}\")\n",
        "    print(f\"Rows with maximum NaNs: {rows_with_max_nans:,}\")\n",
        "\n",
        "    # Show distribution of NaNs per row\n",
        "    nan_distribution = nans_per_row.value_counts().sort_index()\n",
        "    print(f\"\\nDistribution of NaNs per row:\")\n",
        "    for nan_count, row_count in nan_distribution.head(10).items():\n",
        "        print(f\"   {nan_count} NaNs: {row_count:,} rows\")\n",
        "\n",
        "    # 6. Show sample of problematic rows\n",
        "    if max_nans_per_row > 0:\n",
        "        print(f\"\\nğŸ“‹ Sample of rows with most NaNs:\")\n",
        "        sample_problematic = train_final[nans_per_row == max_nans_per_row].head(3)\n",
        "        basic_info = sample_problematic[['Store', 'Dept', 'Date', 'Weekly_Sales']].copy()\n",
        "        print(basic_info.to_string(index=False))\n",
        "\n",
        "    # 7. Recommendations\n",
        "    print(f\"\\nğŸ’¡ Recommendations:\")\n",
        "    print(\"=\"*20)\n",
        "\n",
        "    if total_nans == 0:\n",
        "        print(\"âœ… No NaNs found! Dataset is clean.\")\n",
        "        mlflow.log_param(\"recommendation\", \"no_action_needed\")\n",
        "    else:\n",
        "        if nan_percentage < 1:\n",
        "            print(\"âœ… Low NaN percentage (<1%) - mostly expected from lagging features\")\n",
        "            mlflow.log_param(\"recommendation\", \"minimal_cleaning_needed\")\n",
        "        elif nan_percentage < 5:\n",
        "            print(\"âš ï¸  Moderate NaN percentage (1-5%) - review and handle appropriately\")\n",
        "            mlflow.log_param(\"recommendation\", \"moderate_cleaning_needed\")\n",
        "        else:\n",
        "            print(\"ğŸš¨ High NaN percentage (>5%) - significant cleaning required\")\n",
        "            mlflow.log_param(\"recommendation\", \"extensive_cleaning_needed\")\n",
        "\n",
        "        print(f\"\\nSuggested actions:\")\n",
        "        print(f\"1. Lag features: NaNs are expected - consider forward fill or drop early rows\")\n",
        "        print(f\"2. Rolling features: Should have minimal NaNs due to min_periods=1\")\n",
        "        print(f\"3. Diff features: NaNs expected for first observations\")\n",
        "        print(f\"4. Consider Prophet's built-in missing value handling\")\n",
        "\n",
        "    print(f\"\\nğŸƒ Analysis complete! Check MLflow for detailed metrics.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35nBxlgmJwPl",
        "outputId": "0c9e9837-09aa-4dca-ff75-fc8e60894489"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ” Starting NaN analysis run: 9835ab3010f64015a3c7afaa25d3071f\n",
            "ğŸ“Š Overall NaN Summary:\n",
            "   Total cells: 31,617,750\n",
            "   Total NaNs: 325,131\n",
            "   NaN percentage: 1.03%\n",
            "\n",
            "ğŸ“‹ Columns with NaNs: 14 out of 75\n",
            "\n",
            "ğŸ” Detailed NaN Analysis by Column:\n",
            "============================================================\n",
            "                          Column  NaN_Count  NaN_Percentage Data_Type\n",
            "Weekly_Sales_same_week_prev_year     159167       37.755770   float64\n",
            "             Weekly_Sales_lag_12      38615        9.159807   float64\n",
            "     Weekly_Sales_quarterly_diff      38615        9.159807   float64\n",
            "              Weekly_Sales_lag_8      25966        6.159357   float64\n",
            "              Weekly_Sales_lag_4      13134        3.115497   float64\n",
            "       Weekly_Sales_monthly_diff      13134        3.115497   float64\n",
            "              Weekly_Sales_lag_3       9889        2.345755   float64\n",
            "              Weekly_Sales_lag_2       6625        1.571507   float64\n",
            "              Weekly_Sales_lag_1       3331        0.790142   float64\n",
            "      Weekly_Sales_rolling_std_4       3331        0.790142   float64\n",
            "     Weekly_Sales_rolling_std_26       3331        0.790142   float64\n",
            "     Weekly_Sales_rolling_std_12       3331        0.790142   float64\n",
            "      Weekly_Sales_rolling_std_8       3331        0.790142   float64\n",
            "         Weekly_Sales_trend_diff       3331        0.790142   float64\n",
            "\n",
            "ğŸ“ˆ NaN Analysis by Feature Category:\n",
            "============================================================\n",
            "Original Features:\n",
            "   Features: 7\n",
            "   NaNs: 0 (0.00%)\n",
            "\n",
            "Date Features:\n",
            "   Features: 65\n",
            "   NaNs: 325,131 (1.19%)\n",
            "   Features with NaNs: ['Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_3', 'Weekly_Sales_lag_4', 'Weekly_Sales_lag_8', 'Weekly_Sales_lag_12', 'Weekly_Sales_rolling_std_4', 'Weekly_Sales_rolling_std_8', 'Weekly_Sales_rolling_std_12', 'Weekly_Sales_rolling_std_26', 'Weekly_Sales_trend_diff', 'Weekly_Sales_monthly_diff', 'Weekly_Sales_quarterly_diff', 'Weekly_Sales_same_week_prev_year']\n",
            "\n",
            "Holiday Features:\n",
            "   Features: 6\n",
            "   NaNs: 0 (0.00%)\n",
            "\n",
            "Lag Features:\n",
            "   Features: 6\n",
            "   NaNs: 97,560 (3.86%)\n",
            "   Features with NaNs: ['Weekly_Sales_lag_1', 'Weekly_Sales_lag_2', 'Weekly_Sales_lag_3', 'Weekly_Sales_lag_4', 'Weekly_Sales_lag_8', 'Weekly_Sales_lag_12']\n",
            "\n",
            "Rolling Features:\n",
            "   Features: 16\n",
            "   NaNs: 13,324 (0.20%)\n",
            "   Features with NaNs: ['Weekly_Sales_rolling_std_4', 'Weekly_Sales_rolling_std_8', 'Weekly_Sales_rolling_std_12', 'Weekly_Sales_rolling_std_26']\n",
            "\n",
            "EWM Features:\n",
            "   Features: 3\n",
            "   NaNs: 0 (0.00%)\n",
            "\n",
            "Diff Features:\n",
            "   Features: 4\n",
            "   NaNs: 214,247 (12.71%)\n",
            "   Features with NaNs: ['Weekly_Sales_trend_diff', 'Weekly_Sales_monthly_diff', 'Weekly_Sales_quarterly_diff', 'Weekly_Sales_same_week_prev_year']\n",
            "\n",
            "\n",
            "ğŸ” NaN Pattern Analysis:\n",
            "========================================\n",
            "Lag Features NaN Analysis:\n",
            "   Weekly_Sales_lag_1: 3,331 NaNs\n",
            "   Weekly_Sales_lag_2: 6,625 NaNs\n",
            "   Weekly_Sales_lag_3: 9,889 NaNs\n",
            "   Weekly_Sales_lag_4: 13,134 NaNs\n",
            "   Weekly_Sales_lag_8: 25,966 NaNs\n",
            "   Weekly_Sales_lag_12: 38,615 NaNs\n",
            "\n",
            "Rolling Features NaN Analysis:\n",
            "   Weekly_Sales_rolling_mean_4: 0 NaNs\n",
            "   Weekly_Sales_rolling_std_4: 3,331 NaNs\n",
            "   Weekly_Sales_rolling_min_4: 0 NaNs\n",
            "\n",
            "Diff Features NaN Analysis:\n",
            "   Weekly_Sales_trend_diff: 3,331 NaNs\n",
            "   Weekly_Sales_monthly_diff: 13,134 NaNs\n",
            "   Weekly_Sales_quarterly_diff: 38,615 NaNs\n",
            "\n",
            "ğŸ¯ Sample Analysis:\n",
            "==============================\n",
            "Maximum NaNs in single row: 14\n",
            "Rows with maximum NaNs: 3,331\n",
            "\n",
            "Distribution of NaNs per row:\n",
            "   0 NaNs: 262,325 rows\n",
            "   1 NaNs: 120,630 rows\n",
            "   2 NaNs: 52 rows\n",
            "   3 NaNs: 12,617 rows\n",
            "   4 NaNs: 12,812 rows\n",
            "   5 NaNs: 3 rows\n",
            "   6 NaNs: 3,243 rows\n",
            "   7 NaNs: 3,265 rows\n",
            "   8 NaNs: 3,292 rows\n",
            "   14 NaNs: 3,331 rows\n",
            "\n",
            "ğŸ“‹ Sample of rows with most NaNs:\n",
            " Store  Dept       Date  Weekly_Sales\n",
            "     1     1 2010-02-05      24924.50\n",
            "     1     2 2010-02-05      50605.27\n",
            "     1     3 2010-02-05      13740.12\n",
            "\n",
            "ğŸ’¡ Recommendations:\n",
            "====================\n",
            "âš ï¸  Moderate NaN percentage (1-5%) - review and handle appropriately\n",
            "\n",
            "Suggested actions:\n",
            "1. Lag features: NaNs are expected - consider forward fill or drop early rows\n",
            "2. Rolling features: Should have minimal NaNs due to min_periods=1\n",
            "3. Diff features: NaNs expected for first observations\n",
            "4. Consider Prophet's built-in missing value handling\n",
            "\n",
            "ğŸƒ Analysis complete! Check MLflow for detailed metrics.\n",
            "ğŸƒ View run Data_Quality_NaN_Analysis at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/9835ab3010f64015a3c7afaa25d3071f\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Fixed Feature Engineering - Handle NaNs Properly\n",
        "with mlflow.start_run(run_name=\"Feature_Engineering_NaN_Fixed\") as run:\n",
        "\n",
        "    print(f\"ğŸ”§ Starting NaN-fixed feature engineering: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"feature_engineering_type\", \"nan_fixed_comprehensive\")\n",
        "    mlflow.log_param(\"nan_handling_strategy\", \"remove_problematic_fill_expected\")\n",
        "    mlflow.log_param(\"removed_features\", \"Weekly_Sales_same_week_prev_year\")\n",
        "\n",
        "    def create_comprehensive_date_features(df):\n",
        "        \"\"\"Create extensive date features for retail forecasting\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Basic date components\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['DayOfYear'] = df['Date'].dt.dayofyear\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "        # Retail-specific time features\n",
        "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "        df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "        df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "        df['IsQuarterStart'] = df['Date'].dt.is_quarter_start.astype(int)\n",
        "        df['IsQuarterEnd'] = df['Date'].dt.is_quarter_end.astype(int)\n",
        "        df['IsYearStart'] = df['Date'].dt.is_year_start.astype(int)\n",
        "        df['IsYearEnd'] = df['Date'].dt.is_year_end.astype(int)\n",
        "\n",
        "        # Cyclical encodings\n",
        "        df['MonthSin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['MonthCos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['DayOfWeekSin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeekCos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfYearSin'] = np.sin(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "        df['DayOfYearCos'] = np.cos(2 * np.pi * df['DayOfYear'] / 365.25)\n",
        "        df['WeekOfYearSin'] = np.sin(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "        df['WeekOfYearCos'] = np.cos(2 * np.pi * df['WeekOfYear'] / 52)\n",
        "\n",
        "        # Time since reference\n",
        "        reference_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        df['MonthsFromStart'] = ((df['Date'].dt.year - reference_date.year) * 12 +\n",
        "                                df['Date'].dt.month - reference_date.month)\n",
        "\n",
        "        # Walmart-specific holiday features\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "\n",
        "        # Retail calendar features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        df['IsSummerSeason'] = df['Month'].isin([6, 7, 8]).astype(int)\n",
        "        df['IsSpringSeaso'] = df['Month'].isin([3, 4, 5]).astype(int)\n",
        "\n",
        "        # Week patterns\n",
        "        df['IsFirstWeekOfMonth'] = (df['Day'] <= 7).astype(int)\n",
        "        df['IsLastWeekOfMonth'] = (df['Date'].dt.days_in_month - df['Day'] < 7).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lagging_features_nan_fixed(df, target_col='Weekly_Sales'):\n",
        "        \"\"\"\n",
        "        Create lagging features with proper NaN handling\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # Lag features (expected NaNs at the beginning)\n",
        "        lags = [1, 2, 3, 4, 8, 12]\n",
        "\n",
        "        for lag in lags:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "            # Forward fill NaNs (use the first available value)\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[lag_col].fillna(method='bfill')\n",
        "\n",
        "        # Rolling window features\n",
        "        windows = [4, 8, 12, 26]\n",
        "        for window in windows:\n",
        "            # Rolling mean (min_periods=1 prevents NaNs)\n",
        "            df[f'{target_col}_rolling_mean_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            # Rolling std (min_periods=2 to avoid NaNs when only 1 value)\n",
        "            df[f'{target_col}_rolling_std_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=2)\n",
        "                .std()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            # Fill remaining NaNs in std with 0 (indicates no volatility)\n",
        "            df[f'{target_col}_rolling_std_{window}'] = df[f'{target_col}_rolling_std_{window}'].fillna(0)\n",
        "\n",
        "            # Rolling min/max (min_periods=1)\n",
        "            df[f'{target_col}_rolling_min_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .min()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            df[f'{target_col}_rolling_max_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .max()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Exponential weighted features (no NaNs expected)\n",
        "        alphas = [0.1, 0.3, 0.5]\n",
        "        for alpha in alphas:\n",
        "            df[f'{target_col}_ewm_{alpha}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .ewm(alpha=alpha)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Difference features with proper NaN handling\n",
        "        # Week-over-week difference\n",
        "        df[f'{target_col}_trend_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(1)\n",
        "        )\n",
        "        # Fill first observation with 0 (no change from \"previous\")\n",
        "        df[f'{target_col}_trend_diff'] = df.groupby(['Store', 'Dept'])[f'{target_col}_trend_diff'].fillna(0)\n",
        "\n",
        "        # Month-over-month difference (4 weeks)\n",
        "        df[f'{target_col}_monthly_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(4)\n",
        "        )\n",
        "        # Fill with week-over-week difference for first 4 observations\n",
        "        mask = df[f'{target_col}_monthly_diff'].isnull()\n",
        "        df.loc[mask, f'{target_col}_monthly_diff'] = df.loc[mask, f'{target_col}_trend_diff']\n",
        "\n",
        "        # Quarter-over-quarter difference (12 weeks)\n",
        "        df[f'{target_col}_quarterly_diff'] = (\n",
        "            df.groupby(['Store', 'Dept'])[target_col].diff(12)\n",
        "        )\n",
        "        # Fill with monthly difference for first 12 observations\n",
        "        mask = df[f'{target_col}_quarterly_diff'].isnull()\n",
        "        df.loc[mask, f'{target_col}_quarterly_diff'] = df.loc[mask, f'{target_col}_monthly_diff']\n",
        "\n",
        "        # REMOVED: Weekly_Sales_same_week_prev_year (too many NaNs)\n",
        "        # This was causing 37.8% NaNs which is too much\n",
        "\n",
        "        # Holiday interaction features\n",
        "        df[f'{target_col}_pre_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(-1).fillna(0).astype(int)\n",
        "        df[f'{target_col}_post_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply feature engineering\n",
        "    initial_cols = len(train.columns)\n",
        "\n",
        "    train_merged = train.merge(stores, on='Store', how='left')\n",
        "    mlflow.log_metric(\"after_store_merge_cols\", len(train_merged.columns))\n",
        "\n",
        "    train_with_features = create_comprehensive_date_features(train_merged)\n",
        "    date_features_added = len(train_with_features.columns) - len(train_merged.columns)\n",
        "    mlflow.log_metric(\"date_features_added\", date_features_added)\n",
        "\n",
        "    train_final_fixed = create_lagging_features_nan_fixed(train_with_features)\n",
        "    lag_features_added = len(train_final_fixed.columns) - len(train_with_features.columns)\n",
        "    mlflow.log_metric(\"lag_features_added\", lag_features_added)\n",
        "\n",
        "    total_features_added = len(train_final_fixed.columns) - initial_cols\n",
        "    mlflow.log_metric(\"total_features_added\", total_features_added)\n",
        "    mlflow.log_metric(\"final_dataset_cols\", len(train_final_fixed.columns))\n",
        "    mlflow.log_metric(\"final_dataset_rows\", len(train_final_fixed))\n",
        "\n",
        "    # Check NaNs after fixes\n",
        "    total_nans_after = train_final_fixed.isnull().sum().sum()\n",
        "    mlflow.log_metric(\"total_nans_after_fix\", total_nans_after)\n",
        "    mlflow.log_metric(\"nan_percentage_after_fix\", (total_nans_after / (train_final_fixed.shape[0] * train_final_fixed.shape[1])) * 100)\n",
        "\n",
        "    print(f\"âœ… Feature engineering completed with NaN fixes!\")\n",
        "    print(f\"ğŸ“Š Final dataset shape: {train_final_fixed.shape}\")\n",
        "    print(f\"ğŸ“‹ Features added: {total_features_added}\")\n",
        "    print(f\"ğŸ”§ NaNs after fixes: {total_nans_after:,}\")\n",
        "\n",
        "    # Verify no NaNs remain\n",
        "    if total_nans_after == 0:\n",
        "        print(\"ğŸ‰ SUCCESS: No NaNs remaining in dataset!\")\n",
        "        mlflow.log_param(\"nan_fix_status\", \"complete_success\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  Warning: {total_nans_after:,} NaNs still remain\")\n",
        "        mlflow.log_param(\"nan_fix_status\", \"partial_success\")\n",
        "\n",
        "        # Show remaining NaN columns\n",
        "        remaining_nans = train_final_fixed.isnull().sum()\n",
        "        remaining_nans = remaining_nans[remaining_nans > 0]\n",
        "        if len(remaining_nans) > 0:\n",
        "            print(\"Columns with remaining NaNs:\")\n",
        "            for col, count in remaining_nans.items():\n",
        "                print(f\"   {col}: {count:,} NaNs\")\n",
        "\n",
        "    # Quick verification of key feature types\n",
        "    feature_verification = {\n",
        "        'Lag Features': len([col for col in train_final_fixed.columns if 'lag_' in col]),\n",
        "        'Rolling Features': len([col for col in train_final_fixed.columns if 'rolling_' in col]),\n",
        "        'EWM Features': len([col for col in train_final_fixed.columns if 'ewm_' in col]),\n",
        "        'Diff Features': len([col for col in train_final_fixed.columns if 'diff' in col]),\n",
        "        'Holiday Features': len([col for col in train_final_fixed.columns if any(x in col for x in ['Holiday', 'SuperBowl', 'Labor', 'Thanksgiving', 'Christmas'])])\n",
        "    }\n",
        "\n",
        "    print(f\"\\nğŸ“‹ Feature Verification:\")\n",
        "    for feature_type, count in feature_verification.items():\n",
        "        print(f\"   {feature_type}: {count}\")\n",
        "        mlflow.log_metric(f\"{feature_type.lower().replace(' ', '_')}_count_final\", count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzIihdCoLJ1n",
        "outputId": "e490e049-a43e-4e92-ed57-a061abddf4ed"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Starting NaN-fixed feature engineering: b340ff13b5684dc98dbb82de5b61f914\n",
            "âœ… Feature engineering completed with NaN fixes!\n",
            "ğŸ“Š Final dataset shape: (421570, 73)\n",
            "ğŸ“‹ Features added: 68\n",
            "ğŸ”§ NaNs after fixes: 2,035\n",
            "âš ï¸  Warning: 2,035 NaNs still remain\n",
            "Columns with remaining NaNs:\n",
            "   Weekly_Sales_lag_1: 37 NaNs\n",
            "   Weekly_Sales_lag_2: 97 NaNs\n",
            "   Weekly_Sales_lag_3: 154 NaNs\n",
            "   Weekly_Sales_lag_4: 226 NaNs\n",
            "   Weekly_Sales_lag_8: 526 NaNs\n",
            "   Weekly_Sales_lag_12: 995 NaNs\n",
            "\n",
            "ğŸ“‹ Feature Verification:\n",
            "   Lag Features: 6\n",
            "   Rolling Features: 16\n",
            "   EWM Features: 3\n",
            "   Diff Features: 3\n",
            "   Holiday Features: 7\n",
            "ğŸƒ View run Feature_Engineering_NaN_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/b340ff13b5684dc98dbb82de5b61f914\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Final NaN Cleanup for Lag Features\n",
        "with mlflow.start_run(run_name=\"Final_NaN_Cleanup_Lag_Features\") as run:\n",
        "\n",
        "    print(f\"ğŸ”§ Starting final NaN cleanup: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"cleanup_strategy\", \"fill_lag_nans_with_group_statistics\")\n",
        "    mlflow.log_param(\"remaining_nans_before\", 2035)\n",
        "\n",
        "    # Create a copy for final cleanup\n",
        "    train_final_clean = train_final_fixed.copy()\n",
        "\n",
        "    # Strategy: Fill lag feature NaNs with group-specific statistics\n",
        "    lag_features = [col for col in train_final_clean.columns if 'lag_' in col]\n",
        "\n",
        "    print(f\"ğŸ” Cleaning {len(lag_features)} lag features...\")\n",
        "\n",
        "    for lag_col in lag_features:\n",
        "        nans_before = train_final_clean[lag_col].isnull().sum()\n",
        "\n",
        "        if nans_before > 0:\n",
        "            print(f\"   Fixing {lag_col}: {nans_before} NaNs\")\n",
        "\n",
        "            # Method 1: Forward fill within each Store-Dept group first\n",
        "            train_final_clean[lag_col] = train_final_clean.groupby(['Store', 'Dept'])[lag_col].fillna(method='ffill')\n",
        "\n",
        "            # Method 2: For remaining NaNs, use the group's mean Weekly_Sales\n",
        "            # This handles store-dept combinations with insufficient history\n",
        "            mask = train_final_clean[lag_col].isnull()\n",
        "            if mask.sum() > 0:\n",
        "                group_means = train_final_clean.groupby(['Store', 'Dept'])['Weekly_Sales'].transform('mean')\n",
        "                train_final_clean.loc[mask, lag_col] = group_means[mask]\n",
        "\n",
        "            # Method 3: For any remaining NaNs (shouldn't happen), use overall group statistics\n",
        "            if train_final_clean[lag_col].isnull().sum() > 0:\n",
        "                # Use store-level mean if dept-level isn't available\n",
        "                store_means = train_final_clean.groupby('Store')['Weekly_Sales'].transform('mean')\n",
        "                mask = train_final_clean[lag_col].isnull()\n",
        "                train_final_clean.loc[mask, lag_col] = store_means[mask]\n",
        "\n",
        "            # Method 4: Ultimate fallback - use department-level mean across all stores\n",
        "            if train_final_clean[lag_col].isnull().sum() > 0:\n",
        "                dept_means = train_final_clean.groupby('Dept')['Weekly_Sales'].transform('mean')\n",
        "                mask = train_final_clean[lag_col].isnull()\n",
        "                train_final_clean.loc[mask, lag_col] = dept_means[mask]\n",
        "\n",
        "            nans_after = train_final_clean[lag_col].isnull().sum()\n",
        "            print(f\"      Before: {nans_before}, After: {nans_after}\")\n",
        "            mlflow.log_metric(f\"nans_fixed_{lag_col}\", nans_before - nans_after)\n",
        "\n",
        "    # Final verification\n",
        "    total_nans_final = train_final_clean.isnull().sum().sum()\n",
        "    mlflow.log_metric(\"total_nans_final\", total_nans_final)\n",
        "    mlflow.log_metric(\"final_nan_percentage\", (total_nans_final / (train_final_clean.shape[0] * train_final_clean.shape[1])) * 100)\n",
        "\n",
        "    print(f\"\\nâœ… Final cleanup completed!\")\n",
        "    print(f\"ğŸ“Š Dataset shape: {train_final_clean.shape}\")\n",
        "    print(f\"ğŸ”§ Total NaNs remaining: {total_nans_final}\")\n",
        "\n",
        "    if total_nans_final == 0:\n",
        "        print(\"ğŸ‰ SUCCESS: Dataset is now completely clean with 0 NaNs!\")\n",
        "        mlflow.log_param(\"final_status\", \"completely_clean\")\n",
        "    else:\n",
        "        print(f\"âš ï¸  {total_nans_final} NaNs still remain after all cleanup attempts\")\n",
        "        mlflow.log_param(\"final_status\", \"needs_investigation\")\n",
        "\n",
        "        # Show any remaining problematic columns\n",
        "        remaining_nans = train_final_clean.isnull().sum()\n",
        "        remaining_nans = remaining_nans[remaining_nans > 0]\n",
        "        if len(remaining_nans) > 0:\n",
        "            print(\"Remaining NaN columns:\")\n",
        "            for col, count in remaining_nans.items():\n",
        "                print(f\"   {col}: {count}\")\n",
        "\n",
        "    # Data quality summary\n",
        "    print(f\"\\nğŸ“‹ Final Dataset Summary:\")\n",
        "    print(f\"   Shape: {train_final_clean.shape}\")\n",
        "    print(f\"   Date range: {train_final_clean['Date'].min()} to {train_final_clean['Date'].max()}\")\n",
        "    print(f\"   Stores: {train_final_clean['Store'].nunique()}\")\n",
        "    print(f\"   Departments: {train_final_clean['Dept'].nunique()}\")\n",
        "    print(f\"   Store-Dept combinations: {train_final_clean.groupby(['Store', 'Dept']).ngroups}\")\n",
        "    print(f\"   Average weeks per combination: {len(train_final_clean) / train_final_clean.groupby(['Store', 'Dept']).ngroups:.1f}\")\n",
        "\n",
        "    # Log final dataset metrics\n",
        "    mlflow.log_metric(\"final_shape_rows\", train_final_clean.shape[0])\n",
        "    mlflow.log_metric(\"final_shape_cols\", train_final_clean.shape[1])\n",
        "    mlflow.log_metric(\"final_stores\", train_final_clean['Store'].nunique())\n",
        "    mlflow.log_metric(\"final_departments\", train_final_clean['Dept'].nunique())\n",
        "    mlflow.log_metric(\"final_store_dept_combinations\", train_final_clean.groupby(['Store', 'Dept']).ngroups)\n",
        "\n",
        "    print(f\"\\nğŸ¯ Dataset is ready for Prophet training!\")\n",
        "\n",
        "# Now run the quality verification\n",
        "with mlflow.start_run(run_name=\"Data_Quality_Final_Verification\") as run:\n",
        "\n",
        "    print(f\"ğŸ” Final data quality check: {run.info.run_id}\")\n",
        "\n",
        "    # Check for any data quality issues\n",
        "    quality_checks = {}\n",
        "\n",
        "    # 1. Check for NaNs\n",
        "    total_nans = train_final_clean.isnull().sum().sum()\n",
        "    quality_checks['total_nans'] = total_nans\n",
        "\n",
        "    # 2. Check for infinite values\n",
        "    numeric_cols = train_final_clean.select_dtypes(include=[np.number]).columns\n",
        "    total_infs = np.isinf(train_final_clean[numeric_cols]).sum().sum()\n",
        "    quality_checks['total_infinites'] = total_infs\n",
        "\n",
        "    # 3. Check for negative sales (should be minimal and expected)\n",
        "    negative_sales = (train_final_clean['Weekly_Sales'] < 0).sum()\n",
        "    quality_checks['negative_sales_count'] = negative_sales\n",
        "\n",
        "    # 4. Check for duplicates\n",
        "    duplicates = train_final_clean.duplicated(subset=['Store', 'Dept', 'Date']).sum()\n",
        "    quality_checks['duplicate_rows'] = duplicates\n",
        "\n",
        "    # 5. Check date consistency\n",
        "    date_issues = train_final_clean['Date'].isnull().sum()\n",
        "    quality_checks['date_issues'] = date_issues\n",
        "\n",
        "    # 6. Check feature value ranges\n",
        "    sales_stats = {\n",
        "        'min_sales': train_final_clean['Weekly_Sales'].min(),\n",
        "        'max_sales': train_final_clean['Weekly_Sales'].max(),\n",
        "        'mean_sales': train_final_clean['Weekly_Sales'].mean(),\n",
        "        'std_sales': train_final_clean['Weekly_Sales'].std()\n",
        "    }\n",
        "\n",
        "    # Log all quality checks\n",
        "    for check, value in quality_checks.items():\n",
        "        mlflow.log_metric(f\"quality_{check}\", value)\n",
        "\n",
        "    for stat, value in sales_stats.items():\n",
        "        mlflow.log_metric(f\"sales_{stat}\", value)\n",
        "\n",
        "    print(f\"ğŸ“Š Data Quality Report:\")\n",
        "    print(f\"   âœ… Total NaNs: {total_nans}\")\n",
        "    print(f\"   âœ… Infinite values: {total_infs}\")\n",
        "    print(f\"   âœ… Negative sales: {negative_sales:,} ({negative_sales/len(train_final_clean)*100:.2f}%)\")\n",
        "    print(f\"   âœ… Duplicate rows: {duplicates}\")\n",
        "    print(f\"   âœ… Date issues: {date_issues}\")\n",
        "\n",
        "    print(f\"\\nğŸ“ˆ Sales Statistics:\")\n",
        "    print(f\"   Min: ${sales_stats['min_sales']:,.2f}\")\n",
        "    print(f\"   Max: ${sales_stats['max_sales']:,.2f}\")\n",
        "    print(f\"   Mean: ${sales_stats['mean_sales']:,.2f}\")\n",
        "    print(f\"   Std: ${sales_stats['std_sales']:,.2f}\")\n",
        "\n",
        "    # Overall assessment\n",
        "    critical_issues = total_nans + total_infs + duplicates + date_issues\n",
        "\n",
        "    if critical_issues == 0:\n",
        "        print(f\"\\nğŸ‰ EXCELLENT: No critical data quality issues found!\")\n",
        "        print(f\"ğŸ“¦ Dataset is ready for Prophet modeling!\")\n",
        "        mlflow.log_param(\"data_quality_status\", \"excellent\")\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸  Warning: {critical_issues} critical issues found\")\n",
        "        mlflow.log_param(\"data_quality_status\", \"needs_attention\")\n",
        "\n",
        "    # Sample of final dataset\n",
        "    print(f\"\\nğŸ“‹ Sample of cleaned dataset:\")\n",
        "    sample_cols = ['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday',\n",
        "                   'Weekly_Sales_lag_1', 'Weekly_Sales_rolling_mean_4', 'Weekly_Sales_ewm_0.3']\n",
        "    available_cols = [col for col in sample_cols if col in train_final_clean.columns]\n",
        "    print(train_final_clean[available_cols].head(10).to_string(index=False))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uOMsYACLok6",
        "outputId": "65e427f1-8351-431f-b6ed-e0661ef38da7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Starting final NaN cleanup: 5f09cfc2865b4e409ae582047bb6db43\n",
            "ğŸ” Cleaning 6 lag features...\n",
            "   Fixing Weekly_Sales_lag_1: 37 NaNs\n",
            "      Before: 37, After: 0\n",
            "   Fixing Weekly_Sales_lag_2: 97 NaNs\n",
            "      Before: 97, After: 0\n",
            "   Fixing Weekly_Sales_lag_3: 154 NaNs\n",
            "      Before: 154, After: 0\n",
            "   Fixing Weekly_Sales_lag_4: 226 NaNs\n",
            "      Before: 226, After: 0\n",
            "   Fixing Weekly_Sales_lag_8: 526 NaNs\n",
            "      Before: 526, After: 0\n",
            "   Fixing Weekly_Sales_lag_12: 995 NaNs\n",
            "      Before: 995, After: 0\n",
            "\n",
            "âœ… Final cleanup completed!\n",
            "ğŸ“Š Dataset shape: (421570, 73)\n",
            "ğŸ”§ Total NaNs remaining: 0\n",
            "ğŸ‰ SUCCESS: Dataset is now completely clean with 0 NaNs!\n",
            "\n",
            "ğŸ“‹ Final Dataset Summary:\n",
            "   Shape: (421570, 73)\n",
            "   Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "   Stores: 45\n",
            "   Departments: 81\n",
            "   Store-Dept combinations: 3331\n",
            "   Average weeks per combination: 126.6\n",
            "\n",
            "ğŸ¯ Dataset is ready for Prophet training!\n",
            "ğŸƒ View run Final_NaN_Cleanup_Lag_Features at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/5f09cfc2865b4e409ae582047bb6db43\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "ğŸ” Final data quality check: dfed40fb2d1b4773851dc04add48370c\n",
            "ğŸ“Š Data Quality Report:\n",
            "   âœ… Total NaNs: 0\n",
            "   âœ… Infinite values: 0\n",
            "   âœ… Negative sales: 1,285 (0.30%)\n",
            "   âœ… Duplicate rows: 0\n",
            "   âœ… Date issues: 0\n",
            "\n",
            "ğŸ“ˆ Sales Statistics:\n",
            "   Min: $-4,988.94\n",
            "   Max: $693,099.36\n",
            "   Mean: $15,981.26\n",
            "   Std: $22,711.18\n",
            "\n",
            "ğŸ‰ EXCELLENT: No critical data quality issues found!\n",
            "ğŸ“¦ Dataset is ready for Prophet modeling!\n",
            "\n",
            "ğŸ“‹ Sample of cleaned dataset:\n",
            " Store  Dept       Date  Weekly_Sales  IsHoliday  Weekly_Sales_lag_1  Weekly_Sales_rolling_mean_4  Weekly_Sales_ewm_0.3\n",
            "     1     1 2010-02-05      24924.50      False            24924.50                 24924.500000          24924.500000\n",
            "     1     1 2010-02-12      46039.49       True            24924.50                 35481.995000          37345.082353\n",
            "     1     1 2010-02-19      41595.55      False            46039.49                 37519.846667          39285.935160\n",
            "     1     1 2010-02-26      19403.54      False            41595.55                 32990.770000          31436.588472\n",
            "     1     1 2010-03-05      21827.90      False            19403.54                 32216.620000          27971.625625\n",
            "     1     1 2010-03-12      21043.39      False            21827.90                 25967.595000          25616.020330\n",
            "     1     1 2010-03-19      22136.64      False            21043.39                 21102.867500          24478.528922\n",
            "     1     1 2010-03-26      26229.21      False            22136.64                 22809.285000          25035.862412\n",
            "     1     1 2010-04-02      57258.43      False            26229.21                 31666.917500          35109.125175\n",
            "     1     1 2010-04-09      42960.91      False            57258.43                 37146.297500          37533.132840\n",
            "ğŸƒ View run Data_Quality_Final_Verification at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/dfed40fb2d1b4773851dc04add48370c\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Holiday-Aware Outlier Detection and Filtering\n",
        "with mlflow.start_run(run_name=\"Holiday_Aware_Outlier_Detection\") as run:\n",
        "\n",
        "    print(f\"ğŸ„ Starting holiday-aware outlier detection: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"outlier_detection_method\", \"holiday_aware_separate_thresholds\")\n",
        "    mlflow.log_param(\"original_dataset_shape\", f\"{train_final_clean.shape[0]}x{train_final_clean.shape[1]}\")\n",
        "\n",
        "    # Create copy for outlier analysis\n",
        "    train_holiday_analysis = train_final_clean.copy()\n",
        "\n",
        "    # 1. Separate Holiday vs Non-Holiday Data\n",
        "    print(\"ğŸ” Analyzing Holiday vs Non-Holiday sales patterns...\")\n",
        "\n",
        "    holiday_data = train_holiday_analysis[train_holiday_analysis['IsHoliday'] == True]\n",
        "    non_holiday_data = train_holiday_analysis[train_holiday_analysis['IsHoliday'] == False]\n",
        "\n",
        "    print(f\"   Holiday weeks: {len(holiday_data):,} records ({len(holiday_data)/len(train_holiday_analysis)*100:.1f}%)\")\n",
        "    print(f\"   Non-holiday weeks: {len(non_holiday_data):,} records ({len(non_holiday_data)/len(train_holiday_analysis)*100:.1f}%)\")\n",
        "\n",
        "    mlflow.log_metric(\"holiday_records\", len(holiday_data))\n",
        "    mlflow.log_metric(\"non_holiday_records\", len(non_holiday_data))\n",
        "\n",
        "    # 2. Statistical Analysis for Each Group\n",
        "    def calculate_stats(data, label):\n",
        "        stats = {\n",
        "            'count': len(data),\n",
        "            'min': data['Weekly_Sales'].min(),\n",
        "            'max': data['Weekly_Sales'].max(),\n",
        "            'mean': data['Weekly_Sales'].mean(),\n",
        "            'median': data['Weekly_Sales'].median(),\n",
        "            'std': data['Weekly_Sales'].std(),\n",
        "            'q1': data['Weekly_Sales'].quantile(0.25),\n",
        "            'q3': data['Weekly_Sales'].quantile(0.75),\n",
        "            'q95': data['Weekly_Sales'].quantile(0.95),\n",
        "            'q99': data['Weekly_Sales'].quantile(0.99),\n",
        "            'negative_count': (data['Weekly_Sales'] < 0).sum()\n",
        "        }\n",
        "        stats['iqr'] = stats['q3'] - stats['q1']\n",
        "\n",
        "        print(f\"\\nğŸ“Š {label} Sales Statistics:\")\n",
        "        print(f\"   Count: {stats['count']:,}\")\n",
        "        print(f\"   Mean: ${stats['mean']:,.2f}\")\n",
        "        print(f\"   Median: ${stats['median']:,.2f}\")\n",
        "        print(f\"   Std: ${stats['std']:,.2f}\")\n",
        "        print(f\"   Min: ${stats['min']:,.2f}\")\n",
        "        print(f\"   Max: ${stats['max']:,.2f}\")\n",
        "        print(f\"   95th percentile: ${stats['q95']:,.2f}\")\n",
        "        print(f\"   99th percentile: ${stats['q99']:,.2f}\")\n",
        "        print(f\"   Negative sales: {stats['negative_count']:,}\")\n",
        "\n",
        "        # Log to MLflow\n",
        "        for key, value in stats.items():\n",
        "            mlflow.log_metric(f\"{label.lower()}_{key}\", value)\n",
        "\n",
        "        return stats\n",
        "\n",
        "    holiday_stats = calculate_stats(holiday_data, \"Holiday\")\n",
        "    non_holiday_stats = calculate_stats(non_holiday_data, \"Non-Holiday\")\n",
        "\n",
        "    # 3. Compare Holiday vs Non-Holiday patterns\n",
        "    print(f\"\\nğŸ” Holiday vs Non-Holiday Comparison:\")\n",
        "    mean_ratio = holiday_stats['mean'] / non_holiday_stats['mean']\n",
        "    median_ratio = holiday_stats['median'] / non_holiday_stats['median']\n",
        "    max_ratio = holiday_stats['max'] / non_holiday_stats['max']\n",
        "\n",
        "    print(f\"   Holiday mean is {mean_ratio:.1f}x higher than non-holiday\")\n",
        "    print(f\"   Holiday median is {median_ratio:.1f}x higher than non-holiday\")\n",
        "    print(f\"   Holiday max is {max_ratio:.1f}x higher than non-holiday\")\n",
        "\n",
        "    mlflow.log_metric(\"holiday_mean_ratio\", mean_ratio)\n",
        "    mlflow.log_metric(\"holiday_median_ratio\", median_ratio)\n",
        "    mlflow.log_metric(\"holiday_max_ratio\", max_ratio)\n",
        "\n",
        "    # 4. Holiday-Specific Outlier Detection\n",
        "    print(f\"\\nğŸ¯ Applying separate outlier detection for Holiday vs Non-Holiday...\")\n",
        "\n",
        "    def detect_outliers_by_group(data, stats, label, iqr_multiplier=2.5):\n",
        "        \"\"\"Detect outliers within a specific group (holiday/non-holiday)\"\"\"\n",
        "\n",
        "        # Method 1: IQR-based\n",
        "        lower_bound = stats['q1'] - iqr_multiplier * stats['iqr']\n",
        "        upper_bound = stats['q3'] + iqr_multiplier * stats['iqr']\n",
        "\n",
        "        # Method 2: Percentile-based (keep 99.5% of data in this group)\n",
        "        percentile_lower = data['Weekly_Sales'].quantile(0.0025)  # 0.25%\n",
        "        percentile_upper = data['Weekly_Sales'].quantile(0.9975)  # 99.75%\n",
        "\n",
        "        # Method 3: Z-score within group\n",
        "        z_scores = np.abs((data['Weekly_Sales'] - stats['mean']) / stats['std'])\n",
        "        z_threshold = 3.5 if label == \"Holiday\" else 3.0  # More lenient for holidays\n",
        "\n",
        "        # For negative sales - different thresholds for holidays vs non-holidays\n",
        "        if label == \"Holiday\":\n",
        "            negative_threshold = -15000  # Allow more returns during holidays\n",
        "        else:\n",
        "            negative_threshold = -8000   # Stricter for non-holidays\n",
        "\n",
        "        outlier_flags = {\n",
        "            'iqr': (data['Weekly_Sales'] < lower_bound) | (data['Weekly_Sales'] > upper_bound),\n",
        "            'percentile': (data['Weekly_Sales'] < percentile_lower) | (data['Weekly_Sales'] > percentile_upper),\n",
        "            'z_score': z_scores > z_threshold,\n",
        "            'extreme_negative': data['Weekly_Sales'] < negative_threshold\n",
        "        }\n",
        "\n",
        "        # Count outliers by method\n",
        "        outlier_counts = {method: flags.sum() for method, flags in outlier_flags.items()}\n",
        "\n",
        "        print(f\"\\n   {label} Outlier Detection Results:\")\n",
        "        print(f\"   IQR bounds: ${lower_bound:,.0f} to ${upper_bound:,.0f}\")\n",
        "        print(f\"   Percentile bounds: ${percentile_lower:,.0f} to ${percentile_upper:,.0f}\")\n",
        "        print(f\"   Z-score threshold: {z_threshold}\")\n",
        "        print(f\"   Negative threshold: ${negative_threshold:,.0f}\")\n",
        "\n",
        "        for method, count in outlier_counts.items():\n",
        "            percentage = (count / len(data)) * 100\n",
        "            print(f\"     {method}: {count:,} outliers ({percentage:.2f}%)\")\n",
        "            mlflow.log_metric(f\"{label.lower()}_outliers_{method}\", count)\n",
        "            mlflow.log_metric(f\"{label.lower()}_outliers_{method}_pct\", percentage)\n",
        "\n",
        "        # Smart combination: Use moderate IQR + extreme negative filtering\n",
        "        combined_outliers = outlier_flags['iqr'] | outlier_flags['extreme_negative']\n",
        "\n",
        "        return combined_outliers, {\n",
        "            'bounds': (lower_bound, upper_bound),\n",
        "            'negative_threshold': negative_threshold,\n",
        "            'outlier_counts': outlier_counts,\n",
        "            'combined_count': combined_outliers.sum()\n",
        "        }\n",
        "\n",
        "    # Apply outlier detection to each group\n",
        "    holiday_outliers, holiday_info = detect_outliers_by_group(\n",
        "        holiday_data, holiday_stats, \"Holiday\", iqr_multiplier=3.0  # More lenient for holidays\n",
        "    )\n",
        "\n",
        "    non_holiday_outliers, non_holiday_info = detect_outliers_by_group(\n",
        "        non_holiday_data, non_holiday_stats, \"Non-Holiday\", iqr_multiplier=2.5  # Standard for non-holidays\n",
        "    )\n",
        "\n",
        "    # 5. Store-Department Specific Analysis (Holiday-Aware)\n",
        "    print(f\"\\nğŸª Store-Department specific outlier detection (holiday-aware)...\")\n",
        "\n",
        "    def detect_store_dept_outliers_holiday_aware(group, multiplier_holiday=3.5, multiplier_non_holiday=2.5):\n",
        "        \"\"\"Detect outliers within each store-department, considering holiday status\"\"\"\n",
        "        if len(group) < 10:  # Skip if too few observations\n",
        "            return pd.Series([False] * len(group), index=group.index)\n",
        "\n",
        "        outliers = pd.Series([False] * len(group), index=group.index)\n",
        "\n",
        "        # Separate holiday and non-holiday within this store-dept\n",
        "        holiday_mask = group['IsHoliday'] == True\n",
        "        non_holiday_mask = group['IsHoliday'] == False\n",
        "\n",
        "        # Holiday outliers within this store-dept\n",
        "        if holiday_mask.sum() >= 3:  # Need at least 3 holiday observations\n",
        "            holiday_group = group[holiday_mask]['Weekly_Sales']\n",
        "            q1_h = holiday_group.quantile(0.25)\n",
        "            q3_h = holiday_group.quantile(0.75)\n",
        "            iqr_h = q3_h - q1_h\n",
        "\n",
        "            if iqr_h > 0:  # Avoid division by zero\n",
        "                lower_h = q1_h - multiplier_holiday * iqr_h\n",
        "                upper_h = q3_h + multiplier_holiday * iqr_h\n",
        "                holiday_outliers = (holiday_group < lower_h) | (holiday_group > upper_h)\n",
        "                outliers[holiday_outliers.index] = holiday_outliers\n",
        "\n",
        "        # Non-holiday outliers within this store-dept\n",
        "        if non_holiday_mask.sum() >= 5:  # Need at least 5 non-holiday observations\n",
        "            non_holiday_group = group[non_holiday_mask]['Weekly_Sales']\n",
        "            q1_nh = non_holiday_group.quantile(0.25)\n",
        "            q3_nh = non_holiday_group.quantile(0.75)\n",
        "            iqr_nh = q3_nh - q1_nh\n",
        "\n",
        "            if iqr_nh > 0:  # Avoid division by zero\n",
        "                lower_nh = q1_nh - multiplier_non_holiday * iqr_nh\n",
        "                upper_nh = q3_nh + multiplier_non_holiday * iqr_nh\n",
        "                non_holiday_outliers = (non_holiday_group < lower_nh) | (non_holiday_group > upper_nh)\n",
        "                outliers[non_holiday_outliers.index] = non_holiday_outliers\n",
        "\n",
        "        return outliers\n",
        "\n",
        "    # Apply store-department specific detection\n",
        "    store_dept_outliers = train_holiday_analysis.groupby(['Store', 'Dept']).apply(\n",
        "        detect_store_dept_outliers_holiday_aware\n",
        "    ).reset_index(drop=True)\n",
        "\n",
        "    store_dept_outlier_count = store_dept_outliers.sum()\n",
        "    print(f\"   Store-Dept specific outliers: {store_dept_outlier_count:,} ({store_dept_outlier_count/len(train_holiday_analysis)*100:.2f}%)\")\n",
        "    mlflow.log_metric(\"store_dept_outliers\", store_dept_outlier_count)\n",
        "\n",
        "    # 6. Combine Results and Apply Filtering\n",
        "    print(f\"\\nğŸ§  Combining results and applying smart filtering...\")\n",
        "\n",
        "    # Create outlier mask for the full dataset\n",
        "    full_outlier_mask = pd.Series([False] * len(train_holiday_analysis), index=train_holiday_analysis.index)\n",
        "\n",
        "    # Apply holiday-specific outliers\n",
        "    holiday_indices = train_holiday_analysis[train_holiday_analysis['IsHoliday'] == True].index\n",
        "    full_outlier_mask[holiday_indices] = holiday_outliers.values\n",
        "\n",
        "    # Apply non-holiday-specific outliers\n",
        "    non_holiday_indices = train_holiday_analysis[train_holiday_analysis['IsHoliday'] == False].index\n",
        "    full_outlier_mask[non_holiday_indices] = non_holiday_outliers.values\n",
        "\n",
        "    # Optional: Also include store-department specific outliers\n",
        "    # full_outlier_mask = full_outlier_mask | store_dept_outliers\n",
        "\n",
        "    # Apply filtering\n",
        "    train_holiday_filtered = train_holiday_analysis[~full_outlier_mask].copy()\n",
        "\n",
        "    # 7. Results Summary\n",
        "    original_count = len(train_holiday_analysis)\n",
        "    filtered_count = len(train_holiday_filtered)\n",
        "    removed_count = original_count - filtered_count\n",
        "    removal_percentage = (removed_count / original_count) * 100\n",
        "\n",
        "    # Holiday-specific removal stats\n",
        "    holiday_removed = holiday_outliers.sum()\n",
        "    non_holiday_removed = non_holiday_outliers.sum()\n",
        "    holiday_removal_rate = (holiday_removed / len(holiday_data)) * 100\n",
        "    non_holiday_removal_rate = (non_holiday_removed / len(non_holiday_data)) * 100\n",
        "\n",
        "    print(f\"\\nâœ… Holiday-aware filtering completed!\")\n",
        "    print(f\"   Original rows: {original_count:,}\")\n",
        "    print(f\"   Filtered rows: {filtered_count:,}\")\n",
        "    print(f\"   Total removed: {removed_count:,} ({removal_percentage:.2f}%)\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   Holiday outliers removed: {holiday_removed:,} ({holiday_removal_rate:.2f}% of holiday data)\")\n",
        "    print(f\"   Non-holiday outliers removed: {non_holiday_removed:,} ({non_holiday_removal_rate:.2f}% of non-holiday data)\")\n",
        "\n",
        "    # Log final metrics\n",
        "    mlflow.log_metric(\"total_removed\", removed_count)\n",
        "    mlflow.log_metric(\"removal_percentage\", removal_percentage)\n",
        "    mlflow.log_metric(\"holiday_removed\", holiday_removed)\n",
        "    mlflow.log_metric(\"non_holiday_removed\", non_holiday_removed)\n",
        "    mlflow.log_metric(\"holiday_removal_rate\", holiday_removal_rate)\n",
        "    mlflow.log_metric(\"non_holiday_removal_rate\", non_holiday_removal_rate)\n",
        "\n",
        "    # 8. Final dataset verification\n",
        "    print(f\"\\nğŸ“Š Final filtered dataset analysis:\")\n",
        "    final_holiday_data = train_holiday_filtered[train_holiday_filtered['IsHoliday'] == True]\n",
        "    final_non_holiday_data = train_holiday_filtered[train_holiday_filtered['IsHoliday'] == False]\n",
        "\n",
        "    print(f\"   Holiday weeks remaining: {len(final_holiday_data):,}\")\n",
        "    print(f\"   Non-holiday weeks remaining: {len(final_non_holiday_data):,}\")\n",
        "    print(f\"   Holiday mean sales: ${final_holiday_data['Weekly_Sales'].mean():,.2f}\")\n",
        "    print(f\"   Non-holiday mean sales: ${final_non_holiday_data['Weekly_Sales'].mean():,.2f}\")\n",
        "    print(f\"   Holiday/Non-holiday ratio: {final_holiday_data['Weekly_Sales'].mean() / final_non_holiday_data['Weekly_Sales'].mean():.1f}x\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ Holiday-aware outlier filtering completed!\")\n",
        "    print(f\"ğŸ“Š Final dataset shape: {train_holiday_filtered.shape}\")\n",
        "    print(f\"ğŸ¯ Dataset preserves holiday sales patterns and is ready for Prophet training!\")\n",
        "\n",
        "    # Save detailed summary\n",
        "    holiday_aware_summary = {\n",
        "        'original_shape': train_holiday_analysis.shape,\n",
        "        'filtered_shape': train_holiday_filtered.shape,\n",
        "        'holiday_stats': holiday_stats,\n",
        "        'non_holiday_stats': non_holiday_stats,\n",
        "        'removal_summary': {\n",
        "            'total_removed': removed_count,\n",
        "            'holiday_removed': holiday_removed,\n",
        "            'non_holiday_removed': non_holiday_removed,\n",
        "            'removal_rates': {\n",
        "                'overall': removal_percentage,\n",
        "                'holiday': holiday_removal_rate,\n",
        "                'non_holiday': non_holiday_removal_rate\n",
        "            }\n",
        "        },\n",
        "        'filtering_parameters': {\n",
        "            'holiday_iqr_multiplier': 3.0,\n",
        "            'non_holiday_iqr_multiplier': 2.5,\n",
        "            'holiday_negative_threshold': holiday_info['negative_threshold'],\n",
        "            'non_holiday_negative_threshold': non_holiday_info['negative_threshold']\n",
        "        }\n",
        "    }\n",
        "\n",
        "    mlflow.log_dict(holiday_aware_summary, \"holiday_aware_outlier_summary.json\")\n",
        "\n",
        "# End MLflow run\n",
        "mlflow.end_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pn-U3cdVNDGv",
        "outputId": "fc78afba-42d4-4fa1-df78-37b0aa2b1fb0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ„ Starting holiday-aware outlier detection: 608ea81cd34e48829db30c91487f2c08\n",
            "ğŸ” Analyzing Holiday vs Non-Holiday sales patterns...\n",
            "   Holiday weeks: 29,661 records (7.0%)\n",
            "   Non-holiday weeks: 391,909 records (93.0%)\n",
            "\n",
            "ğŸ“Š Holiday Sales Statistics:\n",
            "   Count: 29,661\n",
            "   Mean: $17,035.82\n",
            "   Median: $7,947.74\n",
            "   Std: $27,222.00\n",
            "   Min: $-798.00\n",
            "   Max: $693,099.36\n",
            "   95th percentile: $63,916.21\n",
            "   99th percentile: $118,773.24\n",
            "   Negative sales: 98\n",
            "\n",
            "ğŸ“Š Non-Holiday Sales Statistics:\n",
            "   Count: 391,909\n",
            "   Mean: $15,901.45\n",
            "   Median: $7,589.95\n",
            "   Std: $22,330.75\n",
            "   Min: $-4,988.94\n",
            "   Max: $406,988.63\n",
            "   95th percentile: $61,018.50\n",
            "   99th percentile: $105,704.59\n",
            "   Negative sales: 1,187\n",
            "\n",
            "ğŸ” Holiday vs Non-Holiday Comparison:\n",
            "   Holiday mean is 1.1x higher than non-holiday\n",
            "   Holiday median is 1.0x higher than non-holiday\n",
            "   Holiday max is 1.7x higher than non-holiday\n",
            "\n",
            "ğŸ¯ Applying separate outlier detection for Holiday vs Non-Holiday...\n",
            "\n",
            "   Holiday Outlier Detection Results:\n",
            "   IQR bounds: $-55,254 to $78,521\n",
            "   Percentile bounds: $-4 to $193,132\n",
            "   Z-score threshold: 3.5\n",
            "   Negative threshold: $-15,000\n",
            "     iqr: 897 outliers (3.02%)\n",
            "     percentile: 150 outliers (0.51%)\n",
            "     z_score: 339 outliers (1.14%)\n",
            "     extreme_negative: 0 outliers (0.00%)\n",
            "\n",
            "   Non-Holiday Outlier Detection Results:\n",
            "   IQR bounds: $-43,049 to $65,260\n",
            "   Percentile bounds: $-3 to $150,350\n",
            "   Z-score threshold: 3.0\n",
            "   Negative threshold: $-8,000\n",
            "     iqr: 16,762 outliers (4.28%)\n",
            "     percentile: 1,958 outliers (0.50%)\n",
            "     z_score: 8,516 outliers (2.17%)\n",
            "     extreme_negative: 0 outliers (0.00%)\n",
            "\n",
            "ğŸª Store-Department specific outlier detection (holiday-aware)...\n",
            "   Store-Dept specific outliers: 8,437 (2.00%)\n",
            "\n",
            "ğŸ§  Combining results and applying smart filtering...\n",
            "\n",
            "âœ… Holiday-aware filtering completed!\n",
            "   Original rows: 421,570\n",
            "   Filtered rows: 403,911\n",
            "   Total removed: 17,659 (4.19%)\n",
            "   \n",
            "   Holiday outliers removed: 897 (3.02% of holiday data)\n",
            "   Non-holiday outliers removed: 16,762 (4.28% of non-holiday data)\n",
            "\n",
            "ğŸ“Š Final filtered dataset analysis:\n",
            "   Holiday weeks remaining: 28,764\n",
            "   Non-holiday weeks remaining: 375,147\n",
            "   Holiday mean sales: $13,719.53\n",
            "   Non-holiday mean sales: $12,453.86\n",
            "   Holiday/Non-holiday ratio: 1.1x\n",
            "\n",
            "ğŸ‰ Holiday-aware outlier filtering completed!\n",
            "ğŸ“Š Final dataset shape: (403911, 73)\n",
            "ğŸ¯ Dataset preserves holiday sales patterns and is ready for Prophet training!\n",
            "ğŸƒ View run Holiday_Aware_Outlier_Detection at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/608ea81cd34e48829db30c91487f2c08\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: Time Series Data Splitting (80/20)\n",
        "with mlflow.start_run(run_name=\"Time_Series_Data_Split_80_20\") as run:\n",
        "\n",
        "    print(f\"â° Starting time series data split: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"split_method\", \"temporal_80_20\")\n",
        "    mlflow.log_param(\"original_data_shape\", f\"{train_holiday_filtered.shape[0]}x{train_holiday_filtered.shape[1]}\")\n",
        "\n",
        "    # 1. Prepare and sort data\n",
        "    print(\"ğŸ“Š Preparing data for time series split...\")\n",
        "\n",
        "    # Create a copy and sort by date\n",
        "    df_split = train_holiday_filtered.copy()\n",
        "    df_split = df_split.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    print(f\"   Dataset shape: {df_split.shape}\")\n",
        "    print(f\"   Date range: {df_split['Date'].min()} to {df_split['Date'].max()}\")\n",
        "    print(f\"   Total records: {len(df_split):,}\")\n",
        "\n",
        "    # 2. Analyze date distribution\n",
        "    print(f\"\\nğŸ“… Analyzing date distribution...\")\n",
        "\n",
        "    # Get unique dates and sort them\n",
        "    unique_dates = sorted(df_split['Date'].unique())\n",
        "    total_dates = len(unique_dates)\n",
        "\n",
        "    print(f\"   Unique dates: {total_dates}\")\n",
        "    print(f\"   First date: {unique_dates[0]}\")\n",
        "    print(f\"   Last date: {unique_dates[-1]}\")\n",
        "    print(f\"   Date span: {(unique_dates[-1] - unique_dates[0]).days} days\")\n",
        "\n",
        "    # Records per date\n",
        "    records_per_date = df_split['Date'].value_counts().sort_index()\n",
        "    print(f\"   Average records per date: {records_per_date.mean():.1f}\")\n",
        "    print(f\"   Min records per date: {records_per_date.min()}\")\n",
        "    print(f\"   Max records per date: {records_per_date.max()}\")\n",
        "\n",
        "    # 3. Calculate split point (80% of time period)\n",
        "    print(f\"\\nğŸ”ª Calculating 80/20 time split...\")\n",
        "\n",
        "    split_idx = int(total_dates * 0.8)\n",
        "    split_date = unique_dates[split_idx]\n",
        "\n",
        "    print(f\"   Split index: {split_idx} out of {total_dates} dates\")\n",
        "    print(f\"   Split date: {split_date}\")\n",
        "    print(f\"   \")\n",
        "    print(f\"   Training period: {unique_dates[0]} to {unique_dates[split_idx-1]}\")\n",
        "    print(f\"   Validation period: {split_date} to {unique_dates[-1]}\")\n",
        "\n",
        "    # 4. Create train/validation splits\n",
        "    print(f\"\\nâœ‚ï¸ Creating train/validation splits...\")\n",
        "\n",
        "    train_mask = df_split['Date'] < split_date\n",
        "    val_mask = df_split['Date'] >= split_date\n",
        "\n",
        "    train_data = df_split[train_mask].copy()\n",
        "    val_data = df_split[val_mask].copy()\n",
        "\n",
        "    # 5. Split analysis\n",
        "    print(f\"\\nğŸ“Š Split Analysis:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    # Basic counts\n",
        "    print(f\"Training Set:\")\n",
        "    print(f\"   Records: {len(train_data):,} ({len(train_data)/len(df_split)*100:.1f}%)\")\n",
        "    print(f\"   Date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "    print(f\"   Unique dates: {train_data['Date'].nunique()}\")\n",
        "    print(f\"   Stores: {train_data['Store'].nunique()}\")\n",
        "    print(f\"   Departments: {train_data['Dept'].nunique()}\")\n",
        "    print(f\"   Store-Dept combinations: {train_data.groupby(['Store', 'Dept']).ngroups}\")\n",
        "\n",
        "    print(f\"\\nValidation Set:\")\n",
        "    print(f\"   Records: {len(val_data):,} ({len(val_data)/len(df_split)*100:.1f}%)\")\n",
        "    print(f\"   Date range: {val_data['Date'].min()} to {val_data['Date'].max()}\")\n",
        "    print(f\"   Unique dates: {val_data['Date'].nunique()}\")\n",
        "    print(f\"   Stores: {val_data['Store'].nunique()}\")\n",
        "    print(f\"   Departments: {val_data['Dept'].nunique()}\")\n",
        "    print(f\"   Store-Dept combinations: {val_data.groupby(['Store', 'Dept']).ngroups}\")\n",
        "\n",
        "    # 6. Sales statistics comparison\n",
        "    print(f\"\\nğŸ’° Sales Statistics Comparison:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    train_sales_stats = {\n",
        "        'count': len(train_data),\n",
        "        'mean': train_data['Weekly_Sales'].mean(),\n",
        "        'median': train_data['Weekly_Sales'].median(),\n",
        "        'std': train_data['Weekly_Sales'].std(),\n",
        "        'min': train_data['Weekly_Sales'].min(),\n",
        "        'max': train_data['Weekly_Sales'].max(),\n",
        "        'negative_count': (train_data['Weekly_Sales'] < 0).sum()\n",
        "    }\n",
        "\n",
        "    val_sales_stats = {\n",
        "        'count': len(val_data),\n",
        "        'mean': val_data['Weekly_Sales'].mean(),\n",
        "        'median': val_data['Weekly_Sales'].median(),\n",
        "        'std': val_data['Weekly_Sales'].std(),\n",
        "        'min': val_data['Weekly_Sales'].min(),\n",
        "        'max': val_data['Weekly_Sales'].max(),\n",
        "        'negative_count': (val_data['Weekly_Sales'] < 0).sum()\n",
        "    }\n",
        "\n",
        "    print(f\"Training Sales Statistics:\")\n",
        "    print(f\"   Count: {train_sales_stats['count']:,}\")\n",
        "    print(f\"   Mean: ${train_sales_stats['mean']:,.2f}\")\n",
        "    print(f\"   Median: ${train_sales_stats['median']:,.2f}\")\n",
        "    print(f\"   Std: ${train_sales_stats['std']:,.2f}\")\n",
        "    print(f\"   Min: ${train_sales_stats['min']:,.2f}\")\n",
        "    print(f\"   Max: ${train_sales_stats['max']:,.2f}\")\n",
        "    print(f\"   Negative sales: {train_sales_stats['negative_count']:,}\")\n",
        "\n",
        "    print(f\"\\nValidation Sales Statistics:\")\n",
        "    print(f\"   Count: {val_sales_stats['count']:,}\")\n",
        "    print(f\"   Mean: ${val_sales_stats['mean']:,.2f}\")\n",
        "    print(f\"   Median: ${val_sales_stats['median']:,.2f}\")\n",
        "    print(f\"   Std: ${val_sales_stats['std']:,.2f}\")\n",
        "    print(f\"   Min: ${val_sales_stats['min']:,.2f}\")\n",
        "    print(f\"   Max: ${val_sales_stats['max']:,.2f}\")\n",
        "    print(f\"   Negative sales: {val_sales_stats['negative_count']:,}\")\n",
        "\n",
        "    # 7. Holiday distribution\n",
        "    print(f\"\\nğŸ„ Holiday Distribution:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    train_holidays = train_data['IsHoliday'].value_counts()\n",
        "    val_holidays = val_data['IsHoliday'].value_counts()\n",
        "\n",
        "    print(f\"Training Set Holidays:\")\n",
        "    print(f\"   Holiday weeks: {train_holidays.get(True, 0):,}\")\n",
        "    print(f\"   Non-holiday weeks: {train_holidays.get(False, 0):,}\")\n",
        "    print(f\"   Holiday percentage: {(train_holidays.get(True, 0)/len(train_data)*100):.2f}%\")\n",
        "\n",
        "    print(f\"\\nValidation Set Holidays:\")\n",
        "    print(f\"   Holiday weeks: {val_holidays.get(True, 0):,}\")\n",
        "    print(f\"   Non-holiday weeks: {val_holidays.get(False, 0):,}\")\n",
        "    print(f\"   Holiday percentage: {(val_holidays.get(True, 0)/len(val_data)*100):.2f}%\")\n",
        "\n",
        "    # 8. Store and Department coverage\n",
        "    print(f\"\\nğŸª Store and Department Coverage:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    # Check if all stores/departments are in both sets\n",
        "    train_stores = set(train_data['Store'].unique())\n",
        "    val_stores = set(val_data['Store'].unique())\n",
        "    train_depts = set(train_data['Dept'].unique())\n",
        "    val_depts = set(val_data['Dept'].unique())\n",
        "\n",
        "    missing_stores_in_val = train_stores - val_stores\n",
        "    missing_depts_in_val = train_depts - val_depts\n",
        "\n",
        "    print(f\"Store Coverage:\")\n",
        "    print(f\"   Training stores: {len(train_stores)}\")\n",
        "    print(f\"   Validation stores: {len(val_stores)}\")\n",
        "    print(f\"   Missing stores in validation: {len(missing_stores_in_val)}\")\n",
        "    if missing_stores_in_val:\n",
        "        print(f\"     Missing stores: {sorted(missing_stores_in_val)}\")\n",
        "\n",
        "    print(f\"\\nDepartment Coverage:\")\n",
        "    print(f\"   Training departments: {len(train_depts)}\")\n",
        "    print(f\"   Validation departments: {len(val_depts)}\")\n",
        "    print(f\"   Missing departments in validation: {len(missing_depts_in_val)}\")\n",
        "    if missing_depts_in_val:\n",
        "        print(f\"     Missing departments: {sorted(missing_depts_in_val)}\")\n",
        "\n",
        "    # 9. Time series continuity check\n",
        "    print(f\"\\nâ° Time Series Continuity Check:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    # Check for gaps in time series\n",
        "    def check_time_gaps(data, name):\n",
        "        data_sorted = data.sort_values('Date')\n",
        "        date_diffs = data_sorted['Date'].diff().dropna()\n",
        "\n",
        "        # Expected difference is 7 days (weekly data)\n",
        "        expected_diff = pd.Timedelta(days=7)\n",
        "        gaps = date_diffs[date_diffs != expected_diff]\n",
        "\n",
        "        print(f\"{name} Time Gaps:\")\n",
        "        print(f\"   Expected interval: 7 days\")\n",
        "        print(f\"   Actual intervals found: {date_diffs.unique()}\")\n",
        "        print(f\"   Number of gaps: {len(gaps)}\")\n",
        "        if len(gaps) > 0:\n",
        "            print(f\"   Gap sizes: {gaps.unique()}\")\n",
        "\n",
        "    check_time_gaps(train_data, \"Training\")\n",
        "    check_time_gaps(val_data, \"Validation\")\n",
        "\n",
        "    # 10. Sample data preview\n",
        "    print(f\"\\nğŸ“‹ Sample Data Preview:\")\n",
        "    print(f\"=\" * 60)\n",
        "\n",
        "    print(f\"Training Data Sample (first 5 rows):\")\n",
        "    print(train_data[['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']].head())\n",
        "\n",
        "    print(f\"\\nValidation Data Sample (first 5 rows):\")\n",
        "    print(val_data[['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']].head())\n",
        "\n",
        "    # 11. Log metrics to MLflow\n",
        "    mlflow.log_param(\"split_date\", str(split_date))\n",
        "    mlflow.log_param(\"train_start_date\", str(train_data['Date'].min()))\n",
        "    mlflow.log_param(\"train_end_date\", str(train_data['Date'].max()))\n",
        "    mlflow.log_param(\"val_start_date\", str(val_data['Date'].min()))\n",
        "    mlflow.log_param(\"val_end_date\", str(val_data['Date'].max()))\n",
        "\n",
        "    mlflow.log_metric(\"train_records\", len(train_data))\n",
        "    mlflow.log_metric(\"val_records\", len(val_data))\n",
        "    mlflow.log_metric(\"train_percentage\", len(train_data)/len(df_split)*100)\n",
        "    mlflow.log_metric(\"val_percentage\", len(val_data)/len(df_split)*100)\n",
        "\n",
        "    mlflow.log_metric(\"train_mean_sales\", train_sales_stats['mean'])\n",
        "    mlflow.log_metric(\"val_mean_sales\", val_sales_stats['mean'])\n",
        "    mlflow.log_metric(\"train_holiday_count\", train_holidays.get(True, 0))\n",
        "    mlflow.log_metric(\"val_holiday_count\", val_holidays.get(True, 0))\n",
        "\n",
        "    # 12. Save split datasets for later use\n",
        "    print(f\"\\nğŸ’¾ Saving split datasets...\")\n",
        "\n",
        "    # Save as variables for next steps\n",
        "    train_final = train_data.copy()\n",
        "    val_final = val_data.copy()\n",
        "\n",
        "    print(f\"   âœ… Training data saved: {len(train_final):,} records\")\n",
        "    print(f\"   âœ… Validation data saved: {len(val_final):,} records\")\n",
        "\n",
        "    # Save split summary\n",
        "    split_summary = {\n",
        "        'split_method': 'temporal_80_20',\n",
        "        'split_date': str(split_date),\n",
        "        'original_data_shape': list(df_split.shape),\n",
        "        'train_data_shape': list(train_data.shape),\n",
        "        'val_data_shape': list(val_data.shape),\n",
        "        'train_date_range': [str(train_data['Date'].min()), str(train_data['Date'].max())],\n",
        "        'val_date_range': [str(val_data['Date'].min()), str(val_data['Date'].max())],\n",
        "        'train_sales_stats': train_sales_stats,\n",
        "        'val_sales_stats': val_sales_stats,\n",
        "        'coverage_check': {\n",
        "            'missing_stores_in_val': list(missing_stores_in_val),\n",
        "            'missing_depts_in_val': list(missing_depts_in_val)\n",
        "        }\n",
        "    }\n",
        "\n",
        "    mlflow.log_dict(split_summary, \"split_summary.json\")\n",
        "\n",
        "    print(f\"\\nğŸ‰ Time Series Split Completed Successfully!\")\n",
        "    print(f\"ğŸ“Š Ready for model training!\")\n",
        "    print(f\"âš ï¸  Note: This is a proper time series split - no future data leakage\")\n",
        "\n",
        "# End MLflow run\n",
        "mlflow.end_run()\n",
        "\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(f\"ğŸ“Š SPLIT SUMMARY\")\n",
        "print(f\"=\"*80)\n",
        "print(f\"âœ… Training Set: {len(train_final):,} records\")\n",
        "print(f\"âœ… Validation Set: {len(val_final):,} records\")\n",
        "print(f\"ğŸ“… Split Date: {split_date}\")\n",
        "print(f\"ğŸ¯ Ready for XGBoost training!\")\n",
        "print(f\"=\"*80)"
      ],
      "metadata": {
        "id": "bkKMHKC2TUor",
        "outputId": "c73156aa-0df9-42bf-98f3-840c3676674f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/api/2.0/mlflow/runs/create\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "â° Starting time series data split: f9fc0fa0a73a4048be3b3b374a5ea53b\n",
            "ğŸ“Š Preparing data for time series split...\n",
            "   Dataset shape: (403911, 73)\n",
            "   Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "   Total records: 403,911\n",
            "\n",
            "ğŸ“… Analyzing date distribution...\n",
            "   Unique dates: 143\n",
            "   First date: 2010-02-05 00:00:00\n",
            "   Last date: 2012-10-26 00:00:00\n",
            "   Date span: 994 days\n",
            "   Average records per date: 2824.6\n",
            "   Min records per date: 2627\n",
            "   Max records per date: 2941\n",
            "\n",
            "ğŸ”ª Calculating 80/20 time split...\n",
            "   Split index: 114 out of 143 dates\n",
            "   Split date: 2012-04-13 00:00:00\n",
            "   \n",
            "   Training period: 2010-02-05 00:00:00 to 2012-04-06 00:00:00\n",
            "   Validation period: 2012-04-13 00:00:00 to 2012-10-26 00:00:00\n",
            "\n",
            "âœ‚ï¸ Creating train/validation splits...\n",
            "\n",
            "ğŸ“Š Split Analysis:\n",
            "============================================================\n",
            "Training Set:\n",
            "   Records: 321,636 (79.6%)\n",
            "   Date range: 2010-02-05 00:00:00 to 2012-04-06 00:00:00\n",
            "   Unique dates: 114\n",
            "   Stores: 45\n",
            "   Departments: 81\n",
            "   Store-Dept combinations: 3279\n",
            "\n",
            "Validation Set:\n",
            "   Records: 82,275 (20.4%)\n",
            "   Date range: 2012-04-13 00:00:00 to 2012-10-26 00:00:00\n",
            "   Unique dates: 29\n",
            "   Stores: 45\n",
            "   Departments: 81\n",
            "   Store-Dept combinations: 3099\n",
            "\n",
            "ğŸ’° Sales Statistics Comparison:\n",
            "============================================================\n",
            "Training Sales Statistics:\n",
            "   Count: 321,636\n",
            "   Mean: $12,553.69\n",
            "   Median: $7,015.84\n",
            "   Std: $14,502.04\n",
            "   Min: $-4,988.94\n",
            "   Max: $78,494.29\n",
            "   Negative sales: 986\n",
            "\n",
            "Validation Sales Statistics:\n",
            "   Count: 82,275\n",
            "   Mean: $12,506.07\n",
            "   Median: $6,861.45\n",
            "   Std: $14,531.78\n",
            "   Min: $-771.90\n",
            "   Max: $78,383.09\n",
            "   Negative sales: 299\n",
            "\n",
            "ğŸ„ Holiday Distribution:\n",
            "============================================================\n",
            "Training Set Holidays:\n",
            "   Holiday weeks: 25,885\n",
            "   Non-holiday weeks: 295,751\n",
            "   Holiday percentage: 8.05%\n",
            "\n",
            "Validation Set Holidays:\n",
            "   Holiday weeks: 2,879\n",
            "   Non-holiday weeks: 79,396\n",
            "   Holiday percentage: 3.50%\n",
            "\n",
            "ğŸª Store and Department Coverage:\n",
            "============================================================\n",
            "Store Coverage:\n",
            "   Training stores: 45\n",
            "   Validation stores: 45\n",
            "   Missing stores in validation: 0\n",
            "\n",
            "Department Coverage:\n",
            "   Training departments: 81\n",
            "   Validation departments: 81\n",
            "   Missing departments in validation: 0\n",
            "\n",
            "â° Time Series Continuity Check:\n",
            "============================================================\n",
            "Training Time Gaps:\n",
            "   Expected interval: 7 days\n",
            "   Actual intervals found: <TimedeltaArray>\n",
            "['0 days', '7 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "   Number of gaps: 321522\n",
            "   Gap sizes: <TimedeltaArray>\n",
            "['0 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "Validation Time Gaps:\n",
            "   Expected interval: 7 days\n",
            "   Actual intervals found: <TimedeltaArray>\n",
            "['0 days', '7 days']\n",
            "Length: 2, dtype: timedelta64[ns]\n",
            "   Number of gaps: 82246\n",
            "   Gap sizes: <TimedeltaArray>\n",
            "['0 days']\n",
            "Length: 1, dtype: timedelta64[ns]\n",
            "\n",
            "ğŸ“‹ Sample Data Preview:\n",
            "============================================================\n",
            "Training Data Sample (first 5 rows):\n",
            "   Store  Dept       Date  Weekly_Sales  IsHoliday\n",
            "0      1     1 2010-02-05      24924.50      False\n",
            "1      1     1 2010-02-12      46039.49       True\n",
            "2      1     1 2010-02-19      41595.55      False\n",
            "3      1     1 2010-02-26      19403.54      False\n",
            "4      1     1 2010-03-05      21827.90      False\n",
            "\n",
            "Validation Data Sample (first 5 rows):\n",
            "     Store  Dept       Date  Weekly_Sales  IsHoliday\n",
            "114      1     1 2012-04-13      34684.21      False\n",
            "115      1     1 2012-04-20      16976.19      False\n",
            "116      1     1 2012-04-27      16347.60      False\n",
            "117      1     1 2012-05-04      17147.44      False\n",
            "118      1     1 2012-05-11      18164.20      False\n",
            "\n",
            "ğŸ’¾ Saving split datasets...\n",
            "   âœ… Training data saved: 321,636 records\n",
            "   âœ… Validation data saved: 82,275 records\n",
            "\n",
            "ğŸ‰ Time Series Split Completed Successfully!\n",
            "ğŸ“Š Ready for model training!\n",
            "âš ï¸  Note: This is a proper time series split - no future data leakage\n",
            "ğŸƒ View run Time_Series_Data_Split_80_20 at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/f9fc0fa0a73a4048be3b3b374a5ea53b\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "\n",
            "================================================================================\n",
            "ğŸ“Š SPLIT SUMMARY\n",
            "================================================================================\n",
            "âœ… Training Set: 321,636 records\n",
            "âœ… Validation Set: 82,275 records\n",
            "ğŸ“… Split Date: 2012-04-13 00:00:00\n",
            "ğŸ¯ Ready for XGBoost training!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step: XGBoost Training with Data Type Fix\n",
        "with mlflow.start_run(run_name=\"XGBoost_Training_DataType_Fixed\") as run:\n",
        "\n",
        "    print(f\"ğŸš€ Starting XGBoost training with data type fix: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"model_type\", \"xgboost_regressor\")\n",
        "    mlflow.log_param(\"evaluation_metric\", \"WMAE_weighted_mean_absolute_error\")\n",
        "    mlflow.log_param(\"train_size\", len(train_final))\n",
        "    mlflow.log_param(\"val_size\", len(val_final))\n",
        "\n",
        "    # Import required libraries\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    import joblib\n",
        "\n",
        "    # 1. Prepare data with proper data types\n",
        "    print(\"ğŸ”§ Preparing data with proper data types...\")\n",
        "\n",
        "    # Define target and features\n",
        "    target_column = 'Weekly_Sales'\n",
        "\n",
        "    # Exclude non-feature columns\n",
        "    exclude_columns = [\n",
        "        'Weekly_Sales',  # target\n",
        "        'Date',         # original date (we have date features)\n",
        "        'Store',        # will encode separately\n",
        "        'Dept',         # will encode separately\n",
        "    ]\n",
        "\n",
        "    # Get feature columns\n",
        "    feature_columns = [col for col in train_final.columns if col not in exclude_columns]\n",
        "\n",
        "    print(f\"   Total features available: {len(feature_columns)}\")\n",
        "\n",
        "    # 2. Handle categorical encoding and data types\n",
        "    print(f\"ğŸ·ï¸ Encoding categorical variables and fixing data types...\")\n",
        "\n",
        "    # Create copies for encoding\n",
        "    train_encoded = train_final.copy()\n",
        "    val_encoded = val_final.copy()\n",
        "\n",
        "    # Encode Store and Dept\n",
        "    categorical_features = ['Store', 'Dept']\n",
        "    label_encoders = {}\n",
        "\n",
        "    for cat_feature in categorical_features:\n",
        "        le = LabelEncoder()\n",
        "        # Fit on training data only\n",
        "        train_encoded[f'{cat_feature}_encoded'] = le.fit_transform(train_encoded[cat_feature])\n",
        "        # Transform validation data\n",
        "        val_encoded[f'{cat_feature}_encoded'] = le.transform(val_encoded[cat_feature])\n",
        "        label_encoders[cat_feature] = le\n",
        "\n",
        "        # Add encoded feature to feature list\n",
        "        if f'{cat_feature}_encoded' not in feature_columns:\n",
        "            feature_columns.append(f'{cat_feature}_encoded')\n",
        "\n",
        "        print(f\"   Encoded {cat_feature}: {train_encoded[cat_feature].nunique()} unique values\")\n",
        "\n",
        "    print(f\"   Final feature count: {len(feature_columns)}\")\n",
        "\n",
        "    # 3. Prepare X and y with proper data types\n",
        "    X_train = train_encoded[feature_columns].copy()\n",
        "    y_train = train_encoded[target_column].copy()\n",
        "    X_val = val_encoded[feature_columns].copy()\n",
        "    y_val = val_encoded[target_column].copy()\n",
        "\n",
        "    # 4. Fix data types - Convert all to numeric\n",
        "    print(f\"ğŸ”§ Converting all features to numeric types...\")\n",
        "\n",
        "    # Check current data types\n",
        "    print(f\"   Checking data types before conversion...\")\n",
        "    object_columns = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype == 'object':\n",
        "            object_columns.append(col)\n",
        "\n",
        "    if object_columns:\n",
        "        print(f\"   Found {len(object_columns)} object columns: {object_columns}\")\n",
        "\n",
        "        # Convert object columns to numeric\n",
        "        for col in object_columns:\n",
        "            try:\n",
        "                # Try to convert to numeric\n",
        "                X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
        "                X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
        "                print(f\"     Converted {col} to numeric\")\n",
        "            except Exception as e:\n",
        "                print(f\"     Warning: Could not convert {col} to numeric: {e}\")\n",
        "    else:\n",
        "        print(f\"   âœ… No object columns found\")\n",
        "\n",
        "    # Convert boolean columns to int\n",
        "    bool_columns = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype == 'bool':\n",
        "            bool_columns.append(col)\n",
        "\n",
        "    if bool_columns:\n",
        "        print(f\"   Found {len(bool_columns)} boolean columns, converting to int...\")\n",
        "        for col in bool_columns:\n",
        "            X_train[col] = X_train[col].astype(int)\n",
        "            X_val[col] = X_val[col].astype(int)\n",
        "            print(f\"     Converted {col} from bool to int\")\n",
        "\n",
        "    # Ensure all columns are float64 or int64\n",
        "    print(f\"   Converting all features to float64...\")\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype not in ['float64', 'int64', 'float32', 'int32']:\n",
        "            X_train[col] = X_train[col].astype('float64')\n",
        "            X_val[col] = X_val[col].astype('float64')\n",
        "\n",
        "    # Handle any missing values that might have been introduced during conversion\n",
        "    print(f\"ğŸ” Checking for missing values after conversion...\")\n",
        "    train_missing = X_train.isnull().sum().sum()\n",
        "    val_missing = X_val.isnull().sum().sum()\n",
        "\n",
        "    if train_missing > 0 or val_missing > 0:\n",
        "        print(f\"   Warning: Missing values found after conversion - Train: {train_missing}, Val: {val_missing}\")\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_val = X_val.fillna(0)\n",
        "        print(f\"   âœ… Missing values filled with 0\")\n",
        "    else:\n",
        "        print(f\"   âœ… No missing values found\")\n",
        "\n",
        "    # Final data type check\n",
        "    print(f\"\\nğŸ“Š Final data type verification:\")\n",
        "    print(f\"   X_train dtypes: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "    print(f\"   X_val dtypes: {X_val.dtypes.value_counts().to_dict()}\")\n",
        "\n",
        "    print(f\"\\nğŸ“Š Final dataset shapes:\")\n",
        "    print(f\"   X_train: {X_train.shape}\")\n",
        "    print(f\"   y_train: {y_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}\")\n",
        "    print(f\"   y_val: {y_val.shape}\")\n",
        "\n",
        "    # 5. Define WMAE function and weights\n",
        "    print(f\"âš–ï¸ Setting up WMAE (Weighted Mean Absolute Error)...\")\n",
        "\n",
        "    def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "        \"\"\"\n",
        "        Calculate Weighted Mean Absolute Error\n",
        "        Holiday weeks are weighted 5x higher than non-holiday weeks\n",
        "        \"\"\"\n",
        "        # Calculate absolute errors\n",
        "        abs_errors = np.abs(y_true - y_pred)\n",
        "\n",
        "        # Create weights: 5.0 for holiday weeks, 1.0 for non-holiday weeks\n",
        "        weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "\n",
        "        # Calculate weighted mean absolute error\n",
        "        wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "\n",
        "        return wmae\n",
        "\n",
        "    def calculate_comprehensive_metrics(y_true, y_pred, is_holiday, dataset_name):\n",
        "        \"\"\"Calculate comprehensive metrics including WMAE\"\"\"\n",
        "\n",
        "        # Standard metrics\n",
        "        mae = mean_absolute_error(y_true, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "        r2 = r2_score(y_true, y_pred)\n",
        "        mape = np.mean(np.abs((y_true - y_pred) / np.abs(y_true))) * 100\n",
        "\n",
        "        # WMAE (main metric for Walmart competition)\n",
        "        wmae = calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0)\n",
        "\n",
        "        # Holiday vs Non-holiday breakdown\n",
        "        holiday_mask = is_holiday == True\n",
        "        non_holiday_mask = is_holiday == False\n",
        "\n",
        "        if holiday_mask.sum() > 0:\n",
        "            holiday_mae = mean_absolute_error(y_true[holiday_mask], y_pred[holiday_mask])\n",
        "            holiday_count = holiday_mask.sum()\n",
        "        else:\n",
        "            holiday_mae = 0\n",
        "            holiday_count = 0\n",
        "\n",
        "        if non_holiday_mask.sum() > 0:\n",
        "            non_holiday_mae = mean_absolute_error(y_true[non_holiday_mask], y_pred[non_holiday_mask])\n",
        "            non_holiday_count = non_holiday_mask.sum()\n",
        "        else:\n",
        "            non_holiday_mae = 0\n",
        "            non_holiday_count = 0\n",
        "\n",
        "        metrics = {\n",
        "            'wmae': wmae,\n",
        "            'mae': mae,\n",
        "            'rmse': rmse,\n",
        "            'r2': r2,\n",
        "            'mape': mape,\n",
        "            'holiday_mae': holiday_mae,\n",
        "            'non_holiday_mae': non_holiday_mae,\n",
        "            'holiday_count': holiday_count,\n",
        "            'non_holiday_count': non_holiday_count,\n",
        "            'mean_true': np.mean(y_true),\n",
        "            'mean_pred': np.mean(y_pred)\n",
        "        }\n",
        "\n",
        "        print(f\"\\nğŸ“Š {dataset_name} Metrics:\")\n",
        "        print(f\"   ğŸ¯ WMAE (Main Metric): ${metrics['wmae']:,.2f}\")\n",
        "        print(f\"   ğŸ“Š MAE: ${metrics['mae']:,.2f}\")\n",
        "        print(f\"   ğŸ“Š RMSE: ${metrics['rmse']:,.2f}\")\n",
        "        print(f\"   ğŸ“Š RÂ²: {metrics['r2']:.4f}\")\n",
        "        print(f\"   ğŸ“Š MAPE: {metrics['mape']:.2f}%\")\n",
        "        print(f\"   ğŸ„ Holiday MAE: ${metrics['holiday_mae']:,.2f} ({metrics['holiday_count']} weeks)\")\n",
        "        print(f\"   ğŸ“… Non-Holiday MAE: ${metrics['non_holiday_mae']:,.2f} ({metrics['non_holiday_count']} weeks)\")\n",
        "        print(f\"   ğŸ’° Mean True: ${metrics['mean_true']:,.2f}\")\n",
        "        print(f\"   ğŸ’° Mean Pred: ${metrics['mean_pred']:,.2f}\")\n",
        "\n",
        "        # Log to MLflow\n",
        "        for metric_name, value in metrics.items():\n",
        "            mlflow.log_metric(f\"{dataset_name.lower()}_{metric_name}\", value)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    # 6. XGBoost model configuration\n",
        "    print(f\"\\nğŸ¯ Configuring XGBoost model...\")\n",
        "\n",
        "    # XGBoost parameters\n",
        "    xgb_params = {\n",
        "        # Objective\n",
        "        'objective': 'reg:squarederror',\n",
        "\n",
        "        # Tree structure\n",
        "        'max_depth': 8,\n",
        "        'min_child_weight': 3,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'colsample_bylevel': 0.8,\n",
        "\n",
        "        # Learning\n",
        "        'learning_rate': 0.1,\n",
        "        'n_estimators': 1000,\n",
        "\n",
        "        # Regularization\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "\n",
        "        # Performance\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'verbosity': 1\n",
        "    }\n",
        "\n",
        "    # Log parameters\n",
        "    for key, value in xgb_params.items():\n",
        "        mlflow.log_param(f\"xgb_{key}\", value)\n",
        "\n",
        "    print(f\"   Parameters configured:\")\n",
        "    for key, value in xgb_params.items():\n",
        "        print(f\"     {key}: {value}\")\n",
        "\n",
        "    # 7. Train XGBoost model\n",
        "    print(f\"\\nğŸš€ Training XGBoost model...\")\n",
        "\n",
        "    model = xgb.XGBRegressor(**xgb_params)\n",
        "\n",
        "    # Simple training without early stopping for now\n",
        "    model.fit(X_train, y_train, verbose=100)\n",
        "\n",
        "    print(f\"\\nâœ… Training completed!\")\n",
        "    print(f\"   Total estimators used: {model.n_estimators}\")\n",
        "\n",
        "    mlflow.log_param(\"total_estimators\", model.n_estimators)\n",
        "\n",
        "    # 8. Make predictions\n",
        "    print(f\"\\nğŸ”® Making predictions...\")\n",
        "\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    print(f\"   Training predictions: {len(y_train_pred):,}\")\n",
        "    print(f\"   Validation predictions: {len(y_val_pred):,}\")\n",
        "\n",
        "    # 9. Evaluate with WMAE and other metrics\n",
        "    print(f\"\\nğŸ“ˆ Evaluating model performance...\")\n",
        "\n",
        "    # Get holiday information for WMAE calculation\n",
        "    train_holiday_flags = train_encoded['IsHoliday'].values\n",
        "    val_holiday_flags = val_encoded['IsHoliday'].values\n",
        "\n",
        "    # Calculate comprehensive metrics\n",
        "    train_metrics = calculate_comprehensive_metrics(\n",
        "        y_train, y_train_pred, train_holiday_flags, \"Training\"\n",
        "    )\n",
        "\n",
        "    val_metrics = calculate_comprehensive_metrics(\n",
        "        y_val, y_val_pred, val_holiday_flags, \"Validation\"\n",
        "    )\n",
        "\n",
        "    # 10. Feature importance analysis\n",
        "    print(f\"\\nğŸ” Analyzing feature importance...\")\n",
        "\n",
        "    feature_importance = model.feature_importances_\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'feature': feature_columns,\n",
        "        'importance': feature_importance\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(f\"\\n   Top 15 Most Important Features:\")\n",
        "    for i, (_, row) in enumerate(feature_importance_df.head(15).iterrows()):\n",
        "        print(f\"     {i+1:2d}. {row['feature']:<40} {row['importance']:.4f}\")\n",
        "\n",
        "    # Log top features\n",
        "    top_15_features = feature_importance_df.head(15)\n",
        "    for i, (_, row) in enumerate(top_15_features.iterrows()):\n",
        "        mlflow.log_metric(f\"feature_importance_rank_{i+1}\", row['importance'])\n",
        "        mlflow.log_param(f\"top_feature_{i+1}\", row['feature'])\n",
        "\n",
        "    # 11. Quick error analysis\n",
        "    print(f\"\\nğŸ”¬ Quick error analysis...\")\n",
        "\n",
        "    # Create predictions dataframe\n",
        "    val_analysis = val_encoded[['Store', 'Dept', 'Date', 'Weekly_Sales', 'IsHoliday']].copy()\n",
        "    val_analysis['Predicted'] = y_val_pred\n",
        "    val_analysis['Error'] = val_analysis['Weekly_Sales'] - val_analysis['Predicted']\n",
        "    val_analysis['Abs_Error'] = np.abs(val_analysis['Error'])\n",
        "\n",
        "    # Error statistics\n",
        "    print(f\"\\n   Error Distribution:\")\n",
        "    print(f\"     Mean Error: ${val_analysis['Error'].mean():,.2f}\")\n",
        "    print(f\"     Mean Abs Error: ${val_analysis['Abs_Error'].mean():,.2f}\")\n",
        "    print(f\"     Median Abs Error: ${val_analysis['Abs_Error'].median():,.2f}\")\n",
        "    print(f\"     Max Abs Error: ${val_analysis['Abs_Error'].max():,.2f}\")\n",
        "\n",
        "    # 12. Save results\n",
        "    print(f\"\\nğŸ’¾ Saving model and results...\")\n",
        "\n",
        "    # Save model\n",
        "    model_path = \"xgboost_walmart_wmae_model.joblib\"\n",
        "    joblib.dump(model, model_path)\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "    # Save feature importance\n",
        "    feature_importance_df.to_csv(\"feature_importance.csv\", index=False)\n",
        "    mlflow.log_artifact(\"feature_importance.csv\")\n",
        "\n",
        "    # Save predictions\n",
        "    val_analysis.to_csv(\"validation_predictions_wmae.csv\", index=False)\n",
        "    mlflow.log_artifact(\"validation_predictions_wmae.csv\")\n",
        "\n",
        "    print(f\"   âœ… Model saved: {model_path}\")\n",
        "    print(f\"   âœ… Results saved: validation_predictions_wmae.csv\")\n",
        "\n",
        "    # 13. Final summary\n",
        "    print(f\"\\nğŸ‰ XGBoost Training Summary:\")\n",
        "    print(f\"=\" * 80)\n",
        "    print(f\"   Model: XGBoost Regressor\")\n",
        "    print(f\"   Training samples: {len(X_train):,}\")\n",
        "    print(f\"   Validation samples: {len(X_val):,}\")\n",
        "    print(f\"   Features used: {len(feature_columns)}\")\n",
        "    print(f\"   Total estimators: {model.n_estimators}\")\n",
        "    print(f\"\")\n",
        "    print(f\"   ğŸ¯ KEY METRICS (Validation Set):\")\n",
        "    print(f\"     WMAE (Walmart Competition Metric): ${val_metrics['wmae']:,.2f}\")\n",
        "    print(f\"     MAE: ${val_metrics['mae']:,.2f}\")\n",
        "    print(f\"     RMSE: ${val_metrics['rmse']:,.2f}\")\n",
        "    print(f\"     RÂ²: {val_metrics['r2']:.4f}\")\n",
        "    print(f\"\")\n",
        "    print(f\"   ğŸ„ Holiday Performance:\")\n",
        "    print(f\"     Holiday weeks MAE: ${val_metrics['holiday_mae']:,.2f}\")\n",
        "    print(f\"     Non-holiday weeks MAE: ${val_metrics['non_holiday_mae']:,.2f}\")\n",
        "    print(f\"     Holiday weight in WMAE: 5x\")\n",
        "    print(f\"=\" * 80)\n",
        "\n",
        "    # Create comprehensive summary\n",
        "    model_summary = {\n",
        "        'model_type': 'XGBoost',\n",
        "        'evaluation_focus': 'WMAE_weighted_mean_absolute_error',\n",
        "        'training_data_size': len(X_train),\n",
        "        'validation_data_size': len(X_val),\n",
        "        'num_features': len(feature_columns),\n",
        "        'validation_metrics': val_metrics,\n",
        "        'training_metrics': train_metrics,\n",
        "        'parameters': xgb_params,\n",
        "        'holiday_weight': 5.0,\n",
        "        'top_10_features': feature_importance_df.head(10).to_dict('records')\n",
        "    }\n",
        "\n",
        "    mlflow.log_dict(model_summary, \"xgboost_wmae_summary.json\")\n",
        "\n",
        "# End MLflow run\n",
        "mlflow.end_run()\n",
        "\n",
        "print(f\"\\nğŸƒ View detailed results at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/\")"
      ],
      "metadata": {
        "id": "8l-yWR0KU92f",
        "outputId": "310de90d-2e09-4d40-d750-ef1b525b1316",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting XGBoost training with data type fix: 912678876c794318881e59358327994d\n",
            "ğŸ”§ Preparing data with proper data types...\n",
            "   Total features available: 69\n",
            "ğŸ·ï¸ Encoding categorical variables and fixing data types...\n",
            "   Encoded Store: 45 unique values\n",
            "   Encoded Dept: 81 unique values\n",
            "   Final feature count: 71\n",
            "ğŸ”§ Converting all features to numeric types...\n",
            "   Checking data types before conversion...\n",
            "   Found 1 object columns: ['Type']\n",
            "     Converted Type to numeric\n",
            "   Found 1 boolean columns, converting to int...\n",
            "     Converted IsHoliday from bool to int\n",
            "   Converting all features to float64...\n",
            "ğŸ” Checking for missing values after conversion...\n",
            "   Warning: Missing values found after conversion - Train: 321636, Val: 82275\n",
            "   âœ… Missing values filled with 0\n",
            "\n",
            "ğŸ“Š Final data type verification:\n",
            "   X_train dtypes: {dtype('float64'): 38, dtype('int64'): 26, dtype('int32'): 7}\n",
            "   X_val dtypes: {dtype('float64'): 38, dtype('int64'): 26, dtype('int32'): 7}\n",
            "\n",
            "ğŸ“Š Final dataset shapes:\n",
            "   X_train: (321636, 71)\n",
            "   y_train: (321636,)\n",
            "   X_val: (82275, 71)\n",
            "   y_val: (82275,)\n",
            "âš–ï¸ Setting up WMAE (Weighted Mean Absolute Error)...\n",
            "\n",
            "ğŸ¯ Configuring XGBoost model...\n",
            "   Parameters configured:\n",
            "     objective: reg:squarederror\n",
            "     max_depth: 8\n",
            "     min_child_weight: 3\n",
            "     subsample: 0.8\n",
            "     colsample_bytree: 0.8\n",
            "     colsample_bylevel: 0.8\n",
            "     learning_rate: 0.1\n",
            "     n_estimators: 1000\n",
            "     reg_alpha: 0.1\n",
            "     reg_lambda: 1.0\n",
            "     random_state: 42\n",
            "     n_jobs: -1\n",
            "     verbosity: 1\n",
            "\n",
            "ğŸš€ Training XGBoost model...\n",
            "\n",
            "âœ… Training completed!\n",
            "   Total estimators used: 1000\n",
            "\n",
            "ğŸ”® Making predictions...\n",
            "   Training predictions: 321,636\n",
            "   Validation predictions: 82,275\n",
            "\n",
            "ğŸ“ˆ Evaluating model performance...\n",
            "\n",
            "ğŸ“Š Training Metrics:\n",
            "   ğŸ¯ WMAE (Main Metric): $52.91\n",
            "   ğŸ“Š MAE: $52.37\n",
            "   ğŸ“Š RMSE: $77.18\n",
            "   ğŸ“Š RÂ²: 1.0000\n",
            "   ğŸ“Š MAPE: inf%\n",
            "   ğŸ„ Holiday MAE: $54.58 (25885 weeks)\n",
            "   ğŸ“… Non-Holiday MAE: $52.18 (295751 weeks)\n",
            "   ğŸ’° Mean True: $12,553.69\n",
            "   ğŸ’° Mean Pred: $12,553.70\n",
            "\n",
            "ğŸ“Š Validation Metrics:\n",
            "   ğŸ¯ WMAE (Main Metric): $94.20\n",
            "   ğŸ“Š MAE: $91.00\n",
            "   ğŸ“Š RMSE: $191.05\n",
            "   ğŸ“Š RÂ²: 0.9998\n",
            "   ğŸ“Š MAPE: inf%\n",
            "   ğŸ„ Holiday MAE: $117.04 (2879 weeks)\n",
            "   ğŸ“… Non-Holiday MAE: $90.05 (79396 weeks)\n",
            "   ğŸ’° Mean True: $12,506.07\n",
            "   ğŸ’° Mean Pred: $12,510.52\n",
            "\n",
            "ğŸ” Analyzing feature importance...\n",
            "\n",
            "   Top 15 Most Important Features:\n",
            "      1. Weekly_Sales_ewm_0.3                     0.6327\n",
            "      2. Weekly_Sales_ewm_0.5                     0.3200\n",
            "      3. Weekly_Sales_rolling_mean_4              0.0173\n",
            "      4. Weekly_Sales_rolling_min_4               0.0101\n",
            "      5. Weekly_Sales_trend_diff                  0.0034\n",
            "      6. Weekly_Sales_rolling_min_8               0.0024\n",
            "      7. Weekly_Sales_monthly_diff                0.0020\n",
            "      8. Weekly_Sales_rolling_max_4               0.0018\n",
            "      9. IsMajorHoliday                           0.0017\n",
            "     10. IsHoliday                                0.0017\n",
            "     11. Weekly_Sales_rolling_std_4               0.0014\n",
            "     12. Weekly_Sales_lag_1                       0.0013\n",
            "     13. Weekly_Sales_quarterly_diff              0.0008\n",
            "     14. Weekly_Sales_rolling_std_8               0.0004\n",
            "     15. IsThanksgivingWeek                       0.0003\n",
            "\n",
            "ğŸ”¬ Quick error analysis...\n",
            "\n",
            "   Error Distribution:\n",
            "     Mean Error: $-4.45\n",
            "     Mean Abs Error: $91.00\n",
            "     Median Abs Error: $37.78\n",
            "     Max Abs Error: $6,951.16\n",
            "\n",
            "ğŸ’¾ Saving model and results...\n",
            "   âœ… Model saved: xgboost_walmart_wmae_model.joblib\n",
            "   âœ… Results saved: validation_predictions_wmae.csv\n",
            "\n",
            "ğŸ‰ XGBoost Training Summary:\n",
            "================================================================================\n",
            "   Model: XGBoost Regressor\n",
            "   Training samples: 321,636\n",
            "   Validation samples: 82,275\n",
            "   Features used: 71\n",
            "   Total estimators: 1000\n",
            "\n",
            "   ğŸ¯ KEY METRICS (Validation Set):\n",
            "     WMAE (Walmart Competition Metric): $94.20\n",
            "     MAE: $91.00\n",
            "     RMSE: $191.05\n",
            "     RÂ²: 0.9998\n",
            "\n",
            "   ğŸ„ Holiday Performance:\n",
            "     Holiday weeks MAE: $117.04\n",
            "     Non-holiday weeks MAE: $90.05\n",
            "     Holiday weight in WMAE: 5x\n",
            "================================================================================\n",
            "ğŸƒ View run XGBoost_Training_DataType_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2/runs/912678876c794318881e59358327994d\n",
            "ğŸ§ª View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/2\n",
            "\n",
            "ğŸƒ View detailed results at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "927ee7e1e5844070886503bc2d9684cf": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7cd4b3c93b2748b68a0d6a287eb01747",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32mâ ¼\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">â ¼</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "7cd4b3c93b2748b68a0d6a287eb01747": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}