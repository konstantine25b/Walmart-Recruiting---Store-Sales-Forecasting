{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# experiment_4_k"
      ],
      "metadata": {
        "id": "R3hfw6V-alWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "GMBxqiRyaKmC",
        "outputId": "148f9d7f-c67b-4b74-ff7e-e33e63aa0e4e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cLdXVjrean6a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Gg3HtWQbao_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2XfVrrCwaqXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "id": "ZP7DAfGWarmW",
        "outputId": "65eb0980-81a4-465b-b4da-b602328b43ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 618MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "id": "G9rkhMhdas2Y",
        "outputId": "67f098c2-c18a-4cd9-c631-866626ef9418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# EXPERIMENT 4: FIXING DATA LEAKAGE - SPLIT FIRST, THEN PREPROCESS\n",
        "# ================================================================================\n",
        "\n",
        "# Step 1: Setup and MLflow/DagsHub Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install prophet plotly mlflow dagshub xgboost -q\n",
        "\n",
        "# Setup MLflow and DagsHub\n",
        "import mlflow\n",
        "import dagshub\n",
        "\n",
        "# DagsHub setup\n",
        "dagshub.init(repo_owner='konstantine25b',\n",
        "             repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "             mlflow=True)\n",
        "\n",
        "# Set tracking URI\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "mlflow.set_experiment(\"Experiment_4_Fixed_Data_Leakage\")"
      ],
      "metadata": {
        "id": "itxEe9N7auGr",
        "outputId": "8b9dd195-2d84-4fc1-9502-9ddd6721dd2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569,
          "referenced_widgets": [
            "81d6ff808a96465fa3024c7401c36ca8",
            "61fca76aad864cfab4bb210be2636831"
          ]
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">‚ùó‚ùó‚ùó AUTHORIZATION REQUIRED ‚ùó‚ùó‚ùó</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81d6ff808a96465fa3024c7401c36ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=eea5fc1c-5b99-43b5-bbcd-b181f521be4e&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=4b71c4df89bc89a066aee754acd4f69f7f37ad7aa596a493ff76d13dce4c82f6\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as konstantine25b\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as konstantine25b\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/06/25 09:40:32 INFO mlflow.tracking.fluent: Experiment with name 'Experiment_4_Fixed_Data_Leakage' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/43ce0a29767b4be4b47a7e6d431382c2', creation_time=1750844432684, experiment_id='3', last_update_time=1750844432684, lifecycle_stage='active', name='Experiment_4_Fixed_Data_Leakage', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and explore data\n",
        "with mlflow.start_run(run_name=\"Data_Loading_Experiment_4\") as run:\n",
        "\n",
        "    print(f\"üìÅ Starting data loading: {run.info.run_id}\")\n",
        "\n",
        "    # Load datasets\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    train = pd.read_csv('train.csv')\n",
        "    stores = pd.read_csv('stores.csv')\n",
        "\n",
        "    # Convert Date column\n",
        "    train['Date'] = pd.to_datetime(train['Date'])\n",
        "\n",
        "    # Log basic info\n",
        "    mlflow.log_param(\"train_shape\", f\"{train.shape[0]}x{train.shape[1]}\")\n",
        "    mlflow.log_param(\"date_range\", f\"{train['Date'].min()} to {train['Date'].max()}\")\n",
        "    mlflow.log_param(\"stores_count\", train['Store'].nunique())\n",
        "    mlflow.log_param(\"departments_count\", train['Dept'].nunique())\n",
        "\n",
        "    # Merge with stores data\n",
        "    train_merged = train.merge(stores, on='Store', how='left')\n",
        "\n",
        "    print(f\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìä Training data shape: {train.shape}\")\n",
        "    print(f\"üè™ Stores data shape: {stores.shape}\")\n",
        "    print(f\"üìä Merged data shape: {train_merged.shape}\")\n",
        "    print(f\"üìÖ Date range: {train['Date'].min()} to {train['Date'].max()}\")\n",
        "    print(f\"üè¨ Stores: {train['Store'].nunique()}, Departments: {train['Dept'].nunique()}\")"
      ],
      "metadata": {
        "id": "zzp6nJcpdVgk",
        "outputId": "cfe76c6e-0817-40ea-ad01-0a741a3608b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Starting data loading: 7045462b0c90416283104deef3fc3a6b\n",
            "‚úÖ Data loaded successfully!\n",
            "üìä Training data shape: (421570, 5)\n",
            "üè™ Stores data shape: (45, 3)\n",
            "üìä Merged data shape: (421570, 7)\n",
            "üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üè¨ Stores: 45, Departments: 81\n",
            "üèÉ View run Data_Loading_Experiment_4 at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/7045462b0c90416283104deef3fc3a6b\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 3: CRITICAL FIX - TEMPORAL SPLIT FIRST (BEFORE PREPROCESSING)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Temporal_Split_BEFORE_Preprocessing\") as run:\n",
        "\n",
        "    print(f\"‚ö†Ô∏è FIXING DATA LEAKAGE: Temporal split BEFORE preprocessing\")\n",
        "    print(f\"üîÑ Starting temporal split run: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"split_method\", \"temporal_80_20_before_preprocessing\")\n",
        "    mlflow.log_param(\"fix_applied\", \"split_before_outlier_detection\")\n",
        "\n",
        "    # Use merged data (only basic merge, no feature engineering yet)\n",
        "    data_for_split = train_merged.copy()\n",
        "\n",
        "    # Sort by date to ensure proper temporal order\n",
        "    data_for_split = data_for_split.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split point (80% for training)\n",
        "    min_date = data_for_split['Date'].min()\n",
        "    max_date = data_for_split['Date'].max()\n",
        "    total_days = (max_date - min_date).days\n",
        "    split_days = int(total_days * 0.8)\n",
        "    split_date = min_date + timedelta(days=split_days)\n",
        "\n",
        "    # Ensure split_date falls on a week boundary (since data is weekly)\n",
        "    # Find the closest Friday (assuming data is weekly ending on Friday)\n",
        "    while split_date.weekday() != 4:  # 4 = Friday\n",
        "        split_date += timedelta(days=1)\n",
        "\n",
        "    # Create temporal split\n",
        "    train_raw = data_for_split[data_for_split['Date'] < split_date].copy()\n",
        "    val_raw = data_for_split[data_for_split['Date'] >= split_date].copy()\n",
        "\n",
        "    # Log split information\n",
        "    mlflow.log_param(\"split_date\", split_date.strftime(\"%Y-%m-%d\"))\n",
        "    mlflow.log_param(\"train_records\", len(train_raw))\n",
        "    mlflow.log_param(\"val_records\", len(val_raw))\n",
        "    mlflow.log_param(\"train_date_range\", f\"{train_raw['Date'].min()} to {train_raw['Date'].max()}\")\n",
        "    mlflow.log_param(\"val_date_range\", f\"{val_raw['Date'].min()} to {val_raw['Date'].max()}\")\n",
        "\n",
        "    # Verify split quality\n",
        "    train_stores = train_raw['Store'].nunique()\n",
        "    val_stores = val_raw['Store'].nunique()\n",
        "    train_depts = train_raw['Dept'].nunique()\n",
        "    val_depts = val_raw['Dept'].nunique()\n",
        "\n",
        "    print(f\"‚úÖ Temporal split completed BEFORE preprocessing!\")\n",
        "    print(f\"üìÖ Split date: {split_date}\")\n",
        "    print(f\"üöÇ Training: {len(train_raw):,} records ({len(train_raw)/len(data_for_split)*100:.1f}%)\")\n",
        "    print(f\"üîÆ Validation: {len(val_raw):,} records ({len(val_raw)/len(data_for_split)*100:.1f}%)\")\n",
        "    print(f\"üè™ Store coverage: Train={train_stores}, Val={val_stores}\")\n",
        "    print(f\"üè¨ Dept coverage: Train={train_depts}, Val={val_depts}\")\n",
        "\n",
        "    # Holiday distribution check\n",
        "    train_holidays = train_raw['IsHoliday'].sum()\n",
        "    val_holidays = val_raw['IsHoliday'].sum()\n",
        "    print(f\"üéÑ Holiday weeks: Train={train_holidays} ({train_holidays/len(train_raw)*100:.1f}%), Val={val_holidays} ({val_holidays/len(val_raw)*100:.1f}%)\")\n",
        "\n",
        "    mlflow.log_metric(\"train_holiday_percentage\", train_holidays/len(train_raw)*100)\n",
        "    mlflow.log_metric(\"val_holiday_percentage\", val_holidays/len(val_raw)*100)"
      ],
      "metadata": {
        "id": "53e5HuBRdjFk",
        "outputId": "d35fc6cb-6aff-4843-dde9-d0e2e28b986a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è FIXING DATA LEAKAGE: Temporal split BEFORE preprocessing\n",
            "üîÑ Starting temporal split run: 024122d9051e43ce80012e95dfcfba45\n",
            "‚úÖ Temporal split completed BEFORE preprocessing!\n",
            "üìÖ Split date: 2012-04-13 00:00:00\n",
            "üöÇ Training: 335,761 records (79.6%)\n",
            "üîÆ Validation: 85,809 records (20.4%)\n",
            "üè™ Store coverage: Train=45, Val=45\n",
            "üè¨ Dept coverage: Train=81, Val=81\n",
            "üéÑ Holiday weeks: Train=26695 (8.0%), Val=2966 (3.5%)\n",
            "üèÉ View run Temporal_Split_BEFORE_Preprocessing at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/024122d9051e43ce80012e95dfcfba45\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 4: FEATURE ENGINEERING (APPLIED TO TRAIN AND VAL SEPARATELY)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Feature_Engineering_After_Split\") as run:\n",
        "\n",
        "    print(f\"üîß Starting feature engineering AFTER split: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"feature_engineering_method\", \"applied_after_temporal_split\")\n",
        "    mlflow.log_param(\"train_records_input\", len(train_raw))\n",
        "    mlflow.log_param(\"val_records_input\", len(val_raw))\n",
        "\n",
        "    def create_comprehensive_date_features(df):\n",
        "        \"\"\"Create comprehensive date features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic date features\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "        # Cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "        # Time since reference\n",
        "        reference_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        df['MonthsFromStart'] = ((df['Date'].dt.year - reference_date.year) * 12 +\n",
        "                                df['Date'].dt.month - reference_date.month)\n",
        "\n",
        "        # Holiday features\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "\n",
        "        # Retail calendar features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        df['IsSummerSeason'] = df['Month'].isin([6, 7, 8]).astype(int)\n",
        "        df['IsSpringSeaso'] = df['Month'].isin([3, 4, 5]).astype(int)\n",
        "\n",
        "        # Week patterns\n",
        "        df['IsFirstWeekOfMonth'] = (df['Day'] <= 7).astype(int)\n",
        "        df['IsLastWeekOfMonth'] = (df['Date'].dt.days_in_month - df['Day'] < 7).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lagging_features_fixed(df, target_col='Weekly_Sales'):\n",
        "        \"\"\"Create lagging features with proper NaN handling\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # Lag features\n",
        "        lags = [1, 2, 3, 4, 8, 12]\n",
        "        for lag in lags:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "            # Fill NaNs with forward fill first, then group mean\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[lag_col].fillna(method='ffill')\n",
        "            # For remaining NaNs, use group mean\n",
        "            mask = df[lag_col].isnull()\n",
        "            if mask.sum() > 0:\n",
        "                group_means = df.groupby(['Store', 'Dept'])[target_col].transform('mean')\n",
        "                df.loc[mask, lag_col] = group_means[mask]\n",
        "\n",
        "        # Rolling window features\n",
        "        windows = [4, 8, 12, 26]\n",
        "        for window in windows:\n",
        "            df[f'{target_col}_rolling_mean_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            df[f'{target_col}_rolling_std_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=2)\n",
        "                .std()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            ).fillna(0)\n",
        "\n",
        "            df[f'{target_col}_rolling_min_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .min()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "            df[f'{target_col}_rolling_max_{window}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .rolling(window=window, min_periods=1)\n",
        "                .max()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Exponential weighted features\n",
        "        alphas = [0.1, 0.3, 0.5]\n",
        "        for alpha in alphas:\n",
        "            df[f'{target_col}_ewm_{alpha}'] = (\n",
        "                df.groupby(['Store', 'Dept'])[target_col]\n",
        "                .ewm(alpha=alpha)\n",
        "                .mean()\n",
        "                .reset_index(level=[0,1], drop=True)\n",
        "            )\n",
        "\n",
        "        # Difference features\n",
        "        df[f'{target_col}_trend_diff'] = df.groupby(['Store', 'Dept'])[target_col].diff(1)\n",
        "        df[f'{target_col}_trend_diff'] = df.groupby(['Store', 'Dept'])[f'{target_col}_trend_diff'].fillna(0)\n",
        "\n",
        "        df[f'{target_col}_monthly_diff'] = df.groupby(['Store', 'Dept'])[target_col].diff(4)\n",
        "        mask = df[f'{target_col}_monthly_diff'].isnull()\n",
        "        df.loc[mask, f'{target_col}_monthly_diff'] = df.loc[mask, f'{target_col}_trend_diff']\n",
        "\n",
        "        df[f'{target_col}_quarterly_diff'] = df.groupby(['Store', 'Dept'])[target_col].diff(12)\n",
        "        mask = df[f'{target_col}_quarterly_diff'].isnull()\n",
        "        df.loc[mask, f'{target_col}_quarterly_diff'] = df.loc[mask, f'{target_col}_monthly_diff']\n",
        "\n",
        "        # Holiday interaction features\n",
        "        df[f'{target_col}_pre_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(-1).fillna(0).astype(int)\n",
        "        df[f'{target_col}_post_holiday'] = df.groupby(['Store', 'Dept'])['IsHoliday'].shift(1).fillna(0).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply feature engineering to training set\n",
        "    print(\"üöÇ Creating features for training set...\")\n",
        "    train_with_features = create_comprehensive_date_features(train_raw)\n",
        "    train_final = create_lagging_features_fixed(train_with_features)\n",
        "\n",
        "    # Apply feature engineering to validation set\n",
        "    print(\"üîÆ Creating features for validation set...\")\n",
        "    val_with_features = create_comprehensive_date_features(val_raw)\n",
        "    val_final = create_lagging_features_fixed(val_with_features)\n",
        "\n",
        "    # Ensure both datasets have the same columns\n",
        "    train_cols = set(train_final.columns)\n",
        "    val_cols = set(val_final.columns)\n",
        "    common_cols = list(train_cols.intersection(val_cols))\n",
        "\n",
        "    # Keep only common columns in the same order\n",
        "    train_final = train_final[common_cols]\n",
        "    val_final = val_final[common_cols]\n",
        "\n",
        "    # Check for any remaining NaNs\n",
        "    train_nans = train_final.isnull().sum().sum()\n",
        "    val_nans = val_final.isnull().sum().sum()\n",
        "\n",
        "    if train_nans > 0 or val_nans > 0:\n",
        "        print(f\"‚ö†Ô∏è Warning: NaNs found - Train: {train_nans}, Val: {val_nans}\")\n",
        "        # Fill any remaining NaNs with 0\n",
        "        train_final = train_final.fillna(0)\n",
        "        val_final = val_final.fillna(0)\n",
        "        print(f\"‚úÖ NaNs filled with 0\")\n",
        "\n",
        "    print(f\"‚úÖ Feature engineering completed!\")\n",
        "    print(f\"üöÇ Train shape: {train_final.shape}\")\n",
        "    print(f\"üîÆ Val shape: {val_final.shape}\")\n",
        "    print(f\"üìä Features created: {len(train_final.columns)}\")\n",
        "    print(f\"üîß NaNs in training: {train_final.isnull().sum().sum()}\")\n",
        "    print(f\"üîß NaNs in validation: {val_final.isnull().sum().sum()}\")\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"train_features_shape\", len(train_final.columns))\n",
        "    mlflow.log_metric(\"train_records_after_features\", len(train_final))\n",
        "    mlflow.log_metric(\"val_records_after_features\", len(val_final))\n",
        "    mlflow.log_metric(\"train_nans_final\", train_final.isnull().sum().sum())\n",
        "    mlflow.log_metric(\"val_nans_final\", val_final.isnull().sum().sum())\n",
        "\n",
        "    # Show feature categories\n",
        "    feature_categories = {\n",
        "        'Date Features': [col for col in train_final.columns if any(x in col for x in ['Year', 'Month', 'Day', 'Week', 'Quarter', 'sin', 'cos'])],\n",
        "        'Holiday Features': [col for col in train_final.columns if any(x in col for x in ['Holiday', 'SuperBowl', 'Labor', 'Thanksgiving', 'Christmas'])],\n",
        "        'Lag Features': [col for col in train_final.columns if 'lag_' in col],\n",
        "        'Rolling Features': [col for col in train_final.columns if 'rolling_' in col],\n",
        "        'EWM Features': [col for col in train_final.columns if 'ewm_' in col],\n",
        "        'Diff Features': [col for col in train_final.columns if 'diff' in col]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìã Feature Categories:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        count = len(features)\n",
        "        print(f\"   {category}: {count} features\")\n",
        "        mlflow.log_metric(f\"{category.lower().replace(' ', '_')}_count\", count)\n",
        "        if count > 0 and count <= 3:\n",
        "            print(f\"     Examples: {features}\")\n",
        "        elif count > 3:\n",
        "            print(f\"     Examples: {features[:3]}\")"
      ],
      "metadata": {
        "id": "z8sy7qqAd16E",
        "outputId": "9bc5aeec-6735-4ab0-819a-9b7a5a158683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Starting feature engineering AFTER split: 9d667b16359c463dbab2c2224c18270f\n",
            "üöÇ Creating features for training set...\n",
            "üîÆ Creating features for validation set...\n",
            "‚úÖ Feature engineering completed!\n",
            "üöÇ Train shape: (335761, 63)\n",
            "üîÆ Val shape: (85809, 63)\n",
            "üìä Features created: 63\n",
            "üîß NaNs in training: 0\n",
            "üîß NaNs in validation: 0\n",
            "\n",
            "üìã Feature Categories:\n",
            "   Date Features: 53 features\n",
            "     Examples: ['Weekly_Sales_lag_8', 'Weekly_Sales_quarterly_diff', 'Weekly_Sales_monthly_diff']\n",
            "   Holiday Features: 7 features\n",
            "     Examples: ['IsChristmasWeek', 'IsLaborDayWeek', 'IsHoliday']\n",
            "   Lag Features: 6 features\n",
            "     Examples: ['Weekly_Sales_lag_8', 'Weekly_Sales_lag_4', 'Weekly_Sales_lag_3']\n",
            "   Rolling Features: 16 features\n",
            "     Examples: ['Weekly_Sales_rolling_mean_12', 'Weekly_Sales_rolling_std_26', 'Weekly_Sales_rolling_min_4']\n",
            "   EWM Features: 3 features\n",
            "     Examples: ['Weekly_Sales_ewm_0.1', 'Weekly_Sales_ewm_0.3', 'Weekly_Sales_ewm_0.5']\n",
            "   Diff Features: 3 features\n",
            "     Examples: ['Weekly_Sales_quarterly_diff', 'Weekly_Sales_monthly_diff', 'Weekly_Sales_trend_diff']\n",
            "üèÉ View run Feature_Engineering_After_Split at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/9d667b16359c463dbab2c2224c18270f\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 5: DATA TYPE CHECK AND CATEGORICAL ENCODING\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Data_Type_Check_And_Encoding\") as run:\n",
        "\n",
        "    print(f\"üîç Checking data types and encoding categoricals: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"encoding_method\", \"one_hot_encoding_for_categoricals\")\n",
        "\n",
        "    # Check data types in both datasets\n",
        "    print(f\"üìä Checking data types...\")\n",
        "\n",
        "    train_dtypes = train_final.dtypes.value_counts()\n",
        "    val_dtypes = val_final.dtypes.value_counts()\n",
        "\n",
        "    print(f\"üöÇ Training data types: {dict(train_dtypes)}\")\n",
        "    print(f\"üîÆ Validation data types: {dict(val_dtypes)}\")\n",
        "\n",
        "    # Identify categorical columns (object, category, or specific known categoricals)\n",
        "    categorical_columns = []\n",
        "\n",
        "    # Check for object/category columns\n",
        "    for col in train_final.columns:\n",
        "        if train_final[col].dtype == 'object' or train_final[col].dtype.name == 'category':\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    # Add known categorical columns that might be numeric but should be treated as categorical\n",
        "    known_categoricals = ['Store', 'Dept']  # These should be categorical\n",
        "    for col in known_categoricals:\n",
        "        if col in train_final.columns and col not in categorical_columns:\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    print(f\"üè∑Ô∏è Categorical columns found: {categorical_columns}\")\n",
        "\n",
        "    if len(categorical_columns) > 0:\n",
        "        print(f\"üîß Applying one-hot encoding to {len(categorical_columns)} categorical columns...\")\n",
        "\n",
        "        # Apply one-hot encoding\n",
        "        from sklearn.preprocessing import OneHotEncoder\n",
        "        import pandas as pd\n",
        "\n",
        "        # Create copies for encoding\n",
        "        train_encoded = train_final.copy()\n",
        "        val_encoded = val_final.copy()\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            print(f\"   Encoding {col}...\")\n",
        "\n",
        "            # Check unique values\n",
        "            train_unique = train_encoded[col].nunique()\n",
        "            val_unique = val_encoded[col].nunique()\n",
        "            combined_unique = pd.concat([train_encoded[col], val_encoded[col]]).nunique()\n",
        "\n",
        "            print(f\"     Train unique: {train_unique}, Val unique: {val_unique}, Combined: {combined_unique}\")\n",
        "\n",
        "            # Use pandas get_dummies for one-hot encoding\n",
        "            # Fit on combined data to ensure same columns\n",
        "            combined_data = pd.concat([\n",
        "                train_encoded[col].reset_index(drop=True),\n",
        "                val_encoded[col].reset_index(drop=True)\n",
        "            ])\n",
        "\n",
        "            # Get dummies\n",
        "            dummies = pd.get_dummies(combined_data, prefix=f'{col}', dummy_na=False)\n",
        "\n",
        "            # Split back into train and val\n",
        "            train_dummies = dummies.iloc[:len(train_encoded)]\n",
        "            val_dummies = dummies.iloc[len(train_encoded):]\n",
        "\n",
        "            # Reset indices\n",
        "            train_dummies.index = train_encoded.index\n",
        "            val_dummies.index = val_encoded.index\n",
        "\n",
        "            # Add dummy columns to datasets\n",
        "            train_encoded = pd.concat([train_encoded, train_dummies], axis=1)\n",
        "            val_encoded = pd.concat([val_encoded, val_dummies], axis=1)\n",
        "\n",
        "            # Remove original categorical column\n",
        "            train_encoded = train_encoded.drop(columns=[col])\n",
        "            val_encoded = val_encoded.drop(columns=[col])\n",
        "\n",
        "            print(f\"     Created {len(dummies.columns)} dummy variables\")\n",
        "            mlflow.log_metric(f\"{col}_dummy_count\", len(dummies.columns))\n",
        "\n",
        "        # Update the final datasets\n",
        "        train_final_encoded = train_encoded\n",
        "        val_final_encoded = val_encoded\n",
        "\n",
        "        print(f\"‚úÖ One-hot encoding completed!\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚úÖ No categorical columns found - using original datasets\")\n",
        "        train_final_encoded = train_final.copy()\n",
        "        val_final_encoded = val_final.copy()\n",
        "\n",
        "    # Final data type check and conversion\n",
        "    print(f\"üîß Converting all features to numeric types...\")\n",
        "\n",
        "    # Convert boolean to int\n",
        "    bool_columns = []\n",
        "    for col in train_final_encoded.columns:\n",
        "        if train_final_encoded[col].dtype == 'bool':\n",
        "            bool_columns.append(col)\n",
        "            train_final_encoded[col] = train_final_encoded[col].astype(int)\n",
        "            val_final_encoded[col] = val_final_encoded[col].astype(int)\n",
        "\n",
        "    if bool_columns:\n",
        "        print(f\"   Converted {len(bool_columns)} boolean columns to int\")\n",
        "\n",
        "    # Convert any remaining object columns to numeric\n",
        "    object_columns = []\n",
        "    for col in train_final_encoded.columns:\n",
        "        if train_final_encoded[col].dtype == 'object':\n",
        "            object_columns.append(col)\n",
        "            train_final_encoded[col] = pd.to_numeric(train_final_encoded[col], errors='coerce')\n",
        "            val_final_encoded[col] = pd.to_numeric(val_final_encoded[col], errors='coerce')\n",
        "\n",
        "    if object_columns:\n",
        "        print(f\"   Converted {len(object_columns)} object columns to numeric\")\n",
        "\n",
        "    # Fill any NaNs introduced during conversion\n",
        "    train_nans_after = train_final_encoded.isnull().sum().sum()\n",
        "    val_nans_after = val_final_encoded.isnull().sum().sum()\n",
        "\n",
        "    if train_nans_after > 0 or val_nans_after > 0:\n",
        "        print(f\"‚ö†Ô∏è NaNs introduced during conversion - Train: {train_nans_after}, Val: {val_nans_after}\")\n",
        "        train_final_encoded = train_final_encoded.fillna(0)\n",
        "        val_final_encoded = val_final_encoded.fillna(0)\n",
        "        print(f\"‚úÖ NaNs filled with 0\")\n",
        "\n",
        "    # Final data type summary\n",
        "    final_train_dtypes = train_final_encoded.dtypes.value_counts()\n",
        "    final_val_dtypes = val_final_encoded.dtypes.value_counts()\n",
        "\n",
        "    print(f\"\\nüìä Final Data Types:\")\n",
        "    print(f\"üöÇ Training: {dict(final_train_dtypes)}\")\n",
        "    print(f\"üîÆ Validation: {dict(final_val_dtypes)}\")\n",
        "\n",
        "    print(f\"\\nüìä Final Dataset Shapes:\")\n",
        "    print(f\"üöÇ Training: {train_final_encoded.shape}\")\n",
        "    print(f\"üîÆ Validation: {val_final_encoded.shape}\")\n",
        "\n",
        "    # Verify all columns are numeric\n",
        "    non_numeric_train = [col for col in train_final_encoded.columns\n",
        "                        if train_final_encoded[col].dtype not in ['int64', 'float64', 'int32', 'float32']]\n",
        "    non_numeric_val = [col for col in val_final_encoded.columns\n",
        "                      if val_final_encoded[col].dtype not in ['int64', 'float64', 'int32', 'float32']]\n",
        "\n",
        "    if non_numeric_train or non_numeric_val:\n",
        "        print(f\"‚ö†Ô∏è Warning: Non-numeric columns still exist:\")\n",
        "        print(f\"   Train: {non_numeric_train}\")\n",
        "        print(f\"   Val: {non_numeric_val}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ All columns are numeric!\")\n",
        "\n",
        "    # Log final metrics\n",
        "    mlflow.log_metric(\"final_train_shape_rows\", train_final_encoded.shape[0])\n",
        "    mlflow.log_metric(\"final_train_shape_cols\", train_final_encoded.shape[1])\n",
        "    mlflow.log_metric(\"final_val_shape_rows\", val_final_encoded.shape[0])\n",
        "    mlflow.log_metric(\"final_val_shape_cols\", val_final_encoded.shape[1])\n",
        "    mlflow.log_metric(\"categorical_columns_encoded\", len(categorical_columns))\n",
        "\n",
        "    # Update variable names for next steps\n",
        "    train_final = train_final_encoded\n",
        "    val_final = val_final_encoded\n",
        "\n",
        "    print(f\"üéØ Data is ready for outlier detection!\")"
      ],
      "metadata": {
        "id": "369LUxxJeZ3m",
        "outputId": "56735e7f-59c5-4328-fcb1-2acce6269ff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking data types and encoding categoricals: b05372d87cc545c8ba468ece324637d0\n",
            "üìä Checking data types...\n",
            "üöÇ Training data types: {dtype('float64'): np.int64(33), dtype('int64'): np.int64(18), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('bool'): np.int64(1), dtype('<M8[ns]'): np.int64(1), dtype('O'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üîÆ Validation data types: {dtype('float64'): np.int64(33), dtype('int64'): np.int64(18), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('bool'): np.int64(1), dtype('<M8[ns]'): np.int64(1), dtype('O'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üè∑Ô∏è Categorical columns found: ['Type', 'Store', 'Dept']\n",
            "üîß Applying one-hot encoding to 3 categorical columns...\n",
            "   Encoding Type...\n",
            "     Train unique: 3, Val unique: 3, Combined: 3\n",
            "     Created 3 dummy variables\n",
            "   Encoding Store...\n",
            "     Train unique: 45, Val unique: 45, Combined: 45\n",
            "     Created 45 dummy variables\n",
            "   Encoding Dept...\n",
            "     Train unique: 81, Val unique: 81, Combined: 81\n",
            "     Created 81 dummy variables\n",
            "‚úÖ One-hot encoding completed!\n",
            "üîß Converting all features to numeric types...\n",
            "   Converted 130 boolean columns to int\n",
            "\n",
            "üìä Final Data Types:\n",
            "üöÇ Training: {dtype('int64'): np.int64(146), dtype('float64'): np.int64(33), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('<M8[ns]'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üîÆ Validation: {dtype('int64'): np.int64(146), dtype('float64'): np.int64(33), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('<M8[ns]'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "\n",
            "üìä Final Dataset Shapes:\n",
            "üöÇ Training: (335761, 189)\n",
            "üîÆ Validation: (85809, 189)\n",
            "‚ö†Ô∏è Warning: Non-numeric columns still exist:\n",
            "   Train: ['Date', 'Week', 'Week_sin', 'Week_cos']\n",
            "   Val: ['Date', 'Week', 'Week_sin', 'Week_cos']\n",
            "üéØ Data is ready for outlier detection!\n",
            "üèÉ View run Data_Type_Check_And_Encoding at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/b05372d87cc545c8ba468ece324637d0\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 6: OUTLIER DETECTION USING ONLY TRAINING DATA (FIXING DATA LEAKAGE)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Outlier_Detection_Training_Only\") as run:\n",
        "\n",
        "    print(f\"üéØ CRITICAL FIX: Outlier detection using ONLY training data\")\n",
        "    print(f\"üîç Starting outlier detection: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"outlier_method\", \"holiday_aware_training_data_only\")\n",
        "    mlflow.log_param(\"data_leakage_fixed\", \"yes_using_only_training_data\")\n",
        "    mlflow.log_param(\"input_train_shape\", f\"{train_final.shape[0]}x{train_final.shape[1]}\")\n",
        "    mlflow.log_param(\"input_val_shape\", f\"{val_final.shape[0]}x{val_final.shape[1]}\")\n",
        "\n",
        "    # ============================================\n",
        "    # CALCULATE OUTLIER THRESHOLDS FROM TRAINING DATA ONLY\n",
        "    # ============================================\n",
        "\n",
        "    # Separate holiday vs non-holiday in TRAINING DATA ONLY\n",
        "    train_holiday = train_final[train_final['IsHoliday'] == True]\n",
        "    train_non_holiday = train_final[train_final['IsHoliday'] == False]\n",
        "\n",
        "    print(f\"üìä Training data holiday analysis:\")\n",
        "    print(f\"   Holiday records: {len(train_holiday):,} ({len(train_holiday)/len(train_final)*100:.1f}%)\")\n",
        "    print(f\"   Non-holiday records: {len(train_non_holiday):,} ({len(train_non_holiday)/len(train_final)*100:.1f}%)\")\n",
        "\n",
        "    def calculate_outlier_thresholds_from_training(holiday_data, non_holiday_data):\n",
        "        \"\"\"Calculate outlier thresholds using ONLY training data\"\"\"\n",
        "\n",
        "        # Holiday thresholds (more lenient)\n",
        "        holiday_q1 = holiday_data['Weekly_Sales'].quantile(0.25)\n",
        "        holiday_q3 = holiday_data['Weekly_Sales'].quantile(0.75)\n",
        "        holiday_iqr = holiday_q3 - holiday_q1\n",
        "        holiday_multiplier = 3.0  # More lenient for holidays\n",
        "\n",
        "        holiday_lower = holiday_q1 - holiday_multiplier * holiday_iqr\n",
        "        holiday_upper = holiday_q3 + holiday_multiplier * holiday_iqr\n",
        "        holiday_negative_threshold = -15000  # Allow more returns during holidays\n",
        "\n",
        "        # Non-holiday thresholds (stricter)\n",
        "        non_holiday_q1 = non_holiday_data['Weekly_Sales'].quantile(0.25)\n",
        "        non_holiday_q3 = non_holiday_data['Weekly_Sales'].quantile(0.75)\n",
        "        non_holiday_iqr = non_holiday_q3 - non_holiday_q1\n",
        "        non_holiday_multiplier = 2.5  # Standard for non-holidays\n",
        "\n",
        "        non_holiday_lower = non_holiday_q1 - non_holiday_multiplier * non_holiday_iqr\n",
        "        non_holiday_upper = non_holiday_q3 + non_holiday_multiplier * non_holiday_iqr\n",
        "        non_holiday_negative_threshold = -8000  # Stricter for non-holidays\n",
        "\n",
        "        # Holiday statistics\n",
        "        print(f\"\\nüìä Holiday Sales Statistics (Training Only):\")\n",
        "        print(f\"   Count: {len(holiday_data):,}\")\n",
        "        print(f\"   Mean: ${holiday_data['Weekly_Sales'].mean():,.2f}\")\n",
        "        print(f\"   Median: ${holiday_data['Weekly_Sales'].median():,.2f}\")\n",
        "        print(f\"   Q1: ${holiday_q1:,.2f}, Q3: ${holiday_q3:,.2f}\")\n",
        "        print(f\"   IQR: ${holiday_iqr:,.2f}\")\n",
        "        print(f\"   Min: ${holiday_data['Weekly_Sales'].min():,.2f}\")\n",
        "        print(f\"   Max: ${holiday_data['Weekly_Sales'].max():,.2f}\")\n",
        "\n",
        "        # Non-holiday statistics\n",
        "        print(f\"\\nüìä Non-Holiday Sales Statistics (Training Only):\")\n",
        "        print(f\"   Count: {len(non_holiday_data):,}\")\n",
        "        print(f\"   Mean: ${non_holiday_data['Weekly_Sales'].mean():,.2f}\")\n",
        "        print(f\"   Median: ${non_holiday_data['Weekly_Sales'].median():,.2f}\")\n",
        "        print(f\"   Q1: ${non_holiday_q1:,.2f}, Q3: ${non_holiday_q3:,.2f}\")\n",
        "        print(f\"   IQR: ${non_holiday_iqr:,.2f}\")\n",
        "        print(f\"   Min: ${non_holiday_data['Weekly_Sales'].min():,.2f}\")\n",
        "        print(f\"   Max: ${non_holiday_data['Weekly_Sales'].max():,.2f}\")\n",
        "\n",
        "        thresholds = {\n",
        "            'holiday': {\n",
        "                'lower': holiday_lower,\n",
        "                'upper': holiday_upper,\n",
        "                'negative_threshold': holiday_negative_threshold,\n",
        "                'multiplier': holiday_multiplier,\n",
        "                'q1': holiday_q1,\n",
        "                'q3': holiday_q3,\n",
        "                'iqr': holiday_iqr\n",
        "            },\n",
        "            'non_holiday': {\n",
        "                'lower': non_holiday_lower,\n",
        "                'upper': non_holiday_upper,\n",
        "                'negative_threshold': non_holiday_negative_threshold,\n",
        "                'multiplier': non_holiday_multiplier,\n",
        "                'q1': non_holiday_q1,\n",
        "                'q3': non_holiday_q3,\n",
        "                'iqr': non_holiday_iqr\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return thresholds\n",
        "\n",
        "    # Calculate thresholds using ONLY training data\n",
        "    outlier_thresholds = calculate_outlier_thresholds_from_training(train_holiday, train_non_holiday)\n",
        "\n",
        "    print(f\"\\nüéØ Outlier thresholds calculated from TRAINING DATA ONLY:\")\n",
        "    print(f\"   Holiday bounds: ${outlier_thresholds['holiday']['lower']:,.0f} to ${outlier_thresholds['holiday']['upper']:,.0f}\")\n",
        "    print(f\"   Holiday negative threshold: ${outlier_thresholds['holiday']['negative_threshold']:,.0f}\")\n",
        "    print(f\"   Non-holiday bounds: ${outlier_thresholds['non_holiday']['lower']:,.0f} to ${outlier_thresholds['non_holiday']['upper']:,.0f}\")\n",
        "    print(f\"   Non-holiday negative threshold: ${outlier_thresholds['non_holiday']['negative_threshold']:,.0f}\")\n",
        "\n",
        "    # Log thresholds to MLflow\n",
        "    for category in ['holiday', 'non_holiday']:\n",
        "        for threshold_type, value in outlier_thresholds[category].items():\n",
        "            mlflow.log_metric(f\"{category}_{threshold_type}\", value)\n",
        "\n",
        "    # ============================================\n",
        "    # APPLY THRESHOLDS TO BOTH TRAINING AND VALIDATION\n",
        "    # ============================================\n",
        "\n",
        "    def apply_outlier_detection(df, thresholds, dataset_name):\n",
        "        \"\"\"Apply outlier detection using pre-calculated thresholds\"\"\"\n",
        "\n",
        "        outliers = pd.Series([False] * len(df), index=df.index)\n",
        "\n",
        "        # Holiday outliers\n",
        "        holiday_mask = df['IsHoliday'] == True\n",
        "        if holiday_mask.sum() > 0:\n",
        "            holiday_outliers = (\n",
        "                (df.loc[holiday_mask, 'Weekly_Sales'] < thresholds['holiday']['lower']) |\n",
        "                (df.loc[holiday_mask, 'Weekly_Sales'] > thresholds['holiday']['upper']) |\n",
        "                (df.loc[holiday_mask, 'Weekly_Sales'] < thresholds['holiday']['negative_threshold'])\n",
        "            )\n",
        "            outliers.loc[holiday_mask] = holiday_outliers\n",
        "\n",
        "        # Non-holiday outliers\n",
        "        non_holiday_mask = df['IsHoliday'] == False\n",
        "        if non_holiday_mask.sum() > 0:\n",
        "            non_holiday_outliers = (\n",
        "                (df.loc[non_holiday_mask, 'Weekly_Sales'] < thresholds['non_holiday']['lower']) |\n",
        "                (df.loc[non_holiday_mask, 'Weekly_Sales'] > thresholds['non_holiday']['upper']) |\n",
        "                (df.loc[non_holiday_mask, 'Weekly_Sales'] < thresholds['non_holiday']['negative_threshold'])\n",
        "            )\n",
        "            outliers.loc[non_holiday_mask] = non_holiday_outliers\n",
        "\n",
        "        outlier_count = outliers.sum()\n",
        "        outlier_percentage = (outlier_count / len(df)) * 100\n",
        "\n",
        "        # Breakdown by holiday status\n",
        "        holiday_outliers_count = outliers[holiday_mask].sum() if holiday_mask.sum() > 0 else 0\n",
        "        non_holiday_outliers_count = outliers[non_holiday_mask].sum() if non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "        print(f\"\\nüìä {dataset_name} Outlier Detection Results:\")\n",
        "        print(f\"   Total outliers: {outlier_count:,} ({outlier_percentage:.2f}%)\")\n",
        "        print(f\"   Holiday outliers: {holiday_outliers_count:,}\")\n",
        "        print(f\"   Non-holiday outliers: {non_holiday_outliers_count:,}\")\n",
        "\n",
        "        # Log metrics\n",
        "        mlflow.log_metric(f\"{dataset_name.lower()}_total_outliers\", outlier_count)\n",
        "        mlflow.log_metric(f\"{dataset_name.lower()}_outlier_percentage\", outlier_percentage)\n",
        "        mlflow.log_metric(f\"{dataset_name.lower()}_holiday_outliers\", holiday_outliers_count)\n",
        "        mlflow.log_metric(f\"{dataset_name.lower()}_non_holiday_outliers\", non_holiday_outliers_count)\n",
        "\n",
        "        return outliers\n",
        "\n",
        "    # Apply to training data\n",
        "    print(f\"\\nüöÇ Applying outlier detection to training data...\")\n",
        "    train_outliers = apply_outlier_detection(train_final, outlier_thresholds, \"Training\")\n",
        "    train_clean = train_final[~train_outliers].copy()\n",
        "\n",
        "    # Apply SAME thresholds to validation data\n",
        "    print(f\"\\nüîÆ Applying SAME thresholds to validation data...\")\n",
        "    val_outliers = apply_outlier_detection(val_final, outlier_thresholds, \"Validation\")\n",
        "    val_clean = val_final[~val_outliers].copy()\n",
        "\n",
        "    print(f\"\\n‚úÖ Outlier detection completed using training-derived thresholds!\")\n",
        "    print(f\"üöÇ Training: {len(train_clean):,} records (removed {len(train_final) - len(train_clean):,})\")\n",
        "    print(f\"üîÆ Validation: {len(val_clean):,} records (removed {len(val_final) - len(val_clean):,})\")\n",
        "\n",
        "    # Final data quality check\n",
        "    print(f\"\\nüîç Final data quality check:\")\n",
        "    print(f\"   Training NaNs: {train_clean.isnull().sum().sum()}\")\n",
        "    print(f\"   Validation NaNs: {val_clean.isnull().sum().sum()}\")\n",
        "    print(f\"   Training infinite values: {np.isinf(train_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "    print(f\"   Validation infinite values: {np.isinf(val_clean.select_dtypes(include=[np.number])).sum().sum()}\")\n",
        "\n",
        "    # Log final metrics\n",
        "    mlflow.log_metric(\"final_train_records\", len(train_clean))\n",
        "    mlflow.log_metric(\"final_val_records\", len(val_clean))\n",
        "    mlflow.log_metric(\"train_records_removed\", len(train_final) - len(train_clean))\n",
        "    mlflow.log_metric(\"val_records_removed\", len(val_final) - len(val_clean))\n",
        "\n",
        "    # Update variables for next step\n",
        "    train_final_clean = train_clean\n",
        "    val_final_clean = val_clean\n",
        "\n",
        "    print(f\"\\nüéØ Data is ready for XGBoost training!\")\n",
        "    print(f\"‚úÖ NO DATA LEAKAGE: Outlier thresholds calculated from training data only!\")"
      ],
      "metadata": {
        "id": "5_oAdmSgefC1",
        "outputId": "bd572f99-79c7-47b1-b897-754c273a3656",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ CRITICAL FIX: Outlier detection using ONLY training data\n",
            "üîç Starting outlier detection: 84e3b26883fb4a25b086bf89f08a7046\n",
            "üìä Training data holiday analysis:\n",
            "   Holiday records: 26,695 (8.0%)\n",
            "   Non-holiday records: 309,066 (92.0%)\n",
            "\n",
            "üìä Holiday Sales Statistics (Training Only):\n",
            "   Count: 26,695\n",
            "   Mean: $17,118.17\n",
            "   Median: $7,978.35\n",
            "   Q1: $2,112.02, Q3: $21,266.14\n",
            "   IQR: $19,154.12\n",
            "   Min: $-798.00\n",
            "   Max: $693,099.36\n",
            "\n",
            "üìä Non-Holiday Sales Statistics (Training Only):\n",
            "   Count: 309,066\n",
            "   Mean: $15,938.62\n",
            "   Median: $7,625.23\n",
            "   Q1: $2,104.64, Q3: $20,112.22\n",
            "   IQR: $18,007.58\n",
            "   Min: $-4,988.94\n",
            "   Max: $406,988.63\n",
            "\n",
            "üéØ Outlier thresholds calculated from TRAINING DATA ONLY:\n",
            "   Holiday bounds: $-55,350 to $78,728\n",
            "   Holiday negative threshold: $-15,000\n",
            "   Non-holiday bounds: $-42,914 to $65,131\n",
            "   Non-holiday negative threshold: $-8,000\n",
            "\n",
            "üöÇ Applying outlier detection to training data...\n",
            "\n",
            "üìä Training Outlier Detection Results:\n",
            "   Total outliers: 14,179 (4.22%)\n",
            "   Holiday outliers: 807\n",
            "   Non-holiday outliers: 13,372\n",
            "\n",
            "üîÆ Applying SAME thresholds to validation data...\n",
            "\n",
            "üìä Validation Outlier Detection Results:\n",
            "   Total outliers: 3,549 (4.14%)\n",
            "   Holiday outliers: 87\n",
            "   Non-holiday outliers: 3,462\n",
            "\n",
            "‚úÖ Outlier detection completed using training-derived thresholds!\n",
            "üöÇ Training: 321,582 records (removed 14,179)\n",
            "üîÆ Validation: 82,260 records (removed 3,549)\n",
            "\n",
            "üîç Final data quality check:\n",
            "   Training NaNs: 0\n",
            "   Validation NaNs: 0\n",
            "   Training infinite values: 0\n",
            "   Validation infinite values: 0\n",
            "\n",
            "üéØ Data is ready for XGBoost training!\n",
            "‚úÖ NO DATA LEAKAGE: Outlier thresholds calculated from training data only!\n",
            "üèÉ View run Outlier_Detection_Training_Only at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/84e3b26883fb4a25b086bf89f08a7046\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 7: XGBOOST TRAINING (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Training_Fixed_Data_Leakage\") as run:\n",
        "\n",
        "    print(f\"üöÄ Starting XGBoost training with FIXED data leakage: {run.info.run_id}\")\n",
        "\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import joblib\n",
        "\n",
        "    mlflow.log_param(\"model_type\", \"xgboost_regressor\")\n",
        "    mlflow.log_param(\"evaluation_metric\", \"WMAE_weighted_mean_absolute_error\")\n",
        "    mlflow.log_param(\"data_leakage_status\", \"FIXED\")\n",
        "    mlflow.log_param(\"train_size\", len(train_final_clean))\n",
        "    mlflow.log_param(\"val_size\", len(val_final_clean))\n",
        "\n",
        "    # 1. Prepare features and target\n",
        "    print(\"üîß Preparing features and target variables...\")\n",
        "\n",
        "    target_column = 'Weekly_Sales'\n",
        "    exclude_columns = ['Weekly_Sales', 'Date']\n",
        "\n",
        "    # Get feature columns\n",
        "    feature_columns = [col for col in train_final_clean.columns if col not in exclude_columns]\n",
        "\n",
        "    print(f\"   Total features available: {len(feature_columns)}\")\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_final_clean[feature_columns].copy()\n",
        "    y_train = train_final_clean[target_column].copy()\n",
        "    X_val = val_final_clean[feature_columns].copy()\n",
        "    y_val = val_final_clean[target_column].copy()\n",
        "\n",
        "    # Get holiday flags for WMAE calculation\n",
        "    train_is_holiday = train_final_clean['IsHoliday'].copy()\n",
        "    val_is_holiday = val_final_clean['IsHoliday'].copy()\n",
        "\n",
        "    print(f\"üìä Final data shapes:\")\n",
        "    print(f\"   X_train: {X_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}\")\n",
        "\n",
        "    # 2. Define WMAE function\n",
        "    def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "        \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "        abs_errors = np.abs(y_true - y_pred)\n",
        "        weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "        wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "        return wmae\n",
        "\n",
        "    # 3. Train XGBoost model (Fixed)\n",
        "    print(f\"ü§ñ Training XGBoost model...\")\n",
        "\n",
        "    # Model parameters\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'objective': 'reg:squarederror'\n",
        "    }\n",
        "\n",
        "    # Log parameters\n",
        "    for param, value in xgb_params.items():\n",
        "        mlflow.log_param(f\"xgb_{param}\", value)\n",
        "\n",
        "    # Initialize model\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "\n",
        "    # Train model (simplified without eval_metric in fit)\n",
        "    print(\"   Fitting model...\")\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"‚úÖ Model training completed!\")\n",
        "\n",
        "    # 4. Make predictions\n",
        "    print(f\"üîÆ Making predictions...\")\n",
        "\n",
        "    train_pred = xgb_model.predict(X_train)\n",
        "    val_pred = xgb_model.predict(X_val)\n",
        "\n",
        "    # 5. Calculate metrics\n",
        "    print(f\"üìä Calculating metrics...\")\n",
        "\n",
        "    # Training metrics\n",
        "    train_mae = mean_absolute_error(y_train, train_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    train_wmae = calculate_wmae(y_train, train_pred, train_is_holiday)\n",
        "\n",
        "    # Validation metrics\n",
        "    val_mae = mean_absolute_error(y_val, val_pred)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "    val_wmae = calculate_wmae(y_val, val_pred, val_is_holiday)\n",
        "\n",
        "    # Holiday breakdown\n",
        "    train_holiday_mask = train_is_holiday == True\n",
        "    train_non_holiday_mask = train_is_holiday == False\n",
        "    val_holiday_mask = val_is_holiday == True\n",
        "    val_non_holiday_mask = val_is_holiday == False\n",
        "\n",
        "    # Holiday MAE\n",
        "    train_holiday_mae = mean_absolute_error(y_train[train_holiday_mask], train_pred[train_holiday_mask]) if train_holiday_mask.sum() > 0 else 0\n",
        "    train_non_holiday_mae = mean_absolute_error(y_train[train_non_holiday_mask], train_pred[train_non_holiday_mask]) if train_non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "    val_holiday_mae = mean_absolute_error(y_val[val_holiday_mask], val_pred[val_holiday_mask]) if val_holiday_mask.sum() > 0 else 0\n",
        "    val_non_holiday_mae = mean_absolute_error(y_val[val_non_holiday_mask], val_pred[val_non_holiday_mask]) if val_non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "    # 6. Display results\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üìä EXPERIMENT 4 RESULTS (DATA LEAKAGE FIXED)\")\n",
        "    print(f\"=\"*80)\n",
        "\n",
        "    print(f\"\\nüöÇ Training Metrics:\")\n",
        "    print(f\"   WMAE: ${train_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${train_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${train_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {train_r2:.4f}\")\n",
        "    print(f\"   Holiday MAE: ${train_holiday_mae:,.2f}\")\n",
        "    print(f\"   Non-Holiday MAE: ${train_non_holiday_mae:,.2f}\")\n",
        "\n",
        "    print(f\"\\nüîÆ Validation Metrics:\")\n",
        "    print(f\"   WMAE: ${val_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${val_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${val_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {val_r2:.4f}\")\n",
        "    print(f\"   Holiday MAE: ${val_holiday_mae:,.2f}\")\n",
        "    print(f\"   Non-Holiday MAE: ${val_non_holiday_mae:,.2f}\")\n",
        "\n",
        "    # 7. Log metrics to MLflow\n",
        "    metrics_to_log = {\n",
        "        \"train_wmae\": train_wmae,\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"train_r2\": train_r2,\n",
        "        \"train_holiday_mae\": train_holiday_mae,\n",
        "        \"train_non_holiday_mae\": train_non_holiday_mae,\n",
        "        \"val_wmae\": val_wmae,\n",
        "        \"val_mae\": val_mae,\n",
        "        \"val_rmse\": val_rmse,\n",
        "        \"val_r2\": val_r2,\n",
        "        \"val_holiday_mae\": val_holiday_mae,\n",
        "        \"val_non_holiday_mae\": val_non_holiday_mae,\n",
        "        \"total_features\": len(feature_columns),\n",
        "        \"train_holiday_count\": train_holiday_mask.sum(),\n",
        "        \"val_holiday_count\": val_holiday_mask.sum()\n",
        "    }\n",
        "\n",
        "    for metric_name, value in metrics_to_log.items():\n",
        "        mlflow.log_metric(metric_name, value)\n",
        "\n",
        "    # 8. Feature importance\n",
        "    print(f\"\\nüéØ Top 15 Feature Importance:\")\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': xgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
        "        print(f\"   {i+1:2d}. {row['feature']:35s}: {row['importance']:.4f}\")\n",
        "\n",
        "    # Log top features\n",
        "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
        "        mlflow.log_metric(f\"feature_importance_rank_{i+1}\", row['importance'])\n",
        "        mlflow.log_param(f\"top_feature_{i+1}\", row['feature'])\n",
        "\n",
        "    # 9. Model performance analysis\n",
        "    print(f\"\\nüìà Model Performance Analysis:\")\n",
        "    print(f\"   Training vs Validation WMAE: ${train_wmae:,.2f} vs ${val_wmae:,.2f}\")\n",
        "    print(f\"   Overfitting check: {(train_wmae - val_wmae)/val_wmae*100:.1f}% difference\")\n",
        "    print(f\"   Holiday vs Non-Holiday performance:\")\n",
        "    print(f\"     Holiday MAE: ${val_holiday_mae:,.2f} ({val_holiday_mask.sum()} records)\")\n",
        "    print(f\"     Non-Holiday MAE: ${val_non_holiday_mae:,.2f} ({val_non_holiday_mask.sum()} records)\")\n",
        "\n",
        "    # 10. Save model\n",
        "    print(f\"\\nüíæ Saving model...\")\n",
        "    model_path = \"xgboost_walmart_experiment4_fixed.pkl\"\n",
        "    joblib.dump(xgb_model, model_path)\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "    # Save feature importance\n",
        "    feature_importance.to_csv(\"feature_importance_experiment4.csv\", index=False)\n",
        "    mlflow.log_artifact(\"feature_importance_experiment4.csv\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üéâ EXPERIMENT 4 COMPLETED!\")\n",
        "    print(f\"‚úÖ Data Leakage: FIXED\")\n",
        "    print(f\"üéØ Main Metric (Validation WMAE): ${val_wmae:,.2f}\")\n",
        "    print(f\"üìä Validation R¬≤: {val_r2:.4f}\")\n",
        "    print(f\"üîß Features Used: {len(feature_columns)}\")\n",
        "    print(f\"=\"*80)\n",
        "\n",
        "    # Final status\n",
        "    mlflow.log_param(\"experiment_status\", \"COMPLETED\")\n",
        "    mlflow.log_param(\"data_leakage_fixed\", \"YES\")\n",
        "    mlflow.log_param(\"main_wmae\", f\"${val_wmae:.2f}\")"
      ],
      "metadata": {
        "id": "zy54QDAyfHoH",
        "outputId": "66aa4b60-a709-4496-cdac-869b342fd1d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting XGBoost training with FIXED data leakage: 1484ec1c722541a3b793d05622ee2364\n",
            "üîß Preparing features and target variables...\n",
            "   Total features available: 187\n",
            "üìä Final data shapes:\n",
            "   X_train: (321582, 187)\n",
            "   X_val: (82260, 187)\n",
            "ü§ñ Training XGBoost model...\n",
            "   Fitting model...\n",
            "‚úÖ Model training completed!\n",
            "üîÆ Making predictions...\n",
            "üìä Calculating metrics...\n",
            "\n",
            "================================================================================\n",
            "üìä EXPERIMENT 4 RESULTS (DATA LEAKAGE FIXED)\n",
            "================================================================================\n",
            "\n",
            "üöÇ Training Metrics:\n",
            "   WMAE: $52.89\n",
            "   MAE: $52.34\n",
            "   RMSE: $77.13\n",
            "   R¬≤: 1.0000\n",
            "   Holiday MAE: $54.58\n",
            "   Non-Holiday MAE: $52.15\n",
            "\n",
            "üîÆ Validation Metrics:\n",
            "   WMAE: $93.04\n",
            "   MAE: $90.73\n",
            "   RMSE: $196.31\n",
            "   R¬≤: 0.9998\n",
            "   Holiday MAE: $109.52\n",
            "   Non-Holiday MAE: $90.05\n",
            "\n",
            "üéØ Top 15 Feature Importance:\n",
            "    1. Weekly_Sales_ewm_0.5               : 0.7531\n",
            "    2. Weekly_Sales_ewm_0.3               : 0.2101\n",
            "    3. Weekly_Sales_rolling_min_4         : 0.0115\n",
            "    4. Weekly_Sales_trend_diff            : 0.0046\n",
            "    5. Weekly_Sales_rolling_max_4         : 0.0034\n",
            "    6. Weekly_Sales_rolling_std_4         : 0.0025\n",
            "    7. Weekly_Sales_monthly_diff          : 0.0022\n",
            "    8. IsHoliday                          : 0.0021\n",
            "    9. Dept_7                             : 0.0019\n",
            "   10. IsMajorHoliday                     : 0.0015\n",
            "   11. Weekly_Sales_lag_1                 : 0.0007\n",
            "   12. Weekly_Sales_rolling_max_8         : 0.0005\n",
            "   13. Weekly_Sales_rolling_mean_4        : 0.0004\n",
            "   14. IsChristmasWeek                    : 0.0004\n",
            "   15. Weekly_Sales_lag_2                 : 0.0004\n",
            "\n",
            "üìà Model Performance Analysis:\n",
            "   Training vs Validation WMAE: $52.89 vs $93.04\n",
            "   Overfitting check: -43.2% difference\n",
            "   Holiday vs Non-Holiday performance:\n",
            "     Holiday MAE: $109.52 (2879 records)\n",
            "     Non-Holiday MAE: $90.05 (79381 records)\n",
            "\n",
            "üíæ Saving model...\n",
            "\n",
            "================================================================================\n",
            "üéâ EXPERIMENT 4 COMPLETED!\n",
            "‚úÖ Data Leakage: FIXED\n",
            "üéØ Main Metric (Validation WMAE): $93.04\n",
            "üìä Validation R¬≤: 0.9998\n",
            "üîß Features Used: 187\n",
            "================================================================================\n",
            "üèÉ View run XGBoost_Training_Fixed_Data_Leakage at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/1484ec1c722541a3b793d05622ee2364\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HbbzKtPPiHKk"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "81d6ff808a96465fa3024c7401c36ca8": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_61fca76aad864cfab4bb210be2636831",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m‚†º\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">‚†º</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "61fca76aad864cfab4bb210be2636831": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}