{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# experiment_4_k"
      ],
      "metadata": {
        "id": "R3hfw6V-alWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMBxqiRyaKmC",
        "outputId": "148f9d7f-c67b-4b74-ff7e-e33e63aa0e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cLdXVjrean6a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Gg3HtWQbao_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2XfVrrCwaqXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP7DAfGWarmW",
        "outputId": "65eb0980-81a4-465b-b4da-b602328b43ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 618MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9rkhMhdas2Y",
        "outputId": "67f098c2-c18a-4cd9-c631-866626ef9418"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# EXPERIMENT 4: FIXING DATA LEAKAGE - SPLIT FIRST, THEN PREPROCESS\n",
        "# ================================================================================\n",
        "\n",
        "# Step 1: Setup and MLflow/DagsHub Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install prophet plotly mlflow dagshub xgboost -q\n",
        "\n",
        "# Setup MLflow and DagsHub\n",
        "import mlflow\n",
        "import dagshub\n",
        "\n",
        "# DagsHub setup\n",
        "dagshub.init(repo_owner='konstantine25b',\n",
        "             repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "             mlflow=True)\n",
        "\n",
        "# Set tracking URI\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "mlflow.set_experiment(\"Experiment_4_Fixed_Data_Leakage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "itxEe9N7auGr",
        "outputId": "35fcab15-33dc-46d1-b141-5a5ac451d47c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/43ce0a29767b4be4b47a7e6d431382c2', creation_time=1750844432684, experiment_id='3', last_update_time=1750844432684, lifecycle_stage='active', name='Experiment_4_Fixed_Data_Leakage', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run(run_name=\"Data_Loading_Fixed\") as run:\n",
        "    # Load datasets\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    train = pd.read_csv('train.csv')\n",
        "    stores = pd.read_csv('stores.csv')\n",
        "\n",
        "    # Convert Date column\n",
        "    train['Date'] = pd.to_datetime(train['Date'])\n",
        "\n",
        "    # Merge with stores data\n",
        "    train_merged = train.merge(stores, on='Store', how='left')\n",
        "\n",
        "    print(f\"‚úÖ Data loaded: {train_merged.shape}\")\n",
        "    print(f\"üìÖ Date range: {train['Date'].min()} to {train['Date'].max()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzp6nJcpdVgk",
        "outputId": "a56d175a-63bc-49b6-8a2e-1097c1b2ddbf"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Data loaded: (421570, 7)\n",
            "üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üèÉ View run Data_Loading_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/df0f0eea88e0441b98208cbed624b5bc\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 2: SINGLE PROPER TEMPORAL SPLIT (BEFORE ANY PREPROCESSING)\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"Proper_Temporal_Split_Fixed\") as run:\n",
        "\n",
        "    print(f\"üî™ PROPER TEMPORAL SPLIT - NO DATA LEAKAGE\")\n",
        "\n",
        "    # Sort data chronologically\n",
        "    data_sorted = train_merged.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Use SINGLE temporal split - 80% of TIME for training\n",
        "    min_date = data_sorted['Date'].min()\n",
        "    max_date = data_sorted['Date'].max()\n",
        "    total_days = (max_date - min_date).days\n",
        "    split_days = int(total_days * 0.8)\n",
        "    split_date = min_date + timedelta(days=split_days)\n",
        "\n",
        "    # Align to Friday (weekly data)\n",
        "    while split_date.weekday() != 4:\n",
        "        split_date += timedelta(days=1)\n",
        "\n",
        "    # Create the split\n",
        "    train_data = data_sorted[data_sorted['Date'] < split_date].copy()\n",
        "    val_data = data_sorted[data_sorted['Date'] >= split_date].copy()\n",
        "\n",
        "    print(f\"üìÖ Split date: {split_date}\")\n",
        "    print(f\"üöÇ Training: {len(train_data):,} records\")\n",
        "    print(f\"üîÆ Validation: {len(val_data):,} records\")\n",
        "    print(f\"üìä Training date range: {train_data['Date'].min()} to {train_data['Date'].max()}\")\n",
        "    print(f\"üìä Validation date range: {val_data['Date'].min()} to {val_data['Date'].max()}\")\n",
        "\n",
        "    # Verify no data leakage\n",
        "    assert train_data['Date'].max() < val_data['Date'].min(), \"‚ùå DATA LEAKAGE DETECTED!\"\n",
        "    print(\"‚úÖ No temporal data leakage - training ends before validation starts\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53e5HuBRdjFk",
        "outputId": "1743a492-d411-47d9-c399-211aa79ee6c1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üî™ PROPER TEMPORAL SPLIT - NO DATA LEAKAGE\n",
            "üìÖ Split date: 2012-04-13 00:00:00\n",
            "üöÇ Training: 335,761 records\n",
            "üîÆ Validation: 85,809 records\n",
            "üìä Training date range: 2010-02-05 00:00:00 to 2012-04-06 00:00:00\n",
            "üìä Validation date range: 2012-04-13 00:00:00 to 2012-10-26 00:00:00\n",
            "‚úÖ No temporal data leakage - training ends before validation starts\n",
            "üèÉ View run Proper_Temporal_Split_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/f40a3a2fde6148e9b782891c64b37a42\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 3: OUTLIER REMOVAL (ONLY ON TRAINING DATA)\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"Outlier_Removal_Training_Only\") as run:\n",
        "\n",
        "    print(f\"üîç OUTLIER REMOVAL - TRAINING DATA ONLY\")\n",
        "\n",
        "    # Calculate outlier thresholds ONLY on training data\n",
        "    train_sales_by_group = train_data.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "    Q1_train = train_sales_by_group.transform(lambda x: x.quantile(0.25))\n",
        "    Q3_train = train_sales_by_group.transform(lambda x: x.quantile(0.75))\n",
        "    IQR_train = Q3_train - Q1_train\n",
        "\n",
        "    lower_bound = Q1_train - 1.5 * IQR_train\n",
        "    upper_bound = Q3_train + 1.5 * IQR_train\n",
        "\n",
        "    # Find outliers in training data\n",
        "    outliers_mask = (train_data['Weekly_Sales'] < lower_bound) | (train_data['Weekly_Sales'] > upper_bound)\n",
        "    outliers_count = outliers_mask.sum()\n",
        "\n",
        "    # Remove outliers from training data ONLY\n",
        "    train_data_clean = train_data[~outliers_mask].copy()\n",
        "    # Keep validation data unchanged\n",
        "    val_data_clean = val_data.copy()\n",
        "\n",
        "    print(f\"üóëÔ∏è Removed {outliers_count:,} outliers from training data ({outliers_count/len(train_data)*100:.1f}%)\")\n",
        "    print(f\"üöÇ Training after outlier removal: {len(train_data_clean):,}\")\n",
        "    print(f\"üîÆ Validation unchanged: {len(val_data_clean):,}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8sy7qqAd16E",
        "outputId": "596d66a0-daea-43fa-980b-874bf1d6d584"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç OUTLIER REMOVAL - TRAINING DATA ONLY\n",
            "üóëÔ∏è Removed 15,467 outliers from training data (4.6%)\n",
            "üöÇ Training after outlier removal: 320,294\n",
            "üîÆ Validation unchanged: 85,809\n",
            "üèÉ View run Outlier_Removal_Training_Only at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/f2aa6279169f4c4d93f1346dee918634\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 4: FEATURE ENGINEERING (APPLIED SEPARATELY)\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"Feature_Engineering_Fixed\") as run:\n",
        "\n",
        "    print(f\"üîß FEATURE ENGINEERING - APPLIED SEPARATELY TO EACH SET\")\n",
        "\n",
        "    def create_date_features(df):\n",
        "        \"\"\"Create basic date features\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "\n",
        "        # Cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "        # Days from start\n",
        "        reference_date = pd.Timestamp('2010-02-05')\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "\n",
        "        # Holiday features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        return df\n",
        "\n",
        "    def create_lag_features(df, target_col='Weekly_Sales'):\n",
        "        \"\"\"Create lag features without data leakage\"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # Simple lag features\n",
        "        for lag in [1, 2, 4]:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag)\n",
        "\n",
        "        # Fill NaNs with 0 (for early periods without enough history)\n",
        "        lag_cols = [col for col in df.columns if 'lag_' in col]\n",
        "        for col in lag_cols:\n",
        "            df[col] = df[col].fillna(0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply feature engineering to each set separately\n",
        "    print(\"üöÇ Creating features for training set...\")\n",
        "    train_features = create_date_features(train_data_clean)\n",
        "    train_final = create_lag_features(train_features)\n",
        "\n",
        "    print(\"üîÆ Creating features for validation set...\")\n",
        "    val_features = create_date_features(val_data_clean)\n",
        "    val_final = create_lag_features(val_features)\n",
        "\n",
        "    # Ensure same columns\n",
        "    common_cols = list(set(train_final.columns).intersection(set(val_final.columns)))\n",
        "    train_final = train_final[common_cols]\n",
        "    val_final = val_final[common_cols]\n",
        "\n",
        "    print(f\"‚úÖ Feature engineering completed\")\n",
        "    print(f\"üöÇ Training shape: {train_final.shape}\")\n",
        "    print(f\"üîÆ Validation shape: {val_final.shape}\")\n",
        "    print(f\"üìä Common features: {len(common_cols)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "369LUxxJeZ3m",
        "outputId": "b5a40b42-b10e-44ca-d11c-b25fa865c2f7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß FEATURE ENGINEERING - APPLIED SEPARATELY TO EACH SET\n",
            "üöÇ Creating features for training set...\n",
            "üîÆ Creating features for validation set...\n",
            "‚úÖ Feature engineering completed\n",
            "üöÇ Training shape: (320294, 19)\n",
            "üîÆ Validation shape: (85809, 19)\n",
            "üìä Common features: 19\n",
            "üèÉ View run Feature_Engineering_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/c31b52911d8341e9a90ed105050da1d8\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ================================================================================\n",
        "# STEP 5: PREPARE DATA FOR TRAINING (CRITICAL FIX)\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"Data_Preparation_Fixed\") as run:\n",
        "\n",
        "    print(f\"üìä PREPARING DATA FOR TRAINING - CRITICAL VERIFICATION\")\n",
        "\n",
        "    # Define features and target\n",
        "    target_col = 'Weekly_Sales'\n",
        "    exclude_cols = ['Weekly_Sales', 'Date']\n",
        "    feature_cols = [col for col in train_final.columns if col not in exclude_cols]\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_final[feature_cols].copy()\n",
        "    y_train = train_final[target_col].copy()\n",
        "    train_is_holiday = train_final['IsHoliday'].copy()\n",
        "\n",
        "    # Prepare validation data\n",
        "    X_val = val_final[feature_cols].copy()\n",
        "    y_val = val_final[target_col].copy()\n",
        "    val_is_holiday = val_final['IsHoliday'].copy()\n",
        "\n",
        "    # CRITICAL VERIFICATION - PREVENT DATA SHAPE CHANGES\n",
        "    assert len(X_train) == len(train_data_clean), f\"‚ùå Training data size changed! Expected {len(train_data_clean)}, got {len(X_train)}\"\n",
        "    assert len(X_val) == len(val_data_clean), f\"‚ùå Validation data size changed! Expected {len(val_data_clean)}, got {len(X_val)}\"\n",
        "\n",
        "    print(f\"‚úÖ Data shape verification passed!\")\n",
        "    print(f\"üöÇ X_train: {X_train.shape}\")\n",
        "    print(f\"üöÇ y_train: {y_train.shape}\")\n",
        "    print(f\"üîÆ X_val: {X_val.shape}\")\n",
        "    print(f\"üîÆ y_val: {y_val.shape}\")\n",
        "    print(f\"üìä Features: {len(feature_cols)}\")\n",
        "\n",
        "    # Handle any remaining NaNs\n",
        "    if X_train.isnull().sum().sum() > 0 or X_val.isnull().sum().sum() > 0:\n",
        "        print(\"‚ö†Ô∏è Filling remaining NaNs...\")\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_val = X_val.fillna(0)\n"
      ],
      "metadata": {
        "id": "gnUSFtnf3dBc",
        "outputId": "b270a6c7-1431-4027-d364-cf6d8f190ca2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä PREPARING DATA FOR TRAINING - CRITICAL VERIFICATION\n",
            "‚úÖ Data shape verification passed!\n",
            "üöÇ X_train: (320294, 17)\n",
            "üöÇ y_train: (320294,)\n",
            "üîÆ X_val: (85809, 17)\n",
            "üîÆ y_val: (85809,)\n",
            "üìä Features: 17\n",
            "üèÉ View run Data_Preparation_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/80d1164881464557addf982a51a4d80e\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 6: DATA TYPE FIX BEFORE XGBOOST TRAINING\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"Data_Type_Fix_Before_XGBoost\") as run:\n",
        "\n",
        "    print(f\"üîß FIXING DATA TYPES FOR XGBOOST\")\n",
        "\n",
        "    # Check data types\n",
        "    print(\"üìä Checking data types...\")\n",
        "    print(\"Training data types:\")\n",
        "    print(X_train.dtypes.value_counts())\n",
        "    print(\"\\nValidation data types:\")\n",
        "    print(X_val.dtypes.value_counts())\n",
        "\n",
        "    # Find problematic columns\n",
        "    object_cols = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype == 'object' or X_val[col].dtype == 'object':\n",
        "            object_cols.append(col)\n",
        "            print(f\"   Found object column: {col}\")\n",
        "\n",
        "    # Fix categorical columns with one-hot encoding\n",
        "    if object_cols:\n",
        "        print(f\"üîß One-hot encoding {len(object_cols)} categorical columns...\")\n",
        "\n",
        "        for col in object_cols:\n",
        "            print(f\"   Encoding {col}...\")\n",
        "\n",
        "            # Combine train and val for consistent encoding\n",
        "            combined_data = pd.concat([\n",
        "                X_train[col].reset_index(drop=True),\n",
        "                X_val[col].reset_index(drop=True)\n",
        "            ])\n",
        "\n",
        "            # Create dummy variables\n",
        "            dummies = pd.get_dummies(combined_data, prefix=col, dummy_na=False)\n",
        "\n",
        "            # Split back\n",
        "            train_dummies = dummies.iloc[:len(X_train)].copy()\n",
        "            val_dummies = dummies.iloc[len(X_train):].copy()\n",
        "\n",
        "            # Reset indices to match original data\n",
        "            train_dummies.index = X_train.index\n",
        "            val_dummies.index = X_val.index\n",
        "\n",
        "            # Add dummy columns and remove original\n",
        "            X_train = pd.concat([X_train.drop(columns=[col]), train_dummies], axis=1)\n",
        "            X_val = pd.concat([X_val.drop(columns=[col]), val_dummies], axis=1)\n",
        "\n",
        "            print(f\"     Created {len(dummies.columns)} dummy variables\")\n",
        "\n",
        "    # Convert any remaining non-numeric columns\n",
        "    print(\"üîß Converting remaining columns to numeric...\")\n",
        "\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype not in ['int64', 'float64', 'int32', 'float32', 'bool']:\n",
        "            print(f\"   Converting {col} from {X_train[col].dtype}\")\n",
        "            X_train[col] = pd.to_numeric(X_train[col], errors='coerce')\n",
        "            X_val[col] = pd.to_numeric(X_val[col], errors='coerce')\n",
        "\n",
        "    # Convert boolean to int\n",
        "    bool_cols = []\n",
        "    for col in X_train.columns:\n",
        "        if X_train[col].dtype == 'bool':\n",
        "            bool_cols.append(col)\n",
        "            X_train[col] = X_train[col].astype(int)\n",
        "            X_val[col] = X_val[col].astype(int)\n",
        "\n",
        "    if bool_cols:\n",
        "        print(f\"   Converted {len(bool_cols)} boolean columns to int\")\n",
        "\n",
        "    # Fill any NaNs introduced during conversion\n",
        "    train_nans = X_train.isnull().sum().sum()\n",
        "    val_nans = X_val.isnull().sum().sum()\n",
        "\n",
        "    if train_nans > 0 or val_nans > 0:\n",
        "        print(f\"‚ö†Ô∏è Filling NaNs: Train={train_nans}, Val={val_nans}\")\n",
        "        X_train = X_train.fillna(0)\n",
        "        X_val = X_val.fillna(0)\n",
        "\n",
        "    # Final verification\n",
        "    print(f\"\\n‚úÖ Final data type check:\")\n",
        "    print(f\"   Training data types: {X_train.dtypes.value_counts().to_dict()}\")\n",
        "    print(f\"   Validation data types: {X_val.dtypes.value_counts().to_dict()}\")\n",
        "    print(f\"   Training shape: {X_train.shape}\")\n",
        "    print(f\"   Validation shape: {X_val.shape}\")\n",
        "\n",
        "    # Verify no object columns remain\n",
        "    train_objects = [col for col in X_train.columns if X_train[col].dtype == 'object']\n",
        "    val_objects = [col for col in X_val.columns if X_val[col].dtype == 'object']\n",
        "\n",
        "    if train_objects or val_objects:\n",
        "        print(f\"‚ùå Still have object columns: Train={train_objects}, Val={val_objects}\")\n",
        "        raise ValueError(\"Object columns still exist!\")\n",
        "    else:\n",
        "        print(f\"‚úÖ All columns are now numeric!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy54QDAyfHoH",
        "outputId": "702abf1c-1c44-4cb2-d49d-b65aff9e86d9"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß FIXING DATA TYPES FOR XGBOOST\n",
            "üìä Checking data types...\n",
            "Training data types:\n",
            "float64    5\n",
            "int64      5\n",
            "int32      4\n",
            "bool       1\n",
            "object     1\n",
            "UInt32     1\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Validation data types:\n",
            "float64    5\n",
            "int64      5\n",
            "int32      4\n",
            "bool       1\n",
            "object     1\n",
            "UInt32     1\n",
            "Name: count, dtype: int64\n",
            "   Found object column: Type\n",
            "üîß One-hot encoding 1 categorical columns...\n",
            "   Encoding Type...\n",
            "     Created 3 dummy variables\n",
            "üîß Converting remaining columns to numeric...\n",
            "   Converting Week from UInt32\n",
            "   Converted 4 boolean columns to int\n",
            "\n",
            "‚úÖ Final data type check:\n",
            "   Training data types: {dtype('int64'): 9, dtype('float64'): 5, dtype('int32'): 4, UInt32Dtype(): 1}\n",
            "   Validation data types: {dtype('int64'): 9, dtype('float64'): 5, dtype('int32'): 4, UInt32Dtype(): 1}\n",
            "   Training shape: (320294, 19)\n",
            "   Validation shape: (85809, 19)\n",
            "‚úÖ All columns are now numeric!\n",
            "üèÉ View run Data_Type_Fix_Before_XGBoost at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/d0bd6313e619422985bbc3b5d4faa273\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 7: XGBOOST TRAINING (AFTER DATA TYPE FIX)\n",
        "# ================================================================================\n",
        "with mlflow.start_run(run_name=\"XGBoost_Training_PROPERLY_Fixed\") as run:\n",
        "\n",
        "    print(f\"üöÄ XGBOOST TRAINING - PROPERLY FIXED VERSION\")\n",
        "\n",
        "    # Define WMAE function\n",
        "    def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "        abs_errors = np.abs(y_true - y_pred)\n",
        "        weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "        wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "        return wmae\n",
        "\n",
        "    # XGBoost parameters (less aggressive to prevent overfitting)\n",
        "    xgb_params = {\n",
        "        'n_estimators': 500,      # Reduced from 1000\n",
        "        'max_depth': 6,           # Reduced from 8\n",
        "        'learning_rate': 0.05,    # Reduced from 0.1\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'reg_alpha': 1.0,         # Increased regularization\n",
        "        'reg_lambda': 2.0,        # Increased regularization\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'objective': 'reg:squarederror'\n",
        "    }\n",
        "\n",
        "    # Train model\n",
        "    print(\"ü§ñ Training XGBoost model...\")\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"üîÆ Making predictions...\")\n",
        "    train_pred = xgb_model.predict(X_train)\n",
        "    val_pred = xgb_model.predict(X_val)\n",
        "\n",
        "    # Calculate metrics\n",
        "    print(\"üìä Calculating metrics...\")\n",
        "\n",
        "    # Training metrics\n",
        "    train_mae = mean_absolute_error(y_train, train_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    train_wmae = calculate_wmae(y_train, train_pred, train_is_holiday)\n",
        "\n",
        "    # Validation metrics\n",
        "    val_mae = mean_absolute_error(y_val, val_pred)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "    val_wmae = calculate_wmae(y_val, val_pred, val_is_holiday)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üéØ PROPERLY FIXED EXPERIMENT 4 RESULTS\")\n",
        "    print(f\"=\"*80)\n",
        "\n",
        "    print(f\"\\nüöÇ Training Metrics:\")\n",
        "    print(f\"   WMAE: ${train_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${train_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${train_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {train_r2:.4f}\")\n",
        "\n",
        "    print(f\"\\nüîÆ Validation Metrics:\")\n",
        "    print(f\"   WMAE: ${val_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${val_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${val_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {val_r2:.4f}\")\n",
        "\n",
        "    # Overfitting check\n",
        "    wmae_diff = abs(train_wmae - val_wmae) / val_wmae * 100\n",
        "    print(f\"\\nüìà Overfitting Analysis:\")\n",
        "    print(f\"   WMAE difference: {wmae_diff:.1f}%\")\n",
        "    if wmae_diff > 50:\n",
        "        print(f\"   ‚ö†Ô∏è Significant overfitting detected!\")\n",
        "    elif wmae_diff > 20:\n",
        "        print(f\"   ‚ö†Ô∏è Moderate overfitting detected\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ Reasonable generalization\")\n",
        "\n",
        "    # Feature importance\n",
        "    print(f\"\\nüéØ Top 10 Feature Importance:\")\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': xgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
        "        print(f\"   {i+1:2d}. {row['feature']:25s}: {row['importance']:.4f}\")\n",
        "\n",
        "    # Log metrics to MLflow\n",
        "    mlflow.log_metric(\"train_wmae\", train_wmae)\n",
        "    mlflow.log_metric(\"val_wmae\", val_wmae)\n",
        "    mlflow.log_metric(\"train_mae\", train_mae)\n",
        "    mlflow.log_metric(\"val_mae\", val_mae)\n",
        "    mlflow.log_metric(\"train_r2\", train_r2)\n",
        "    mlflow.log_metric(\"val_r2\", val_r2)\n",
        "    mlflow.log_metric(\"overfitting_percentage\", wmae_diff)\n",
        "\n",
        "    for param, value in xgb_params.items():\n",
        "        mlflow.log_param(f\"xgb_{param}\", value)\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üéâ EXPERIMENT 4 PROPERLY COMPLETED!\")\n",
        "    print(f\"‚úÖ Data Leakage: ELIMINATED\")\n",
        "    print(f\"üéØ Validation WMAE: ${val_wmae:,.2f}\")\n",
        "    print(f\"üìä This should be more realistic (~$2000-3000 range)\")\n",
        "    print(f\"=\"*80)"
      ],
      "metadata": {
        "id": "NKSH7gAX309d",
        "outputId": "9d810eaa-3133-4513-9c90-8c0af11b9b81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ XGBOOST TRAINING - PROPERLY FIXED VERSION\n",
            "ü§ñ Training XGBoost model...\n",
            "üîÆ Making predictions...\n",
            "üìä Calculating metrics...\n",
            "\n",
            "================================================================================\n",
            "üéØ PROPERLY FIXED EXPERIMENT 4 RESULTS\n",
            "================================================================================\n",
            "\n",
            "üöÇ Training Metrics:\n",
            "   WMAE: $1,338.89\n",
            "   MAE: $1,267.53\n",
            "   RMSE: $2,495.64\n",
            "   R¬≤: 0.9868\n",
            "\n",
            "üîÆ Validation Metrics:\n",
            "   WMAE: $2,519.10\n",
            "   MAE: $2,544.79\n",
            "   RMSE: $5,669.32\n",
            "   R¬≤: 0.9333\n",
            "\n",
            "üìà Overfitting Analysis:\n",
            "   WMAE difference: 46.9%\n",
            "   ‚ö†Ô∏è Moderate overfitting detected\n",
            "\n",
            "üéØ Top 10 Feature Importance:\n",
            "    1. Weekly_Sales_lag_1       : 0.6347\n",
            "    2. Weekly_Sales_lag_2       : 0.2868\n",
            "    3. Weekly_Sales_lag_4       : 0.0345\n",
            "    4. Month                    : 0.0072\n",
            "    5. DaysFromStart            : 0.0056\n",
            "    6. Dept                     : 0.0054\n",
            "    7. Type_A                   : 0.0048\n",
            "    8. Type_B                   : 0.0034\n",
            "    9. Size                     : 0.0028\n",
            "   10. Week                     : 0.0027\n",
            "\n",
            "================================================================================\n",
            "üéâ EXPERIMENT 4 PROPERLY COMPLETED!\n",
            "‚úÖ Data Leakage: ELIMINATED\n",
            "üéØ Validation WMAE: $2,519.10\n",
            "üìä This should be more realistic (~$2000-3000 range)\n",
            "================================================================================\n",
            "üèÉ View run XGBoost_Training_PROPERLY_Fixed at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/a08238038ffb4bd79589a4525c7b8b4c\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}