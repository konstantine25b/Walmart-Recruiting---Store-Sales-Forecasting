{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# experiment_4_k"
      ],
      "metadata": {
        "id": "R3hfw6V-alWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMBxqiRyaKmC",
        "outputId": "148f9d7f-c67b-4b74-ff7e-e33e63aa0e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "cLdXVjrean6a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "Gg3HtWQbao_8"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "2XfVrrCwaqXN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP7DAfGWarmW",
        "outputId": "65eb0980-81a4-465b-b4da-b602328b43ac"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 618MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G9rkhMhdas2Y",
        "outputId": "67f098c2-c18a-4cd9-c631-866626ef9418"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# EXPERIMENT 4: FIXING DATA LEAKAGE - SPLIT FIRST, THEN PREPROCESS\n",
        "# ================================================================================\n",
        "\n",
        "# Step 1: Setup and MLflow/DagsHub Configuration\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Install required packages\n",
        "!pip install prophet plotly mlflow dagshub xgboost -q\n",
        "\n",
        "# Setup MLflow and DagsHub\n",
        "import mlflow\n",
        "import dagshub\n",
        "\n",
        "# DagsHub setup\n",
        "dagshub.init(repo_owner='konstantine25b',\n",
        "             repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "             mlflow=True)\n",
        "\n",
        "# Set tracking URI\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "mlflow.set_experiment(\"Experiment_4_Fixed_Data_Leakage\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "itxEe9N7auGr",
        "outputId": "35fcab15-33dc-46d1-b141-5a5ac451d47c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/43ce0a29767b4be4b47a7e6d431382c2', creation_time=1750844432684, experiment_id='3', last_update_time=1750844432684, lifecycle_stage='active', name='Experiment_4_Fixed_Data_Leakage', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load and explore data\n",
        "with mlflow.start_run(run_name=\"Data_Loading_Experiment_4\") as run:\n",
        "\n",
        "    print(f\"üìÅ Starting data loading: {run.info.run_id}\")\n",
        "\n",
        "    # Load datasets\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "    train = pd.read_csv('train.csv')\n",
        "    stores = pd.read_csv('stores.csv')\n",
        "\n",
        "    # Convert Date column\n",
        "    train['Date'] = pd.to_datetime(train['Date'])\n",
        "\n",
        "    # Log basic info\n",
        "    mlflow.log_param(\"train_shape\", f\"{train.shape[0]}x{train.shape[1]}\")\n",
        "    mlflow.log_param(\"date_range\", f\"{train['Date'].min()} to {train['Date'].max()}\")\n",
        "    mlflow.log_param(\"stores_count\", train['Store'].nunique())\n",
        "    mlflow.log_param(\"departments_count\", train['Dept'].nunique())\n",
        "\n",
        "    # Merge with stores data\n",
        "    train_merged = train.merge(stores, on='Store', how='left')\n",
        "\n",
        "    print(f\"‚úÖ Data loaded successfully!\")\n",
        "    print(f\"üìä Training data shape: {train.shape}\")\n",
        "    print(f\"üè™ Stores data shape: {stores.shape}\")\n",
        "    print(f\"üìä Merged data shape: {train_merged.shape}\")\n",
        "    print(f\"üìÖ Date range: {train['Date'].min()} to {train['Date'].max()}\")\n",
        "    print(f\"üè¨ Stores: {train['Store'].nunique()}, Departments: {train['Dept'].nunique()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzp6nJcpdVgk",
        "outputId": "f811bccb-6295-44b1-e641-aabce9e91be0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Starting data loading: a2ab45443a7e43ccb95a5028edc7ebba\n",
            "‚úÖ Data loaded successfully!\n",
            "üìä Training data shape: (421570, 5)\n",
            "üè™ Stores data shape: (45, 3)\n",
            "üìä Merged data shape: (421570, 7)\n",
            "üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üè¨ Stores: 45, Departments: 81\n",
            "üèÉ View run Data_Loading_Experiment_4 at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/a2ab45443a7e43ccb95a5028edc7ebba\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 3: CRITICAL FIX - TEMPORAL SPLIT FIRST (BEFORE PREPROCESSING)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Temporal_Split_BEFORE_Preprocessing\") as run:\n",
        "\n",
        "    print(f\"‚ö†Ô∏è FIXING DATA LEAKAGE: Temporal split BEFORE preprocessing\")\n",
        "    print(f\"üîÑ Starting temporal split run: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"split_method\", \"temporal_80_20_before_preprocessing\")\n",
        "    mlflow.log_param(\"fix_applied\", \"split_before_outlier_detection\")\n",
        "\n",
        "    # Use merged data (only basic merge, no feature engineering yet)\n",
        "    data_for_split = train_merged.copy()\n",
        "\n",
        "    # Sort by date to ensure proper temporal order\n",
        "    data_for_split = data_for_split.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "    # Calculate split point (80% for training)\n",
        "    min_date = data_for_split['Date'].min()\n",
        "    max_date = data_for_split['Date'].max()\n",
        "    total_days = (max_date - min_date).days\n",
        "    split_days = int(total_days * 0.8)\n",
        "    split_date = min_date + timedelta(days=split_days)\n",
        "\n",
        "    # Ensure split_date falls on a week boundary (since data is weekly)\n",
        "    # Find the closest Friday (assuming data is weekly ending on Friday)\n",
        "    while split_date.weekday() != 4:  # 4 = Friday\n",
        "        split_date += timedelta(days=1)\n",
        "\n",
        "    # Create temporal split\n",
        "    train_raw = data_for_split[data_for_split['Date'] < split_date].copy()\n",
        "    val_raw = data_for_split[data_for_split['Date'] >= split_date].copy()\n",
        "\n",
        "    # Log split information\n",
        "    mlflow.log_param(\"split_date\", split_date.strftime(\"%Y-%m-%d\"))\n",
        "    mlflow.log_param(\"train_records\", len(train_raw))\n",
        "    mlflow.log_param(\"val_records\", len(val_raw))\n",
        "    mlflow.log_param(\"train_date_range\", f\"{train_raw['Date'].min()} to {train_raw['Date'].max()}\")\n",
        "    mlflow.log_param(\"val_date_range\", f\"{val_raw['Date'].min()} to {val_raw['Date'].max()}\")\n",
        "\n",
        "    # Verify split quality\n",
        "    train_stores = train_raw['Store'].nunique()\n",
        "    val_stores = val_raw['Store'].nunique()\n",
        "    train_depts = train_raw['Dept'].nunique()\n",
        "    val_depts = val_raw['Dept'].nunique()\n",
        "\n",
        "    print(f\"‚úÖ Temporal split completed BEFORE preprocessing!\")\n",
        "    print(f\"üìÖ Split date: {split_date}\")\n",
        "    print(f\"üöÇ Training: {len(train_raw):,} records ({len(train_raw)/len(data_for_split)*100:.1f}%)\")\n",
        "    print(f\"üîÆ Validation: {len(val_raw):,} records ({len(val_raw)/len(data_for_split)*100:.1f}%)\")\n",
        "    print(f\"üè™ Store coverage: Train={train_stores}, Val={val_stores}\")\n",
        "    print(f\"üè¨ Dept coverage: Train={train_depts}, Val={val_depts}\")\n",
        "\n",
        "    # Holiday distribution check\n",
        "    train_holidays = train_raw['IsHoliday'].sum()\n",
        "    val_holidays = val_raw['IsHoliday'].sum()\n",
        "    print(f\"üéÑ Holiday weeks: Train={train_holidays} ({train_holidays/len(train_raw)*100:.1f}%), Val={val_holidays} ({val_holidays/len(val_raw)*100:.1f}%)\")\n",
        "\n",
        "    mlflow.log_metric(\"train_holiday_percentage\", train_holidays/len(train_raw)*100)\n",
        "    mlflow.log_metric(\"val_holiday_percentage\", val_holidays/len(val_raw)*100)\n",
        "\n",
        "# ================================================================================\n",
        "# STEP 3: TEMPORAL SPLIT (AND NOW OUTLIER REMOVAL)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Temporal_Split_And_Outlier_Removal\") as run:\n",
        "\n",
        "    print(f\"üî™ Starting temporal split: {run.info.run_id}\")\n",
        "\n",
        "    # Define the cutoff date for the validation set\n",
        "    cutoff_date = train_merged['Date'].max() - timedelta(weeks=40)\n",
        "    mlflow.log_param(\"validation_cutoff_date\", cutoff_date.strftime('%Y-%m-%d'))\n",
        "    mlflow.log_param(\"validation_duration_weeks\", 40)\n",
        "\n",
        "    # Split the data into training and validation sets\n",
        "    train_raw = train_merged[train_merged['Date'] <= cutoff_date].copy()\n",
        "    val_raw = train_merged[train_merged['Date'] > cutoff_date].copy()\n",
        "\n",
        "    # ========================================================================\n",
        "    # ‚úÖ NEW: OUTLIER DETECTION (APPLIED TO TRAINING DATA ONLY)\n",
        "    # This code is now in the correct place to prevent data leakage.\n",
        "    # ========================================================================\n",
        "    print(\"\\nüîç Starting outlier detection on the TRAINING set only...\")\n",
        "\n",
        "    # Calculate thresholds ONLY on the training data\n",
        "    sales_by_store_dept_train = train_raw.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "    Q1_train = sales_by_store_dept_train.transform(lambda x: x.quantile(0.25))\n",
        "    Q3_train = sales_by_store_dept_train.transform(lambda x: x.quantile(0.75))\n",
        "    IQR_train = Q3_train - Q1_train\n",
        "\n",
        "    lower_bound_train = Q1_train - 1.5 * IQR_train\n",
        "    upper_bound_train = Q3_train + 1.5 * IQR_train\n",
        "\n",
        "    # Find outliers in the training set\n",
        "    train_outliers = (train_raw['Weekly_Sales'] < lower_bound_train) | (train_raw['Weekly_Sales'] > upper_bound_train)\n",
        "\n",
        "    # Log and remove outliers from the training set\n",
        "    num_outliers_train = train_outliers.sum()\n",
        "    print(f\"   Found {num_outliers_train} outliers in the training set ({num_outliers_train / len(train_raw):.2%})\")\n",
        "    mlflow.log_metric(\"outliers_found_in_train\", num_outliers_train)\n",
        "\n",
        "    # Remove outliers from train_raw\n",
        "    train_raw = train_raw[~train_outliers]\n",
        "    print(f\"   Removed outliers. New training set shape: {train_raw.shape}\")\n",
        "    mlflow.log_metric(\"train_records_after_outlier_removal\", len(train_raw))\n",
        "    # ========================================================================\n",
        "\n",
        "    print(f\"\\n‚úÖ Temporal split and outlier removal completed!\")\n",
        "    print(f\"   Training data shape: {train_raw.shape}\")\n",
        "    print(f\"   Validation data shape: {val_raw.shape}\")\n",
        "    print(f\"   Training date range: {train_raw['Date'].min()} to {train_raw['Date'].max()}\")\n",
        "    print(f\"   Validation date range: {val_raw['Date'].min()} to {val_raw['Date'].max()}\")\n",
        "\n",
        "    mlflow.log_metric(\"train_records_after_split\", len(train_raw))\n",
        "    mlflow.log_metric(\"val_records_after_split\", len(val_raw))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53e5HuBRdjFk",
        "outputId": "45721ead-e534-4110-ae5f-a557bf6f9958"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö†Ô∏è FIXING DATA LEAKAGE: Temporal split BEFORE preprocessing\n",
            "üîÑ Starting temporal split run: 3a3f85f7bb0e41619cb94e59100c6c3c\n",
            "‚úÖ Temporal split completed BEFORE preprocessing!\n",
            "üìÖ Split date: 2012-04-13 00:00:00\n",
            "üöÇ Training: 335,761 records (79.6%)\n",
            "üîÆ Validation: 85,809 records (20.4%)\n",
            "üè™ Store coverage: Train=45, Val=45\n",
            "üè¨ Dept coverage: Train=81, Val=81\n",
            "üéÑ Holiday weeks: Train=26695 (8.0%), Val=2966 (3.5%)\n",
            "üèÉ View run Temporal_Split_BEFORE_Preprocessing at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/3a3f85f7bb0e41619cb94e59100c6c3c\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n",
            "üî™ Starting temporal split: 04640c568cd6465da7788c8b154a66b4\n",
            "\n",
            "üîç Starting outlier detection on the TRAINING set only...\n",
            "   Found 14796 outliers in the training set (4.88%)\n",
            "   Removed outliers. New training set shape: (288234, 7)\n",
            "\n",
            "‚úÖ Temporal split and outlier removal completed!\n",
            "   Training data shape: (288234, 7)\n",
            "   Validation data shape: (118540, 7)\n",
            "   Training date range: 2010-02-05 00:00:00 to 2012-01-20 00:00:00\n",
            "   Validation date range: 2012-01-27 00:00:00 to 2012-10-26 00:00:00\n",
            "üèÉ View run Temporal_Split_And_Outlier_Removal at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/04640c568cd6465da7788c8b154a66b4\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 4: FEATURE ENGINEERING (APPLIED TO TRAIN AND VAL SEPARATELY)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Feature_Engineering_After_Split\") as run:\n",
        "\n",
        "    print(f\"üîß Starting feature engineering AFTER split: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"feature_engineering_method\", \"applied_after_temporal_split\")\n",
        "    mlflow.log_param(\"train_records_input\", len(train_raw))\n",
        "    mlflow.log_param(\"val_records_input\", len(val_raw))\n",
        "\n",
        "    def create_comprehensive_date_features(df):\n",
        "        \"\"\"Create comprehensive date features\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Basic date features\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['Week'] = df['Date'].dt.isocalendar().week\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "\n",
        "        # Cyclical features\n",
        "        df['Month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['Month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "        df['Week_sin'] = np.sin(2 * np.pi * df['Week'] / 52)\n",
        "        df['Week_cos'] = np.cos(2 * np.pi * df['Week'] / 52)\n",
        "        df['DayOfWeek_sin'] = np.sin(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "        df['DayOfWeek_cos'] = np.cos(2 * np.pi * df['DayOfWeek'] / 7)\n",
        "\n",
        "        # Time since reference\n",
        "        reference_date = pd.Timestamp('2010-02-05')\n",
        "        df['DaysFromStart'] = (df['Date'] - reference_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        df['MonthsFromStart'] = ((df['Date'].dt.year - reference_date.year) * 12 +\n",
        "                                df['Date'].dt.month - reference_date.month)\n",
        "\n",
        "        # Holiday features\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "\n",
        "        # Retail calendar features\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        df['IsSummerSeason'] = df['Month'].isin([6, 7, 8]).astype(int)\n",
        "        df['IsSpringSeaso'] = df['Month'].isin([3, 4, 5]).astype(int)\n",
        "\n",
        "        # Week patterns\n",
        "        df['IsFirstWeekOfMonth'] = (df['Day'] <= 7).astype(int)\n",
        "        df['IsLastWeekOfMonth'] = (df['Date'].dt.days_in_month - df['Day'] < 7).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "    def create_lagging_features_time_aware(df, target_col='Weekly_Sales'):\n",
        "        \"\"\"\n",
        "        Create time-aware lagging features that prevent data leakage\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        # ONLY simple lag features\n",
        "        lags = [1, 2, 4, 8, 12]\n",
        "        for lag in lags:\n",
        "            lag_col = f'{target_col}_lag_{lag}'\n",
        "            df[lag_col] = df.groupby(['Store', 'Dept'])[target_col].shift(lag).fillna(0)\n",
        "\n",
        "        # ONLY basic difference features\n",
        "        df[f'{target_col}_diff_1'] = df.groupby(['Store', 'Dept'])[target_col].diff(1).fillna(0)\n",
        "        df[f'{target_col}_diff_4'] = df.groupby(['Store', 'Dept'])[target_col].diff(4).fillna(0)\n",
        "\n",
        "        return df\n",
        "\n",
        "    # Apply feature engineering to training set\n",
        "    print(\"üöÇ Creating features for training set...\")\n",
        "    train_with_features = create_comprehensive_date_features(train_raw)\n",
        "    train_final = create_lagging_features_time_aware(train_with_features)\n",
        "\n",
        "    # Apply feature engineering to validation set\n",
        "    print(\"üîÆ Creating features for validation set...\")\n",
        "    val_with_features = create_comprehensive_date_features(val_raw)\n",
        "    val_final = create_lagging_features_time_aware(val_with_features)\n",
        "\n",
        "    # Ensure both datasets have the same columns\n",
        "    train_cols = set(train_final.columns)\n",
        "    val_cols = set(val_final.columns)\n",
        "    common_cols = list(train_cols.intersection(val_cols))\n",
        "\n",
        "    # Keep only common columns in the same order\n",
        "    train_final = train_final[common_cols]\n",
        "    val_final = val_final[common_cols]\n",
        "\n",
        "    # Check for any remaining NaNs\n",
        "    train_nans = train_final.isnull().sum().sum()\n",
        "    val_nans = val_final.isnull().sum().sum()\n",
        "\n",
        "    if train_nans > 0 or val_nans > 0:\n",
        "        print(f\"‚ö†Ô∏è Warning: NaNs found - Train: {train_nans}, Val: {val_nans}\")\n",
        "        # Fill any remaining NaNs with 0\n",
        "        train_final = train_final.fillna(0)\n",
        "        val_final = val_final.fillna(0)\n",
        "        print(f\"‚úÖ NaNs filled with 0\")\n",
        "\n",
        "    print(f\"‚úÖ Feature engineering completed!\")\n",
        "    print(f\"üöÇ Train shape: {train_final.shape}\")\n",
        "    print(f\"üîÆ Val shape: {val_final.shape}\")\n",
        "    print(f\"üìä Features created: {len(train_final.columns)}\")\n",
        "    print(f\"üîß NaNs in training: {train_final.isnull().sum().sum()}\")\n",
        "    print(f\"üîß NaNs in validation: {val_final.isnull().sum().sum()}\")\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"train_features_shape\", len(train_final.columns))\n",
        "    mlflow.log_metric(\"train_records_after_features\", len(train_final))\n",
        "    mlflow.log_metric(\"val_records_after_features\", len(val_final))\n",
        "    mlflow.log_metric(\"train_nans_final\", train_final.isnull().sum().sum())\n",
        "    mlflow.log_metric(\"val_nans_final\", val_final.isnull().sum().sum())\n",
        "\n",
        "    # Show feature categories\n",
        "    feature_categories = {\n",
        "        'Date Features': [col for col in train_final.columns if any(x in col for x in ['Year', 'Month', 'Day', 'Week', 'Quarter', 'sin', 'cos'])],\n",
        "        'Holiday Features': [col for col in train_final.columns if any(x in col for x in ['Holiday', 'SuperBowl', 'Labor', 'Thanksgiving', 'Christmas'])],\n",
        "        'Lag Features': [col for col in train_final.columns if 'lag_' in col],\n",
        "        'Rolling Features': [col for col in train_final.columns if 'rolling_' in col],\n",
        "        'EWM Features': [col for col in train_final.columns if 'ewm_' in col],\n",
        "        'Diff Features': [col for col in train_final.columns if 'diff' in col]\n",
        "    }\n",
        "\n",
        "    print(f\"\\nüìã Feature Categories:\")\n",
        "    for category, features in feature_categories.items():\n",
        "        count = len(features)\n",
        "        print(f\"   {category}: {count} features\")\n",
        "        mlflow.log_metric(f\"{category.lower().replace(' ', '_')}_count\", count)\n",
        "        if count > 0 and count <= 3:\n",
        "            print(f\"     Examples: {features}\")\n",
        "        elif count > 3:\n",
        "            print(f\"     Examples: {features[:3]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8sy7qqAd16E",
        "outputId": "8b61fc3b-ba3b-4fa0-baa1-da67022dd548"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîß Starting feature engineering AFTER split: 8b4bae263a114247bc992331da4946b7\n",
            "üöÇ Creating features for training set...\n",
            "üîÆ Creating features for validation set...\n",
            "‚úÖ Feature engineering completed!\n",
            "üöÇ Train shape: (288234, 40)\n",
            "üîÆ Val shape: (118540, 40)\n",
            "üìä Features created: 40\n",
            "üîß NaNs in training: 0\n",
            "üîß NaNs in validation: 0\n",
            "\n",
            "üìã Feature Categories:\n",
            "   Date Features: 30 features\n",
            "     Examples: ['Weekly_Sales_lag_8', 'IsChristmasWeek', 'Weekly_Sales_lag_4']\n",
            "   Holiday Features: 7 features\n",
            "     Examples: ['IsChristmasWeek', 'IsLaborDayWeek', 'IsHoliday']\n",
            "   Lag Features: 5 features\n",
            "     Examples: ['Weekly_Sales_lag_8', 'Weekly_Sales_lag_4', 'Weekly_Sales_lag_12']\n",
            "   Rolling Features: 0 features\n",
            "   EWM Features: 0 features\n",
            "   Diff Features: 2 features\n",
            "     Examples: ['Weekly_Sales_diff_4', 'Weekly_Sales_diff_1']\n",
            "üèÉ View run Feature_Engineering_After_Split at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/8b4bae263a114247bc992331da4946b7\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 5: DATA TYPE CHECK AND CATEGORICAL ENCODING\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"Data_Type_Check_And_Encoding\") as run:\n",
        "\n",
        "    print(f\"üîç Checking data types and encoding categoricals: {run.info.run_id}\")\n",
        "\n",
        "    mlflow.log_param(\"encoding_method\", \"one_hot_encoding_for_categoricals\")\n",
        "\n",
        "    # Check data types in both datasets\n",
        "    print(f\"üìä Checking data types...\")\n",
        "\n",
        "    train_dtypes = train_final.dtypes.value_counts()\n",
        "    val_dtypes = val_final.dtypes.value_counts()\n",
        "\n",
        "    print(f\"üöÇ Training data types: {dict(train_dtypes)}\")\n",
        "    print(f\"üîÆ Validation data types: {dict(val_dtypes)}\")\n",
        "\n",
        "    # Identify categorical columns (object, category, or specific known categoricals)\n",
        "    categorical_columns = []\n",
        "\n",
        "    # Check for object/category columns\n",
        "    for col in train_final.columns:\n",
        "        if train_final[col].dtype == 'object' or train_final[col].dtype.name == 'category':\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    # Add known categorical columns that might be numeric but should be treated as categorical\n",
        "    known_categoricals = ['Store', 'Dept']  # These should be categorical\n",
        "    for col in known_categoricals:\n",
        "        if col in train_final.columns and col not in categorical_columns:\n",
        "            categorical_columns.append(col)\n",
        "\n",
        "    print(f\"üè∑Ô∏è Categorical columns found: {categorical_columns}\")\n",
        "\n",
        "    if len(categorical_columns) > 0:\n",
        "        print(f\"üîß Applying one-hot encoding to {len(categorical_columns)} categorical columns...\")\n",
        "\n",
        "        # Apply one-hot encoding\n",
        "        from sklearn.preprocessing import OneHotEncoder\n",
        "        import pandas as pd\n",
        "\n",
        "        # Create copies for encoding\n",
        "        train_encoded = train_final.copy()\n",
        "        val_encoded = val_final.copy()\n",
        "\n",
        "        for col in categorical_columns:\n",
        "            print(f\"   Encoding {col}...\")\n",
        "\n",
        "            # Check unique values\n",
        "            train_unique = train_encoded[col].nunique()\n",
        "            val_unique = val_encoded[col].nunique()\n",
        "            combined_unique = pd.concat([train_encoded[col], val_encoded[col]]).nunique()\n",
        "\n",
        "            print(f\"     Train unique: {train_unique}, Val unique: {val_unique}, Combined: {combined_unique}\")\n",
        "\n",
        "            # Use pandas get_dummies for one-hot encoding\n",
        "            # Fit on combined data to ensure same columns\n",
        "            combined_data = pd.concat([\n",
        "                train_encoded[col].reset_index(drop=True),\n",
        "                val_encoded[col].reset_index(drop=True)\n",
        "            ])\n",
        "\n",
        "            # Get dummies\n",
        "            dummies = pd.get_dummies(combined_data, prefix=f'{col}', dummy_na=False)\n",
        "\n",
        "            # Split back into train and val\n",
        "            train_dummies = dummies.iloc[:len(train_encoded)]\n",
        "            val_dummies = dummies.iloc[len(train_encoded):]\n",
        "\n",
        "            # Reset indices\n",
        "            train_dummies.index = train_encoded.index\n",
        "            val_dummies.index = val_encoded.index\n",
        "\n",
        "            # Add dummy columns to datasets\n",
        "            train_encoded = pd.concat([train_encoded, train_dummies], axis=1)\n",
        "            val_encoded = pd.concat([val_encoded, val_dummies], axis=1)\n",
        "\n",
        "            # Remove original categorical column\n",
        "            train_encoded = train_encoded.drop(columns=[col])\n",
        "            val_encoded = val_encoded.drop(columns=[col])\n",
        "\n",
        "            print(f\"     Created {len(dummies.columns)} dummy variables\")\n",
        "            mlflow.log_metric(f\"{col}_dummy_count\", len(dummies.columns))\n",
        "\n",
        "        # Update the final datasets\n",
        "        train_final_encoded = train_encoded\n",
        "        val_final_encoded = val_encoded\n",
        "\n",
        "        print(f\"‚úÖ One-hot encoding completed!\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚úÖ No categorical columns found - using original datasets\")\n",
        "        train_final_encoded = train_final.copy()\n",
        "        val_final_encoded = val_final.copy()\n",
        "\n",
        "    # Final data type check and conversion\n",
        "    print(f\"üîß Converting all features to numeric types...\")\n",
        "\n",
        "    # Convert boolean to int\n",
        "    bool_columns = []\n",
        "    for col in train_final_encoded.columns:\n",
        "        if train_final_encoded[col].dtype == 'bool':\n",
        "            bool_columns.append(col)\n",
        "            train_final_encoded[col] = train_final_encoded[col].astype(int)\n",
        "            val_final_encoded[col] = val_final_encoded[col].astype(int)\n",
        "\n",
        "    if bool_columns:\n",
        "        print(f\"   Converted {len(bool_columns)} boolean columns to int\")\n",
        "\n",
        "    # Convert any remaining object columns to numeric\n",
        "    object_columns = []\n",
        "    for col in train_final_encoded.columns:\n",
        "        if train_final_encoded[col].dtype == 'object':\n",
        "            object_columns.append(col)\n",
        "            train_final_encoded[col] = pd.to_numeric(train_final_encoded[col], errors='coerce')\n",
        "            val_final_encoded[col] = pd.to_numeric(val_final_encoded[col], errors='coerce')\n",
        "\n",
        "    if object_columns:\n",
        "        print(f\"   Converted {len(object_columns)} object columns to numeric\")\n",
        "\n",
        "    # Fill any NaNs introduced during conversion\n",
        "    train_nans_after = train_final_encoded.isnull().sum().sum()\n",
        "    val_nans_after = val_final_encoded.isnull().sum().sum()\n",
        "\n",
        "    if train_nans_after > 0 or val_nans_after > 0:\n",
        "        print(f\"‚ö†Ô∏è NaNs introduced during conversion - Train: {train_nans_after}, Val: {val_nans_after}\")\n",
        "        train_final_encoded = train_final_encoded.fillna(0)\n",
        "        val_final_encoded = val_final_encoded.fillna(0)\n",
        "        print(f\"‚úÖ NaNs filled with 0\")\n",
        "\n",
        "    # Final data type summary\n",
        "    final_train_dtypes = train_final_encoded.dtypes.value_counts()\n",
        "    final_val_dtypes = val_final_encoded.dtypes.value_counts()\n",
        "\n",
        "    print(f\"\\nüìä Final Data Types:\")\n",
        "    print(f\"üöÇ Training: {dict(final_train_dtypes)}\")\n",
        "    print(f\"üîÆ Validation: {dict(final_val_dtypes)}\")\n",
        "\n",
        "    print(f\"\\nüìä Final Dataset Shapes:\")\n",
        "    print(f\"üöÇ Training: {train_final_encoded.shape}\")\n",
        "    print(f\"üîÆ Validation: {val_final_encoded.shape}\")\n",
        "\n",
        "    # Verify all columns are numeric\n",
        "    non_numeric_train = [col for col in train_final_encoded.columns\n",
        "                        if train_final_encoded[col].dtype not in ['int64', 'float64', 'int32', 'float32']]\n",
        "    non_numeric_val = [col for col in val_final_encoded.columns\n",
        "                      if val_final_encoded[col].dtype not in ['int64', 'float64', 'int32', 'float32']]\n",
        "\n",
        "    if non_numeric_train or non_numeric_val:\n",
        "        print(f\"‚ö†Ô∏è Warning: Non-numeric columns still exist:\")\n",
        "        print(f\"   Train: {non_numeric_train}\")\n",
        "        print(f\"   Val: {non_numeric_val}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ All columns are numeric!\")\n",
        "\n",
        "    # Log final metrics\n",
        "    mlflow.log_metric(\"final_train_shape_rows\", train_final_encoded.shape[0])\n",
        "    mlflow.log_metric(\"final_train_shape_cols\", train_final_encoded.shape[1])\n",
        "    mlflow.log_metric(\"final_val_shape_rows\", val_final_encoded.shape[0])\n",
        "    mlflow.log_metric(\"final_val_shape_cols\", val_final_encoded.shape[1])\n",
        "    mlflow.log_metric(\"categorical_columns_encoded\", len(categorical_columns))\n",
        "\n",
        "    # Update variable names for next steps\n",
        "    train_final = train_final_encoded\n",
        "    val_final = val_final_encoded\n",
        "\n",
        "    print(f\"üéØ Data is ready for outlier detection!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "369LUxxJeZ3m",
        "outputId": "969205ef-1b05-4b37-ec32-e3db37a95020"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîç Checking data types and encoding categoricals: d0acc360432743fe87c8785e80caef88\n",
            "üìä Checking data types...\n",
            "üöÇ Training data types: {dtype('int64'): np.int64(16), dtype('float64'): np.int64(12), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('bool'): np.int64(1), dtype('<M8[ns]'): np.int64(1), dtype('O'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üîÆ Validation data types: {dtype('int64'): np.int64(16), dtype('float64'): np.int64(12), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('bool'): np.int64(1), dtype('<M8[ns]'): np.int64(1), dtype('O'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üè∑Ô∏è Categorical columns found: ['Type', 'Store', 'Dept']\n",
            "üîß Applying one-hot encoding to 3 categorical columns...\n",
            "   Encoding Type...\n",
            "     Train unique: 3, Val unique: 3, Combined: 3\n",
            "     Created 3 dummy variables\n",
            "   Encoding Store...\n",
            "     Train unique: 45, Val unique: 45, Combined: 45\n",
            "     Created 45 dummy variables\n",
            "   Encoding Dept...\n",
            "     Train unique: 81, Val unique: 81, Combined: 81\n",
            "     Created 81 dummy variables\n",
            "‚úÖ One-hot encoding completed!\n",
            "üîß Converting all features to numeric types...\n",
            "   Converted 130 boolean columns to int\n",
            "\n",
            "üìä Final Data Types:\n",
            "üöÇ Training: {dtype('int64'): np.int64(144), dtype('float64'): np.int64(12), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('<M8[ns]'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "üîÆ Validation: {dtype('int64'): np.int64(144), dtype('float64'): np.int64(12), dtype('int32'): np.int64(6), Float64Dtype(): np.int64(2), dtype('<M8[ns]'): np.int64(1), UInt32Dtype(): np.int64(1)}\n",
            "\n",
            "üìä Final Dataset Shapes:\n",
            "üöÇ Training: (288234, 166)\n",
            "üîÆ Validation: (118540, 166)\n",
            "‚ö†Ô∏è Warning: Non-numeric columns still exist:\n",
            "   Train: ['Date', 'Week', 'Week_sin', 'Week_cos']\n",
            "   Val: ['Date', 'Week', 'Week_sin', 'Week_cos']\n",
            "üéØ Data is ready for outlier detection!\n",
            "üèÉ View run Data_Type_Check_And_Encoding at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/d0acc360432743fe87c8785e80caef88\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================================\n",
        "# STEP 7: XGBOOST TRAINING (FIXED VERSION)\n",
        "# ================================================================================\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Training_Fixed_Data_Leakage\") as run:\n",
        "\n",
        "    print(f\"üöÄ Starting XGBoost training with FIXED data leakage: {run.info.run_id}\")\n",
        "\n",
        "    import xgboost as xgb\n",
        "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "    import joblib\n",
        "\n",
        "    mlflow.log_param(\"model_type\", \"xgboost_regressor\")\n",
        "    mlflow.log_param(\"evaluation_metric\", \"WMAE_weighted_mean_absolute_error\")\n",
        "    mlflow.log_param(\"data_leakage_status\", \"FIXED\")\n",
        "    mlflow.log_param(\"train_size\", len(train_final))\n",
        "    mlflow.log_param(\"val_size\", len(val_final))\n",
        "\n",
        "    # 1. Prepare features and target\n",
        "    print(\"üîß Preparing features and target variables...\")\n",
        "\n",
        "    target_column = 'Weekly_Sales'\n",
        "    exclude_columns = ['Weekly_Sales', 'Date']\n",
        "\n",
        "    # Get feature columns\n",
        "    feature_columns = [col for col in train_final.columns if col not in exclude_columns]\n",
        "\n",
        "    print(f\"   Total features available: {len(feature_columns)}\")\n",
        "\n",
        "    # Prepare training data\n",
        "    X_train = train_final[feature_columns].copy()\n",
        "    y_train = train_final[target_column].copy()\n",
        "    X_val = val_final[feature_columns].copy()\n",
        "    y_val = val_final[target_column].copy()\n",
        "\n",
        "    # Get holiday flags for WMAE calculation\n",
        "    train_is_holiday = train_final['IsHoliday'].copy()\n",
        "    val_is_holiday = val_final['IsHoliday'].copy()\n",
        "\n",
        "    print(f\"üìä Final data shapes:\")\n",
        "    print(f\"   X_train: {X_train.shape}\")\n",
        "    print(f\"   X_val: {X_val.shape}\")\n",
        "\n",
        "    # 2. Define WMAE function\n",
        "    def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "        \"\"\"Calculate Weighted Mean Absolute Error\"\"\"\n",
        "        abs_errors = np.abs(y_true - y_pred)\n",
        "        weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "        wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "        return wmae\n",
        "\n",
        "    # 3. Train XGBoost model (Fixed)\n",
        "    print(f\"ü§ñ Training XGBoost model...\")\n",
        "\n",
        "    # Model parameters\n",
        "    xgb_params = {\n",
        "        'n_estimators': 1000,\n",
        "        'max_depth': 8,\n",
        "        'learning_rate': 0.1,\n",
        "        'subsample': 0.8,\n",
        "        'colsample_bytree': 0.8,\n",
        "        'reg_alpha': 0.1,\n",
        "        'reg_lambda': 1.0,\n",
        "        'random_state': 42,\n",
        "        'n_jobs': -1,\n",
        "        'objective': 'reg:squarederror'\n",
        "    }\n",
        "\n",
        "    # Log parameters\n",
        "    for param, value in xgb_params.items():\n",
        "        mlflow.log_param(f\"xgb_{param}\", value)\n",
        "\n",
        "    # Initialize model\n",
        "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
        "\n",
        "    # Train model (simplified without eval_metric in fit)\n",
        "    print(\"   Fitting model...\")\n",
        "    xgb_model.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"‚úÖ Model training completed!\")\n",
        "\n",
        "    # 4. Make predictions\n",
        "    print(f\"üîÆ Making predictions...\")\n",
        "\n",
        "    train_pred = xgb_model.predict(X_train)\n",
        "    val_pred = xgb_model.predict(X_val)\n",
        "\n",
        "    # 5. Calculate metrics\n",
        "    print(f\"üìä Calculating metrics...\")\n",
        "\n",
        "    # Training metrics\n",
        "    train_mae = mean_absolute_error(y_train, train_pred)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))\n",
        "    train_r2 = r2_score(y_train, train_pred)\n",
        "    train_wmae = calculate_wmae(y_train, train_pred, train_is_holiday)\n",
        "\n",
        "    # Validation metrics\n",
        "    val_mae = mean_absolute_error(y_val, val_pred)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
        "    val_r2 = r2_score(y_val, val_pred)\n",
        "    val_wmae = calculate_wmae(y_val, val_pred, val_is_holiday)\n",
        "\n",
        "    # Holiday breakdown\n",
        "    train_holiday_mask = train_is_holiday == True\n",
        "    train_non_holiday_mask = train_is_holiday == False\n",
        "    val_holiday_mask = val_is_holiday == True\n",
        "    val_non_holiday_mask = val_is_holiday == False\n",
        "\n",
        "    # Holiday MAE\n",
        "    train_holiday_mae = mean_absolute_error(y_train[train_holiday_mask], train_pred[train_holiday_mask]) if train_holiday_mask.sum() > 0 else 0\n",
        "    train_non_holiday_mae = mean_absolute_error(y_train[train_non_holiday_mask], train_pred[train_non_holiday_mask]) if train_non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "    val_holiday_mae = mean_absolute_error(y_val[val_holiday_mask], val_pred[val_holiday_mask]) if val_holiday_mask.sum() > 0 else 0\n",
        "    val_non_holiday_mae = mean_absolute_error(y_val[val_non_holiday_mask], val_pred[val_non_holiday_mask]) if val_non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "    # 6. Display results\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üìä EXPERIMENT 4 RESULTS (DATA LEAKAGE FIXED)\")\n",
        "    print(f\"=\"*80)\n",
        "\n",
        "    print(f\"\\nüöÇ Training Metrics:\")\n",
        "    print(f\"   WMAE: ${train_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${train_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${train_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {train_r2:.4f}\")\n",
        "    print(f\"   Holiday MAE: ${train_holiday_mae:,.2f}\")\n",
        "    print(f\"   Non-Holiday MAE: ${train_non_holiday_mae:,.2f}\")\n",
        "\n",
        "    print(f\"\\nüîÆ Validation Metrics:\")\n",
        "    print(f\"   WMAE: ${val_wmae:,.2f}\")\n",
        "    print(f\"   MAE: ${val_mae:,.2f}\")\n",
        "    print(f\"   RMSE: ${val_rmse:,.2f}\")\n",
        "    print(f\"   R¬≤: {val_r2:.4f}\")\n",
        "    print(f\"   Holiday MAE: ${val_holiday_mae:,.2f}\")\n",
        "    print(f\"   Non-Holiday MAE: ${val_non_holiday_mae:,.2f}\")\n",
        "\n",
        "    # 7. Log metrics to MLflow\n",
        "    metrics_to_log = {\n",
        "        \"train_wmae\": train_wmae,\n",
        "        \"train_mae\": train_mae,\n",
        "        \"train_rmse\": train_rmse,\n",
        "        \"train_r2\": train_r2,\n",
        "        \"train_holiday_mae\": train_holiday_mae,\n",
        "        \"train_non_holiday_mae\": train_non_holiday_mae,\n",
        "        \"val_wmae\": val_wmae,\n",
        "        \"val_mae\": val_mae,\n",
        "        \"val_rmse\": val_rmse,\n",
        "        \"val_r2\": val_r2,\n",
        "        \"val_holiday_mae\": val_holiday_mae,\n",
        "        \"val_non_holiday_mae\": val_non_holiday_mae,\n",
        "        \"total_features\": len(feature_columns),\n",
        "        \"train_holiday_count\": train_holiday_mask.sum(),\n",
        "        \"val_holiday_count\": val_holiday_mask.sum()\n",
        "    }\n",
        "\n",
        "    for metric_name, value in metrics_to_log.items():\n",
        "        mlflow.log_metric(metric_name, value)\n",
        "\n",
        "    # 8. Feature importance\n",
        "    print(f\"\\nüéØ Top 15 Feature Importance:\")\n",
        "\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': X_train.columns,\n",
        "        'importance': xgb_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    for i, (_, row) in enumerate(feature_importance.head(15).iterrows()):\n",
        "        print(f\"   {i+1:2d}. {row['feature']:35s}: {row['importance']:.4f}\")\n",
        "\n",
        "    # Log top features\n",
        "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
        "        mlflow.log_metric(f\"feature_importance_rank_{i+1}\", row['importance'])\n",
        "        mlflow.log_param(f\"top_feature_{i+1}\", row['feature'])\n",
        "\n",
        "    # 9. Model performance analysis\n",
        "    print(f\"\\nüìà Model Performance Analysis:\")\n",
        "    print(f\"   Training vs Validation WMAE: ${train_wmae:,.2f} vs ${val_wmae:,.2f}\")\n",
        "    print(f\"   Overfitting check: {(train_wmae - val_wmae)/val_wmae*100:.1f}% difference\")\n",
        "    print(f\"   Holiday vs Non-Holiday performance:\")\n",
        "    print(f\"     Holiday MAE: ${val_holiday_mae:,.2f} ({val_holiday_mask.sum()} records)\")\n",
        "    print(f\"     Non-Holiday MAE: ${val_non_holiday_mae:,.2f} ({val_non_holiday_mask.sum()} records)\")\n",
        "\n",
        "    # 10. Save model\n",
        "    print(f\"\\nüíæ Saving model...\")\n",
        "    model_path = \"xgboost_walmart_experiment4_fixed.pkl\"\n",
        "    joblib.dump(xgb_model, model_path)\n",
        "    mlflow.log_artifact(model_path)\n",
        "\n",
        "    # Save feature importance\n",
        "    feature_importance.to_csv(\"feature_importance_experiment4.csv\", index=False)\n",
        "    mlflow.log_artifact(\"feature_importance_experiment4.csv\")\n",
        "\n",
        "    print(f\"\\n\" + \"=\"*80)\n",
        "    print(f\"üéâ EXPERIMENT 4 COMPLETED!\")\n",
        "    print(f\"‚úÖ Data Leakage: FIXED\")\n",
        "    print(f\"üéØ Main Metric (Validation WMAE): ${val_wmae:,.2f}\")\n",
        "    print(f\"üìä Validation R¬≤: {val_r2:.4f}\")\n",
        "    print(f\"üîß Features Used: {len(feature_columns)}\")\n",
        "    print(f\"=\"*80)\n",
        "\n",
        "    # Final status\n",
        "    mlflow.log_param(\"experiment_status\", \"COMPLETED\")\n",
        "    mlflow.log_param(\"data_leakage_fixed\", \"YES\")\n",
        "    mlflow.log_param(\"main_wmae\", f\"${val_wmae:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy54QDAyfHoH",
        "outputId": "c534176f-7c9d-46a7-edb5-96a5b612008f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting XGBoost training with FIXED data leakage: 1d16409318a342c89f4aa06c0f46766b\n",
            "üîß Preparing features and target variables...\n",
            "   Total features available: 164\n",
            "üìä Final data shapes:\n",
            "   X_train: (321582, 164)\n",
            "   X_val: (82260, 164)\n",
            "ü§ñ Training XGBoost model...\n",
            "   Fitting model...\n",
            "‚úÖ Model training completed!\n",
            "üîÆ Making predictions...\n",
            "üìä Calculating metrics...\n",
            "\n",
            "================================================================================\n",
            "üìä EXPERIMENT 4 RESULTS (DATA LEAKAGE FIXED)\n",
            "================================================================================\n",
            "\n",
            "üöÇ Training Metrics:\n",
            "   WMAE: $95.21\n",
            "   MAE: $91.72\n",
            "   RMSE: $176.55\n",
            "   R¬≤: 0.9999\n",
            "   Holiday MAE: $106.04\n",
            "   Non-Holiday MAE: $90.46\n",
            "\n",
            "üîÆ Validation Metrics:\n",
            "   WMAE: $506.65\n",
            "   MAE: $557.08\n",
            "   RMSE: $3,029.55\n",
            "   R¬≤: 0.9564\n",
            "   Holiday MAE: $146.44\n",
            "   Non-Holiday MAE: $571.98\n",
            "\n",
            "üéØ Top 15 Feature Importance:\n",
            "    1. Weekly_Sales_lag_1                 : 0.6252\n",
            "    2. Weekly_Sales_lag_4                 : 0.0314\n",
            "    3. Weekly_Sales_diff_1                : 0.0231\n",
            "    4. IsChristmasWeek                    : 0.0153\n",
            "    5. Weekly_Sales_lag_2                 : 0.0111\n",
            "    6. Dept_38                            : 0.0111\n",
            "    7. Dept_95                            : 0.0091\n",
            "    8. Dept_40                            : 0.0090\n",
            "    9. IsMajorHoliday                     : 0.0089\n",
            "   10. Dept_92                            : 0.0082\n",
            "   11. Weekly_Sales_diff_4                : 0.0078\n",
            "   12. Type_B                             : 0.0076\n",
            "   13. WeeksFromStart                     : 0.0075\n",
            "   14. Dept_13                            : 0.0067\n",
            "   15. Dept_2                             : 0.0066\n",
            "\n",
            "üìà Model Performance Analysis:\n",
            "   Training vs Validation WMAE: $95.21 vs $506.65\n",
            "   Overfitting check: -81.2% difference\n",
            "   Holiday vs Non-Holiday performance:\n",
            "     Holiday MAE: $146.44 (2879 records)\n",
            "     Non-Holiday MAE: $571.98 (79381 records)\n",
            "\n",
            "üíæ Saving model...\n",
            "\n",
            "================================================================================\n",
            "üéâ EXPERIMENT 4 COMPLETED!\n",
            "‚úÖ Data Leakage: FIXED\n",
            "üéØ Main Metric (Validation WMAE): $506.65\n",
            "üìä Validation R¬≤: 0.9564\n",
            "üîß Features Used: 164\n",
            "================================================================================\n",
            "üèÉ View run XGBoost_Training_Fixed_Data_Leakage at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3/runs/1d16409318a342c89f4aa06c0f46766b\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/3\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}