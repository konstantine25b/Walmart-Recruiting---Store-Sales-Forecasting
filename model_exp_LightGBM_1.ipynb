{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting/blob/lodia/model_exp_LightGBM_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kaggle dagshub mlflow wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2zn2utMqq-y",
        "outputId": "d081ba67-6355-40e5-af16-bf41fa93498b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m118.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "foFWtundZCTx"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CqdtFSAaZDlt"
      },
      "outputs": [],
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Oyi1Zq8cZFIW"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWwWkCerZGX1",
        "outputId": "d3a9c98a-618a-47b8-b526-85882d12fcc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 849MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrKcnvgKZH_b",
        "outputId": "f1cefffb-cde1-4580-e56b-648c0b63e6d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ],
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from io import StringIO\n",
        "import warnings\n",
        "import mlflow\n",
        "import dagshub\n",
        "import zipfile\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "PAjjz1u-mFOm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "warnings.filterwarnings('ignore', category=FutureWarning)\n",
        "\n",
        "# --- Configuration ---\n",
        "# Set up a temporary directory for CmdStanPy (even if not directly used, good practice)\n",
        "colab_tmp_dir = '/tmp/cmdstanpy_tmp_colab'\n",
        "os.makedirs(colab_tmp_dir, exist_ok=True)\n",
        "os.environ['CMDSTANPY_TEMP'] = colab_tmp_dir\n",
        "# Suppress cmdstanpy and general INFO logs (for Prophet, but good to keep general)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.CRITICAL)\n",
        "logging.getLogger().setLevel(logging.WARNING) # Set root logger to WARNING to reduce general verbosity\n"
      ],
      "metadata": {
        "id": "MTAsOSA5mik-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Aggregates results by Store and Dept to calculate WMAE etc.\n",
        "def aggregate_results(df):\n",
        "    \"\"\"Aggregates sales data to calculate total weekly sales and identifies holidays.\"\"\"\n",
        "    df_agg = df.groupby('Date').agg(\n",
        "        Weekly_Sales=('Weekly_Sales', 'sum'),\n",
        "        IsHoliday=('IsHoliday', 'max') # If any store is holiday, the week is holiday\n",
        "    ).reset_index()\n",
        "    return df_agg\n",
        "\n"
      ],
      "metadata": {
        "id": "I-mxt1oamn-h"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Weighted Mean Absolute Error (WMAE)\n",
        "def calculate_wmae(y_true, y_pred, is_holiday):\n",
        "    \"\"\"\n",
        "    Calculates the Weighted Mean Absolute Error (WMAE) based on Walmart's criteria.\n",
        "    Holiday weeks are weighted 5x.\n",
        "    \"\"\"\n",
        "    weights = np.where(is_holiday, 5, 1)\n",
        "    wmae = np.sum(weights * np.abs(y_true - y_pred)) / np.sum(weights)\n",
        "    return wmae"
      ],
      "metadata": {
        "id": "U4om8aJFmsEf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Preprocessing Pipeline\n",
        "class WalmartPreprocessingPipeline:\n",
        "    def __init__(self, train_date_split='2012-04-13', remove_outliers=True, remove_markdowns=True, enable_lag_features=False):\n",
        "        self.train_date_split = pd.to_datetime(train_date_split)\n",
        "        self.remove_outliers = remove_outliers\n",
        "        self.remove_markdowns = remove_markdowns\n",
        "        self.enable_lag_features = enable_lag_features\n",
        "        self.weekly_sales_outlier_thresholds = {\n",
        "            'A': {'lower': -20000, 'upper': 75000},\n",
        "            'B': {'lower': -10000, 'upper': 40000},\n",
        "            'C': {'lower': -5000, 'upper': 20000}\n",
        "        }\n",
        "        self.outliers_removed_count = 0\n",
        "\n",
        "    def fit_transform(self, train_df, features_df, stores_df):\n",
        "        df = self._merge_data(train_df, features_df, stores_df)\n",
        "        df = self._clean_data(df)\n",
        "        df = self._feature_engineer_dates(df)\n",
        "        df = self._feature_engineer_holidays(df)\n",
        "        df = self._feature_engineer_store_type(df) # Ensure type encoding happens early for outlier removal\n",
        "        if self.remove_outliers:\n",
        "            df = self._remove_outliers(df)\n",
        "        if self.remove_markdowns:\n",
        "            df = self._remove_markdowns(df)\n",
        "        if not self.enable_lag_features:\n",
        "            df = self._remove_lag_features(df) # Remove MarkDowns if not enabled\n",
        "        df = self._handle_missing_values(df)\n",
        "        df = self._remove_redundant_features(df)\n",
        "        return df\n",
        "\n",
        "    def _merge_data(self, train_df, features_df, stores_df):\n",
        "        df = pd.merge(train_df, stores_df, on='Store', how='left')\n",
        "        df = pd.merge(df, features_df, on=['Store', 'Date'], how='left')\n",
        "        return df\n",
        "\n",
        "    def _clean_data(self, df):\n",
        "        # Handle duplicate IsHoliday columns after merge (e.g., IsHoliday_x, IsHoliday_y)\n",
        "        if 'IsHoliday_x' in df.columns and 'IsHoliday_y' in df.columns:\n",
        "            df['IsHoliday'] = df['IsHoliday_x'] | df['IsHoliday_y']\n",
        "            df = df.drop(columns=['IsHoliday_x', 'IsHoliday_y'])\n",
        "        # Ensure IsHoliday is boolean\n",
        "        df['IsHoliday'] = df['IsHoliday'].astype(bool)\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_dates(self, df):\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week.astype(int) # Use isocalendar for week\n",
        "        df['IsWeekend'] = df['DayOfWeek'].isin([5, 6]).astype(int)\n",
        "        df['IsMonthStart'] = (df['Date'].dt.day == 1).astype(int)\n",
        "        df['IsMonthEnd'] = (df['Date'].dt.is_month_end).astype(int)\n",
        "\n",
        "        # Days/Weeks from start\n",
        "        min_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - min_date).dt.days\n",
        "        df['WeeksFromStart'] = (df['DaysFromStart'] // 7).astype(int)\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_holidays(self, df):\n",
        "        # Define major US holidays and their approximate weeks\n",
        "        holidays = {\n",
        "            'SuperBowl': [datetime(y, 2, d) for y in range(2010, 2013) for d in [7, 8, 9, 10, 11, 12, 13]], # Approx 2nd week Feb\n",
        "            'LaborDay': [datetime(y, 9, d) for y in range(2010, 2013) for d in [1, 2, 3, 4, 5, 6, 7]], # Approx 1st week Sep\n",
        "            'Thanksgiving': [datetime(y, 11, d) for y in range(2010, 2013) for d in [22, 23, 24, 25, 26, 27, 28]], # Approx 4th week Nov\n",
        "            'Christmas': [datetime(y, 12, d) for y in range(2010, 2013) for d in [24, 25, 26, 27, 28, 29, 30, 31]] # Approx last week Dec\n",
        "        }\n",
        "\n",
        "        # Create binary flags for holiday weeks\n",
        "        df['IsSuperBowlWeek'] = df['Date'].isin(holidays['SuperBowl']).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].isin(holidays['LaborDay']).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].isin(holidays['Thanksgiving']).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].isin(holidays['Christmas']).astype(int)\n",
        "\n",
        "        # Broader holiday indicators\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] | df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12, 1, 2, 9]).astype(int) # Nov, Dec, Jan, Feb, Sep\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8]).astype(int) # August often back to school\n",
        "\n",
        "        return df\n",
        "\n",
        "    def _feature_engineer_store_type(self, df):\n",
        "        df['Type_Encoded'] = df['Type'].astype('category').cat.codes\n",
        "        df = pd.get_dummies(df, columns=['Type'], prefix='Type', drop_first=False)\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df):\n",
        "        initial_rows = len(df)\n",
        "        df_cleaned = pd.DataFrame()\n",
        "        for store_type in self.weekly_sales_outlier_thresholds.keys():\n",
        "            subset = df[df[f'Type_{store_type}'] == 1].copy() # Ensure we're working on the dummy column\n",
        "            lower = self.weekly_sales_outlier_thresholds[store_type]['lower']\n",
        "            upper = self.weekly_sales_outlier_thresholds[store_type]['upper']\n",
        "            subset = subset[(subset['Weekly_Sales'] >= lower) & (subset['Weekly_Sales'] <= upper)]\n",
        "            df_cleaned = pd.concat([df_cleaned, subset])\n",
        "        self.outliers_removed_count = initial_rows - len(df_cleaned)\n",
        "        if self.outliers_removed_count > 0:\n",
        "            print(f\"Removed {self.outliers_removed_count} outliers based on Weekly_Sales thresholds.\")\n",
        "        return df_cleaned\n",
        "\n",
        "    def _remove_markdowns(self, df):\n",
        "        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n",
        "        return df.drop(columns=markdown_cols, errors='ignore')\n",
        "\n",
        "    def _remove_lag_features(self, df):\n",
        "        # Placeholder: If any specific 'lagged_sales' features were created, they'd be removed here.\n",
        "        # For now, it mainly means removing MarkDowns which act like lag features.\n",
        "        return df\n",
        "\n",
        "    def _handle_missing_values(self, df):\n",
        "        # For simplicity, fill numerical NaNs with 0 (or median/mean as appropriate)\n",
        "        # For LightGBM, it can handle NaNs directly, but explicit filling is safer.\n",
        "        numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "        df[numerical_cols] = df[numerical_cols].fillna(0) # Simple imputation\n",
        "        return df\n",
        "\n",
        "    def _remove_redundant_features(self, df):\n",
        "        # Drop columns not needed for LightGBM or that are redundant after feature engineering\n",
        "        # 'Date' will be used for splitting, then dropped as a feature\n",
        "        cols_to_drop = [\n",
        "            'Year', # Can be redundant with other temporal features\n",
        "            'Day',  # WeekOfYear/DayOfWeek cover this\n",
        "            'Size' # keeping this as a feature for LightGBM\n",
        "        ]\n",
        "        return df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors='ignore')\n"
      ],
      "metadata": {
        "id": "vQS-P13cmtcu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- MLflow Setup Function ---\n",
        "def setup_mlflow_lgbm(repo_owner, repo_name):\n",
        "    \"\"\"Setup MLflow and DagsHub tracking for LightGBM.\"\"\"\n",
        "    print(\"ğŸ”§ Setting up MLflow and DagsHub for LightGBM...\")\n",
        "\n",
        "    # End any active runs first\n",
        "    try:\n",
        "        mlflow.end_run()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize DagsHub\n",
        "    try:\n",
        "        dagshub.init(\n",
        "            repo_owner='konstantine25b',\n",
        "            repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "            mlflow=True\n",
        "        )\n",
        "        print(\"âœ… DagsHub initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ DagsHub init warning: {e}\")\n",
        "        print(\"   (Ensure DAGSHUB_USER_TOKEN environment variable is set or you have write access)\")\n",
        "\n",
        "\n",
        "    # Set MLflow tracking URI\n",
        "    mlflow.set_tracking_uri(f\"https://dagshub.com/{repo_owner}/{repo_name}.mlflow\")\n",
        "    print(f\"MLflow tracking URI set to: {mlflow.get_tracking_uri()}\")\n",
        "\n",
        "    # Create a new experiment for this run\n",
        "    experiment_name = f\"LightGBM_Walmart_Sales_{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    print(f\"MLflow Experiment set to: '{experiment_name}'\")"
      ],
      "metadata": {
        "id": "S9kquY5Xm5ve"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Execution ---\n",
        "def main():\n",
        "    print(\"ğŸš€ Starting LightGBM Walmart Sales Forecasting Experiment...\")\n",
        "\n",
        "    # Load data\n",
        "    print(\"Loading data...\")\n",
        "    train_raw = pd.read_csv('train.csv')\n",
        "    features_raw = pd.read_csv('features.csv')\n",
        "    stores_raw = pd.read_csv('stores.csv')\n",
        "    print(\"Data loaded.\")\n",
        "\n",
        "    # Preprocess data\n",
        "    print(\"Preprocessing data...\")\n",
        "    pipeline = WalmartPreprocessingPipeline(\n",
        "        train_date_split='2012-04-13',\n",
        "        remove_outliers=True,\n",
        "        remove_markdowns=True,\n",
        "        enable_lag_features=False\n",
        "    )\n",
        "    processed_df = pipeline.fit_transform(train_raw, features_raw, stores_raw)\n",
        "    print(\"Data preprocessing complete.\")\n",
        "    print(f\"Total rows after preprocessing: {len(processed_df)}\")\n",
        "    print(f\"Outliers removed during preprocessing: {pipeline.outliers_removed_count}\")\n",
        "\n",
        "    # Define features and target\n",
        "    # 'Date' is used for splitting but not as a feature for LightGBM.\n",
        "    # 'Weekly_Sales' is the target.\n",
        "    # All other columns in processed_df are considered potential features.\n",
        "    all_potential_features = [col for col in processed_df.columns if col not in ['Date', 'Weekly_Sales']]\n",
        "    target = 'Weekly_Sales'\n",
        "\n",
        "    # Identify categorical features for LightGBM\n",
        "    # Ensure 'Store' and 'Dept' are included here\n",
        "    categorical_features_for_lgbm = [\n",
        "        'Store', 'Dept', 'Month', 'DayOfWeek', 'WeekOfYear',\n",
        "        'Type_Encoded', 'Type_A', 'Type_B', 'Type_C',\n",
        "        'IsHoliday', 'IsWeekend', 'IsMonthStart', 'IsMonthEnd',\n",
        "        'IsSuperBowlWeek', 'IsLaborDayWeek', 'IsThanksgivingWeek',\n",
        "        'IsChristmasWeek', 'IsMajorHoliday', 'IsHolidayMonth', 'IsBackToSchool'\n",
        "    ]\n",
        "\n",
        "    # Convert specified categorical columns to 'category' dtype\n",
        "    for col in categorical_features_for_lgbm:\n",
        "        if col in processed_df.columns:\n",
        "            processed_df[col] = processed_df[col].astype('category')\n",
        "        else:\n",
        "            print(f\"Warning: Categorical feature '{col}' not found in processed data. Skipping conversion.\")\n",
        "            # Remove from the list if not found to prevent errors later\n",
        "            categorical_features_for_lgbm = [f for f in categorical_features_for_lgbm if f != col]\n",
        "\n",
        "\n",
        "    # Temporal Split - slice the processed_df first\n",
        "    print(f\"Splitting data into training and validation sets at {pipeline.train_date_split}...\")\n",
        "\n",
        "    train_df_sliced = processed_df[processed_df['Date'] <= pipeline.train_date_split].copy()\n",
        "    val_df_sliced = processed_df[processed_df['Date'] > pipeline.train_date_split].copy()\n",
        "\n",
        "    # Now, define X_train, y_train, etc. from the sliced dataframes\n",
        "    X_train = train_df_sliced[all_potential_features]\n",
        "    y_train = train_df_sliced[target]\n",
        "    is_holiday_train = train_df_sliced['IsHoliday']\n",
        "\n",
        "    X_val = val_df_sliced[all_potential_features]\n",
        "    y_val = val_df_sliced[target]\n",
        "    is_holiday_val = val_df_sliced['IsHoliday']\n",
        "\n",
        "    print(f\"Training set size: {len(X_train)} records\")\n",
        "    print(f\"Validation set size: {len(X_val)} records\")\n",
        "\n",
        "    # --- LightGBM Model Training ---\n",
        "    print(\"\\nğŸš€ Training LightGBM model...\")\n",
        "\n",
        "    lgbm_params = {\n",
        "        'objective': 'regression_l1', # MAE objective\n",
        "        'metric': 'mae',\n",
        "        'n_estimators': 1000,\n",
        "        'learning_rate': 0.05,\n",
        "        'feature_fraction': 0.8,\n",
        "        'bagging_fraction': 0.8,\n",
        "        'bagging_freq': 1,\n",
        "        'lambda_l1': 0.1,\n",
        "        'lambda_l2': 0.1,\n",
        "        'num_leaves': 31,\n",
        "        'verbose': -1, # Suppress verbose output during training\n",
        "        'n_jobs': -1, # Use all available cores\n",
        "        'seed': 42,\n",
        "        'boosting_type': 'gbdt',\n",
        "        # 'early_stopping_round': 50 # Moved to callbacks\n",
        "    }\n",
        "\n",
        "    model = lgb.LGBMRegressor(**lgbm_params)\n",
        "\n",
        "    # Use StringIO to capture LightGBM's default verbose output during fit if not suppressed by verbose=-1\n",
        "    original_stdout = sys.stdout\n",
        "    original_stderr = sys.stderr\n",
        "    sys.stdout = StringIO()\n",
        "    sys.stderr = StringIO()\n",
        "\n",
        "    try:\n",
        "        model.fit(X_train, y_train,\n",
        "                  eval_set=[(X_val, y_val)],\n",
        "                  eval_metric='mae', # Evaluate on MAE for early stopping\n",
        "                  callbacks=[lgb.early_stopping(50, verbose=False)], # Use callback for early stopping\n",
        "                  categorical_feature=[col for col in categorical_features_for_lgbm if col in X_train.columns]\n",
        "                 )\n",
        "    except Exception as e:\n",
        "        print(f\"â›” LightGBM training failed: {e}\")\n",
        "        sys.stdout = original_stdout # Restore stdout on error\n",
        "        sys.stderr = original_stderr\n",
        "        return # Exit if training fails\n",
        "    finally:\n",
        "        sys.stdout = original_stdout # Restore stdout\n",
        "        sys.stderr = original_stderr\n",
        "\n",
        "    print(\"âœ… LightGBM Training Complete!\")\n",
        "\n",
        "    # --- Evaluation ---\n",
        "    print(\"\\nğŸ“Š Evaluating model performance...\")\n",
        "\n",
        "    # Predictions\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "\n",
        "    # Ensure no negative predictions (sales cannot be negative)\n",
        "    y_train_pred[y_train_pred < 0] = 0\n",
        "    y_val_pred[y_val_pred < 0] = 0\n",
        "\n",
        "    # TRAINING METRICS\n",
        "    train_wmae = calculate_wmae(y_train, y_train_pred, is_holiday_train)\n",
        "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
        "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
        "    train_r2 = r2_score(y_train, y_train_pred)\n",
        "\n",
        "    print(\"\\n==================================================\")\n",
        "    print(\"ğŸ“Š TRAINING METRICS (Aggregated In-Sample):\")\n",
        "    print(f\"   WMAE: {train_wmae:.2f}\")\n",
        "    print(f\"   RMSE: {train_rmse:.2f}\")\n",
        "    print(f\"   MAE: {train_mae:.2f}\")\n",
        "    print(f\"   RÂ²: {train_r2:.4f}\")\n",
        "\n",
        "    # VALIDATION METRICS\n",
        "    val_wmae = calculate_wmae(y_val, y_val_pred, is_holiday_val)\n",
        "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "    val_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    print(\"\\nğŸ“Š VALIDATION METRICS (Aggregated Out-of-Sample):\")\n",
        "    print(f\"   WMAE: {val_wmae:.2f} â­\")\n",
        "    print(f\"   RMSE: {val_rmse:.2f}\")\n",
        "    print(f\"   MAE: {val_mae:.2f}\")\n",
        "    print(f\"   RÂ²: {val_r2:.4f}\")\n",
        "\n",
        "    # Overfitting Analysis\n",
        "    print(\"\\nğŸ” OVERFITTING ANALYSIS:\")\n",
        "    wmae_ratio = val_wmae / train_wmae if train_wmae != 0 else float('inf')\n",
        "    r2_diff = train_r2 - val_r2\n",
        "    print(f\"   WMAE Ratio (val/train): {wmae_ratio:.2f}\")\n",
        "    if wmae_ratio > 2.0:\n",
        "        print(\"   âš ï¸ High overfitting detected (WMAE ratio > 2.0)\")\n",
        "    elif wmae_ratio > 1.2:\n",
        "        print(\"   â— Moderate overfitting detected (WMAE ratio > 1.2)\")\n",
        "    else:\n",
        "        print(\"   âœ… Low overfitting detected\")\n",
        "    print(f\"   RÂ² Difference (train-val): {r2_diff:.4f}\")\n",
        "\n",
        "    # Feature Importance\n",
        "    print(\"\\nğŸ¯ FEATURE IMPORTANCE (LightGBM):\")\n",
        "    # Ensure X_train.columns are aligned with model.feature_importances_\n",
        "    # If a categorical feature was not found, it might have been removed from categorical_features_for_lgbm list\n",
        "    # but still needs to be handled in X_train.columns\n",
        "    if len(X_train.columns) == len(model.feature_importances_):\n",
        "        feature_importances = pd.DataFrame({\n",
        "            'feature': X_train.columns,\n",
        "            'importance': model.feature_importances_\n",
        "        }).sort_values(by='importance', ascending=False)\n",
        "        print(feature_importances.head(15)) # Print top 15 features\n",
        "    else:\n",
        "        print(\"Error: Feature importance column count mismatch. Cannot display.\")\n",
        "\n",
        "\n",
        "    # Model Info\n",
        "    print(\"\\nğŸ“ˆ MODEL INFO:\")\n",
        "    print(f\"   Training records: {len(X_train)}\")\n",
        "    print(f\"   Validation records: {len(X_val)}\")\n",
        "    print(f\"   Number of features: {len(X_train.columns)}\")\n",
        "    print(f\"   LightGBM Parameters: {lgbm_params}\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"\\nğŸ‰ EXPERIMENT COMPLETED!\")\n",
        "    print(\"============================================================\")\n",
        "    print(f\"ğŸ† Final Validation WMAE (Aggregated): {val_wmae:.2f}\")\n",
        "    print(f\"ğŸ“Š Final Validation RÂ² (Aggregated): {val_r2:.4f}\")\n",
        "    print(\"ğŸ¯ This experiment uses the correct Walmart WMAE formula:\")\n",
        "    print(\"   â€¢ Holiday weeks weighted 5x\")\n",
        "    print(\"   â€¢ Regular weeks weighted 1x\")\n",
        "    print(\"   â€¢ Global LightGBM model trained on all Store-Dept data\")\n",
        "    print(\"   â€¢ Extensive date and holiday features, plus Store/Dept as categorical features\")\n"
      ],
      "metadata": {
        "id": "cEciNQ39orng"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    train = pd.read_csv('train.csv')\n",
        "    with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    features = pd.read_csv('features.csv')\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vguKig0Eo_kl",
        "outputId": "fb170e86-4108-4d44-a2cb-c688baeaccbc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting LightGBM Walmart Sales Forecasting Experiment...\n",
            "Loading data...\n",
            "Data loaded.\n",
            "Preprocessing data...\n",
            "Removed 28279 outliers based on Weekly_Sales thresholds.\n",
            "Data preprocessing complete.\n",
            "Total rows after preprocessing: 393291\n",
            "Outliers removed during preprocessing: 28279\n",
            "Splitting data into training and validation sets at 2012-04-13 00:00:00...\n",
            "Training set size: 315896 records\n",
            "Validation set size: 77395 records\n",
            "\n",
            "ğŸš€ Training LightGBM model...\n",
            "âœ… LightGBM Training Complete!\n",
            "\n",
            "ğŸ“Š Evaluating model performance...\n",
            "\n",
            "==================================================\n",
            "ğŸ“Š TRAINING METRICS (Aggregated In-Sample):\n",
            "   WMAE: 1554.33\n",
            "   RMSE: 3251.47\n",
            "   MAE: 1464.41\n",
            "   RÂ²: 0.9481\n",
            "\n",
            "ğŸ“Š VALIDATION METRICS (Aggregated Out-of-Sample):\n",
            "   WMAE: 1663.04 â­\n",
            "   RMSE: 3359.20\n",
            "   MAE: 1654.74\n",
            "   RÂ²: 0.9449\n",
            "\n",
            "ğŸ” OVERFITTING ANALYSIS:\n",
            "   WMAE Ratio (val/train): 1.07\n",
            "   âœ… Low overfitting detected\n",
            "   RÂ² Difference (train-val): 0.0031\n",
            "\n",
            "ğŸ¯ FEATURE IMPORTANCE (LightGBM):\n",
            "           feature  importance\n",
            "1             Dept       10765\n",
            "0            Store        8761\n",
            "9       WeekOfYear        5193\n",
            "2      Temperature         881\n",
            "4              CPI         798\n",
            "13   DaysFromStart         770\n",
            "22    Type_Encoded         658\n",
            "7            Month         611\n",
            "3       Fuel_Price         604\n",
            "5     Unemployment         534\n",
            "14  WeeksFromStart         132\n",
            "25          Type_C          71\n",
            "12      IsMonthEnd          52\n",
            "23          Type_A          30\n",
            "20  IsHolidayMonth          29\n",
            "\n",
            "ğŸ“ˆ MODEL INFO:\n",
            "   Training records: 315896\n",
            "   Validation records: 77395\n",
            "   Number of features: 26\n",
            "   LightGBM Parameters: {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1000, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42, 'boosting_type': 'gbdt'}\n",
            "\n",
            "ğŸ‰ EXPERIMENT COMPLETED!\n",
            "============================================================\n",
            "ğŸ† Final Validation WMAE (Aggregated): 1663.04\n",
            "ğŸ“Š Final Validation RÂ² (Aggregated): 0.9449\n",
            "ğŸ¯ This experiment uses the correct Walmart WMAE formula:\n",
            "   â€¢ Holiday weeks weighted 5x\n",
            "   â€¢ Regular weeks weighted 1x\n",
            "   â€¢ Global LightGBM model trained on all Store-Dept data\n",
            "   â€¢ Extensive date and holiday features, plus Store/Dept as categorical features\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}