{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZVAjWk419tY"
      },
      "source": [
        "# Experiment 11 Sarimax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "JlzbNqk619tc",
        "outputId": "110ef522-f3ae-4b1f-b119-8de0520481b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting.zip\n",
        "!unzip train.csv.zip\n",
        "!unzip features.csv.zip"
      ],
      "metadata": {
        "id": "j_NWCiqC1_08",
        "outputId": "db2b1c49-3bb1-4219-e0c6-5363b01565c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 205MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  features.csv.zip\n",
            "  inflating: features.csv            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels mlflow dagshub scikit-learn pandas numpy matplotlib seaborn joblib -q"
      ],
      "metadata": {
        "id": "PnryOXPu2D3d",
        "outputId": "2abc4cc4-8ca3-49c8-d52d-a289dd517ca4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m62.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m246.9/246.9 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import zipfile\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# SPEED OPTIMIZATION: Suppress verbose logging globally\n",
        "import logging\n",
        "logging.getLogger('statsmodels').setLevel(logging.WARNING)\n",
        "\n",
        "# Core libraries\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "# SARIMAX libraries\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import joblib\n",
        "import os\n",
        "import json\n"
      ],
      "metadata": {
        "id": "cXDxPVLY2NTV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartPreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Complete preprocessing pipeline for Walmart sales data\n",
        "    Supports fit/transform pattern for proper train/validation handling\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "        self.outlier_thresholds = None\n",
        "        self.feature_columns = None\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load and merge train.csv, stores.csv, features.csv datasets\"\"\"\n",
        "        print(\"üìä Loading datasets...\")\n",
        "\n",
        "        # Load datasets\n",
        "        train_df = pd.read_csv('train.csv')\n",
        "        stores_df = pd.read_csv('stores.csv')\n",
        "        features_df = pd.read_csv('features.csv')\n",
        "\n",
        "        print(f\"   üìà Train data: {train_df.shape}\")\n",
        "        print(f\"   üè™ Stores data: {stores_df.shape}\")\n",
        "        print(f\"   üéØ Features data: {features_df.shape}\")\n",
        "\n",
        "        # Convert Date column to datetime\n",
        "        train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "        features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "        # Merge datasets\n",
        "        train_stores = train_df.merge(stores_df, on='Store', how='left')\n",
        "        train_full = train_stores.merge(features_df, on=['Store', 'Date'], how='left')\n",
        "\n",
        "        print(f\"   ‚úÖ Merged data: {train_full.shape}\")\n",
        "        print(f\"   üìÖ Date range: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n",
        "\n",
        "        return train_full\n",
        "\n",
        "    def clean_merged_data(self, train_full):\n",
        "        \"\"\"Clean merged data by handling duplicate IsHoliday columns\"\"\"\n",
        "        print(\"üßπ Cleaning merged data...\")\n",
        "\n",
        "        initial_shape = train_full.shape\n",
        "\n",
        "        # Handle duplicate IsHoliday columns if they exist\n",
        "        if 'IsHoliday_x' in train_full.columns and 'IsHoliday_y' in train_full.columns:\n",
        "            print(\"   üîÑ Resolving duplicate IsHoliday columns...\")\n",
        "            train_full['IsHoliday'] = train_full['IsHoliday_x'] | train_full['IsHoliday_y']\n",
        "            train_full = train_full.drop(['IsHoliday_x', 'IsHoliday_y'], axis=1)\n",
        "\n",
        "        print(f\"   ‚úÖ Cleaned data: {train_full.shape} (was {initial_shape})\")\n",
        "        return train_full\n",
        "\n",
        "    def create_temporal_split(self, df, train_ratio=0.8):\n",
        "        \"\"\"Create temporal split to prevent data leakage\"\"\"\n",
        "        print(f\"üìÖ Creating temporal split ({int(train_ratio*100)}/{int((1-train_ratio)*100)})...\")\n",
        "\n",
        "        # Sort by date to ensure temporal order\n",
        "        df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Find split point\n",
        "        split_idx = int(len(df_sorted) * train_ratio)\n",
        "        split_date = df_sorted.iloc[split_idx]['Date']\n",
        "\n",
        "        # Create splits\n",
        "        train_data = df_sorted.iloc[:split_idx].copy()\n",
        "        val_data = df_sorted.iloc[split_idx:].copy()\n",
        "\n",
        "        # Create split info dictionary\n",
        "        split_info = {\n",
        "            'split_date': split_date,\n",
        "            'train_size': len(train_data),\n",
        "            'val_size': len(val_data),\n",
        "            'train_date_range': (train_data['Date'].min(), train_data['Date'].max()),\n",
        "            'val_date_range': (val_data['Date'].min(), val_data['Date'].max())\n",
        "        }\n",
        "\n",
        "        print(f\"   üìä Split date: {split_date}\")\n",
        "        print(f\"   üìà Train: {len(train_data):,} records ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
        "        print(f\"   üìâ Val: {len(val_data):,} records ({val_data['Date'].min()} to {val_data['Date'].max()})\")\n",
        "\n",
        "        return train_data, val_data, split_info\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"Fit the preprocessing pipeline on training data\"\"\"\n",
        "        print(\"üîß Fitting preprocessing pipeline on training data...\")\n",
        "\n",
        "        # Store training data for lag feature creation\n",
        "        self.train_data_for_lags = train_data.copy()\n",
        "\n",
        "        # Fit outlier removal thresholds on training data only\n",
        "        self.outlier_thresholds = {\n",
        "            'A': {'lower': -1000, 'upper': 50000},  # Type A stores\n",
        "            'B': {'lower': -500, 'upper': 25000},   # Type B stores\n",
        "            'C': {'lower': -200, 'upper': 15000}    # Type C stores\n",
        "        }\n",
        "\n",
        "        print(\"‚úÖ Pipeline fitted on training data\")\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, data, is_validation=False):\n",
        "        \"\"\"Transform data using fitted pipeline\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before transform!\")\n",
        "\n",
        "        print(f\"üîÑ Transforming {'validation' if is_validation else 'training'} data...\")\n",
        "\n",
        "        df = data.copy()\n",
        "\n",
        "        # Step 1: Create date features\n",
        "        df = self._create_date_features(df)\n",
        "\n",
        "        # Step 2: Create holiday features\n",
        "        df = self._create_holiday_features(df)\n",
        "\n",
        "        # Step 3: Encode categorical features (BEFORE outlier removal!)\n",
        "        df = self._encode_categorical_features(df)\n",
        "\n",
        "        # Step 4: Create lag features (different for train vs validation)\n",
        "        if is_validation:\n",
        "            df = self._create_lag_features_validation(df)\n",
        "        else:\n",
        "            df = self._create_lag_features_training(df)\n",
        "\n",
        "        # Step 5: Remove outliers (only on training data, AFTER encoding)\n",
        "        if not is_validation:\n",
        "            df = self._remove_outliers(df)\n",
        "\n",
        "        # Step 6: Remove markdown features\n",
        "        df = self._remove_markdown_features(df)\n",
        "\n",
        "        # Step 7: Remove redundant features\n",
        "        df = self._remove_redundant_features(df)\n",
        "\n",
        "        print(f\"‚úÖ Transform complete. Shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, train_data):\n",
        "        \"\"\"Fit and transform training data in one step\"\"\"\n",
        "        return self.fit(train_data).transform(train_data, is_validation=False)\n",
        "\n",
        "    def _create_date_features(self, df):\n",
        "        \"\"\"Create date features\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Year'] = df['Date'].dt.year\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['Day'] = df['Date'].dt.day\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "        df['Quarter'] = df['Date'].dt.quarter\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        df['IsMonthStart'] = df['Date'].dt.is_month_start.astype(int)\n",
        "        df['IsMonthEnd'] = df['Date'].dt.is_month_end.astype(int)\n",
        "        df['IsQuarterStart'] = df['Date'].dt.is_quarter_start.astype(int)\n",
        "        df['IsQuarterEnd'] = df['Date'].dt.is_quarter_end.astype(int)\n",
        "        start_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - start_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        return df\n",
        "\n",
        "    def _create_holiday_features(self, df):\n",
        "        \"\"\"Create holiday features\"\"\"\n",
        "        df = df.copy()\n",
        "        super_bowl_dates = ['2010-02-12', '2011-02-11', '2012-02-10']\n",
        "        labor_day_dates = ['2010-09-10', '2011-09-09', '2012-09-07']\n",
        "        thanksgiving_dates = ['2010-11-26', '2011-11-25', '2012-11-23']\n",
        "        christmas_dates = ['2010-12-31', '2011-12-30', '2012-12-28']\n",
        "\n",
        "        df['IsSuperBowlWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(super_bowl_dates).astype(int)\n",
        "        df['IsLaborDayWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(labor_day_dates).astype(int)\n",
        "        df['IsThanksgivingWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(thanksgiving_dates).astype(int)\n",
        "        df['IsChristmasWeek'] = df['Date'].dt.strftime('%Y-%m-%d').isin(christmas_dates).astype(int)\n",
        "        df['IsMajorHoliday'] = (df['IsSuperBowlWeek'] | df['IsLaborDayWeek'] |\n",
        "                               df['IsThanksgivingWeek'] | df['IsChristmasWeek']).astype(int)\n",
        "        df['IsHolidayMonth'] = df['Month'].isin([11, 12]).astype(int)\n",
        "        df['IsBackToSchool'] = df['Month'].isin([8, 9]).astype(int)\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features_training(self, df):\n",
        "        \"\"\"Create lag features for training data - DISABLED to reduce overfitting\"\"\"\n",
        "        # Lag features removed to prevent overfitting\n",
        "        return df\n",
        "\n",
        "    def _create_lag_features_validation(self, df):\n",
        "        \"\"\"Create lag features for validation data - DISABLED to reduce overfitting\"\"\"\n",
        "        # Lag features removed to prevent overfitting\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df):\n",
        "        \"\"\"Remove outliers from training data only\"\"\"\n",
        "        initial_len = len(df)\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        for store_type, thresholds in self.outlier_thresholds.items():\n",
        "            type_mask = df_clean[f'Type_{store_type}'] == 1\n",
        "            outlier_mask = (\n",
        "                (df_clean['Weekly_Sales'] < thresholds['lower']) |\n",
        "                (df_clean['Weekly_Sales'] > thresholds['upper'])\n",
        "            )\n",
        "            df_clean = df_clean[~(type_mask & outlier_mask)]\n",
        "\n",
        "        removed = initial_len - len(df_clean)\n",
        "        print(f\"   üóëÔ∏è Removed {removed:,} outliers from training data\")\n",
        "        return df_clean\n",
        "\n",
        "    def _remove_markdown_features(self, df):\n",
        "        \"\"\"Remove markdown columns\"\"\"\n",
        "        markdown_cols = [col for col in df.columns if 'MarkDown' in col]\n",
        "        if markdown_cols:\n",
        "            df = df.drop(markdown_cols, axis=1)\n",
        "        return df\n",
        "\n",
        "    def _remove_redundant_features(self, df):\n",
        "        \"\"\"Remove redundant features\"\"\"\n",
        "        redundant_cols = ['Year', 'Quarter', 'Day', 'WeekOfYear', 'DaysFromStart',\n",
        "                         'IsQuarterStart', 'IsQuarterEnd']\n",
        "        existing_redundant = [col for col in redundant_cols if col in df.columns]\n",
        "        if existing_redundant:\n",
        "            df = df.drop(existing_redundant, axis=1)\n",
        "        return df\n",
        "\n",
        "    def _encode_categorical_features(self, df):\n",
        "        \"\"\"Encode categorical features using both one-hot and label encoding\"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        if 'Type' in df.columns:\n",
        "            print(f\"   üîß Encoding Type column using both one-hot and label encoding...\")\n",
        "\n",
        "            # One-hot encoding (existing approach)\n",
        "            type_dummies = pd.get_dummies(df['Type'], prefix='Type', dtype=int)\n",
        "\n",
        "            # Label encoding (experiment_2 approach)\n",
        "            # A=0, B=1, C=2 (same as experiment_2)\n",
        "            type_mapping = {'A': 0, 'B': 1, 'C': 2}\n",
        "            df['Type_Encoded'] = df['Type'].map(type_mapping)\n",
        "\n",
        "            # Add one-hot columns\n",
        "            for col in type_dummies.columns:\n",
        "                df[col] = type_dummies[col]\n",
        "\n",
        "            # Remove original Type column\n",
        "            df = df.drop('Type', axis=1)\n",
        "\n",
        "            print(f\"   ‚úÖ Added both Type_Encoded and {list(type_dummies.columns)}\")\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "Vkxyb74Q2QHL"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_mlflow():\n",
        "    \"\"\"Setup MLflow and DagsHub tracking\"\"\"\n",
        "    print(\"üîß Setting up MLflow and DagsHub...\")\n",
        "\n",
        "    # End any active runs first\n",
        "    try:\n",
        "        mlflow.end_run()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize DagsHub\n",
        "    try:\n",
        "        dagshub.init(\n",
        "            repo_owner='konstantine25b',\n",
        "            repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "            mlflow=True\n",
        "        )\n",
        "        print(\"‚úÖ DagsHub initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è DagsHub init warning: {e}\")\n",
        "\n",
        "    # Set MLflow tracking URI\n",
        "    mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "\n",
        "    # Create unique experiment name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_name = f\"Experiment_11_SARIMAX_{timestamp}\"\n",
        "\n",
        "    try:\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"‚úÖ Created new experiment: {experiment_name}\")\n",
        "    except mlflow.exceptions.MlflowException as e:\n",
        "        if \"already exists\" in str(e):\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"‚úÖ Using existing experiment: {experiment_name}\")\n",
        "        else:\n",
        "            # Fallback to default experiment\n",
        "            experiment_name = \"Default\"\n",
        "            mlflow.set_experiment(experiment_name)\n",
        "            print(f\"‚ö†Ô∏è Using default experiment due to: {e}\")\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    print(f\"‚úÖ MLflow setup complete!\")\n",
        "    print(f\"üîó Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    print(f\"üìä Experiment: {experiment_name}\")\n",
        "\n",
        "    return experiment_name\n",
        "\n"
      ],
      "metadata": {
        "id": "HLtsJIIn2SyQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_preprocessed_data():\n",
        "    \"\"\"\n",
        "    Use preprocessing pipeline to get model-ready data\n",
        "\n",
        "    Returns:\n",
        "        X_train, y_train, X_val, y_val: Model-ready datasets\n",
        "        train_holidays, val_holidays: Holiday indicators for WMAE\n",
        "        split_info: Information about the temporal split\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Getting preprocessed data using pipeline...\")\n",
        "\n",
        "    # Create the preprocessing pipeline\n",
        "    pipeline = WalmartPreprocessingPipeline()\n",
        "\n",
        "    # Load raw data\n",
        "    train_full = pipeline.load_and_prepare_data()\n",
        "    train_full = pipeline.clean_merged_data(train_full)\n",
        "\n",
        "    # Create temporal split\n",
        "    train_data, val_data, split_info = pipeline.create_temporal_split(train_full)\n",
        "\n",
        "    # Extract holiday information before preprocessing\n",
        "    val_holidays = val_data['IsHoliday'].values.astype(bool)\n",
        "\n",
        "    # Separate validation target (realistic test scenario)\n",
        "    y_val = val_data['Weekly_Sales'].copy()\n",
        "    val_data_no_target = val_data.drop('Weekly_Sales', axis=1).copy()\n",
        "\n",
        "    # Fit and transform data using pipeline\n",
        "    pipeline.fit(train_data)\n",
        "    train_processed = pipeline.transform(train_data, is_validation=False)\n",
        "    val_processed = pipeline.transform(val_data_no_target, is_validation=True)\n",
        "\n",
        "    # Prepare final model data\n",
        "    X_train = train_processed.drop(['Weekly_Sales', 'Date'], axis=1)\n",
        "    y_train = train_processed['Weekly_Sales']\n",
        "    X_val = val_processed.drop('Date', axis=1)\n",
        "    train_holidays = train_processed['IsHoliday'].values.astype(bool)\n",
        "\n",
        "    # Store feature columns for later reference\n",
        "    feature_columns = list(X_train.columns)\n",
        "\n",
        "    print(f\"‚úÖ Data preprocessing complete!\")\n",
        "    print(f\"   üìä Training shape: {X_train.shape}\")\n",
        "    print(f\"   üìä Validation shape: {X_val.shape}\")\n",
        "    print(f\"   üéØ Features: {len(feature_columns)}\")\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, train_holidays, val_holidays, split_info, feature_columns\n",
        "\n"
      ],
      "metadata": {
        "id": "cEel455W2UHD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error (WMAE)\"\"\"\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "    wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "    return wmae\n",
        "\n",
        "\n",
        "def prepare_sarimax_data(X_train, y_train, X_val, y_val, train_holidays, val_holidays):\n",
        "    \"\"\"Prepare data for SARIMAX training on Store-Dept combinations\"\"\"\n",
        "    print(\"üìä Preparing data for SARIMAX modeling...\")\n",
        "\n",
        "    # Reconstruct full datasets with dates for SARIMAX\n",
        "    # We need to reload the original data to get dates back\n",
        "    pipeline = WalmartPreprocessingPipeline()\n",
        "    train_full = pipeline.load_and_prepare_data()\n",
        "    train_full = pipeline.clean_merged_data(train_full)\n",
        "    train_data, val_data, _ = pipeline.create_temporal_split(train_full)\n",
        "\n",
        "    print(f\"   üìà Train data shape: {train_data.shape}\")\n",
        "    print(f\"   üìâ Val data shape: {val_data.shape}\")\n",
        "\n",
        "    # Get unique Store-Dept combinations\n",
        "    train_combinations = set(zip(train_data['Store'], train_data['Dept']))\n",
        "    val_combinations = set(zip(val_data['Store'], val_data['Dept']))\n",
        "\n",
        "    print(f\"   üè™ Train combinations: {len(train_combinations)}\")\n",
        "    print(f\"   üîÆ Val combinations: {len(val_combinations)}\")\n",
        "\n",
        "    # Find missing combinations in validation\n",
        "    missing_in_val = train_combinations - val_combinations\n",
        "    missing_in_train = val_combinations - train_combinations\n",
        "\n",
        "    print(f\"   ‚ö†Ô∏è Missing in validation: {len(missing_in_val)}\")\n",
        "    print(f\"   ‚ö†Ô∏è Missing in training: {len(missing_in_train)}\")\n",
        "\n",
        "    return train_data, val_data, train_combinations, val_combinations\n"
      ],
      "metadata": {
        "id": "HkPn2P0o2Vm2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def auto_sarimax_order(ts_data, exog_data=None, max_p=2, max_d=1, max_q=2, max_P=1, max_D=1, max_Q=1):\n",
        "    \"\"\"Automatically determine SARIMAX order using AIC - SPEED OPTIMIZED\"\"\"\n",
        "    best_aic = float('inf')\n",
        "    best_order = (1, 1, 1)\n",
        "    best_seasonal_order = (1, 1, 1, 52)  # Weekly seasonality\n",
        "\n",
        "    # Limited grid search for speed\n",
        "    for p in range(max_p + 1):\n",
        "        for d in range(max_d + 1):\n",
        "            for q in range(max_q + 1):\n",
        "                for P in range(max_P + 1):\n",
        "                    for D in range(max_D + 1):\n",
        "                        for Q in range(max_Q + 1):\n",
        "                            try:\n",
        "                                model = SARIMAX(ts_data,\n",
        "                                              exog=exog_data,\n",
        "                                              order=(p, d, q),\n",
        "                                              seasonal_order=(P, D, Q, 52))\n",
        "                                fitted_model = model.fit(disp=False)\n",
        "                                if fitted_model.aic < best_aic:\n",
        "                                    best_aic = fitted_model.aic\n",
        "                                    best_order = (p, d, q)\n",
        "                                    best_seasonal_order = (P, D, Q, 52)\n",
        "                            except:\n",
        "                                continue\n",
        "\n",
        "    return best_order, best_seasonal_order\n",
        "\n"
      ],
      "metadata": {
        "id": "V9YtWEMu2YPH"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "r0e0t_YP5ODt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_exog_features():\n",
        "    \"\"\"Define external regressor features for SARIMAX - ULTRA OPTIMIZED FOR SPEED\"\"\"\n",
        "    # Use only the most important features to reduce complexity\n",
        "    return [\n",
        "        'IsHoliday',\n",
        "        'Type_Encoded',\n",
        "        'Month'\n",
        "    ]\n",
        "\n"
      ],
      "metadata": {
        "id": "wBBDWntX2cQO"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_sarimax_models(train_data, val_data, feature_columns, max_models=None):\n",
        "    \"\"\"Train individual SARIMAX models for each Store-Dept combination - ULTRA SPEED OPTIMIZED\"\"\"\n",
        "    print(\"üìà Training SARIMAX models for each Store-Dept combination (ULTRA SPEED OPTIMIZED)...\")\n",
        "\n",
        "    # Get unique combinations from training data\n",
        "    train_combinations = train_data.groupby(['Store', 'Dept']).size().index.tolist()\n",
        "\n",
        "    # Option to limit models for faster testing\n",
        "    if max_models and max_models < len(train_combinations):\n",
        "        train_combinations = train_combinations[:max_models]\n",
        "        print(f\"   ‚ö° TESTING MODE: Limited to first {max_models} combinations for speed\")\n",
        "\n",
        "    print(f\"   üìä Training models for {len(train_combinations)} combinations\")\n",
        "\n",
        "    models = {}\n",
        "    training_errors = {}\n",
        "    model_orders = {}\n",
        "    seasonal_orders = {}\n",
        "    exog_features = get_exog_features()\n",
        "\n",
        "    successful_models = 0\n",
        "    failed_models = 0\n",
        "    skipped_models = 0\n",
        "\n",
        "    # Speed monitoring\n",
        "    start_time = time.time()\n",
        "    last_update_time = start_time\n",
        "\n",
        "    print(f\"   üéØ Using external regressors: {exog_features}\")\n",
        "    print(f\"   ‚ö° EXTREME SPEED MODE: No seasonality, minimal ARIMA, ultra-fast fitting\")\n",
        "\n",
        "    for i, (store, dept) in enumerate(train_combinations):\n",
        "        try:\n",
        "            # Filter data for this combination\n",
        "            store_dept_data = train_data[\n",
        "                (train_data['Store'] == store) &\n",
        "                (train_data['Dept'] == dept)\n",
        "            ].copy()\n",
        "\n",
        "            # Skip if insufficient data - reduced requirement for speed\n",
        "            if len(store_dept_data) < 10:  # Reduced from 20 for speed\n",
        "                skipped_models += 1\n",
        "                # Progress updates every 10 processed combinations (including skipped)\n",
        "                if (i + 1) % 10 == 0:\n",
        "                    current_time = time.time()\n",
        "                    elapsed = current_time - start_time\n",
        "                    speed = (i + 1) / elapsed if elapsed > 0 else 0\n",
        "                    print(f\"   üìä Processed {i+1}/{len(train_combinations)} combinations | ‚úÖ Trained: {successful_models} | ‚ö†Ô∏è Skipped: {skipped_models} | ‚ùå Failed: {failed_models} | ‚ö° Speed: {speed:.1f} comb/sec\")\n",
        "                continue\n",
        "\n",
        "            # Sort by date and create time series\n",
        "            store_dept_data = store_dept_data.sort_values('Date').reset_index(drop=True)\n",
        "            ts_data = store_dept_data['Weekly_Sales'].values\n",
        "\n",
        "            # Prepare external regressors - SIMPLIFIED\n",
        "            exog_data = None\n",
        "            if all(feat in store_dept_data.columns for feat in exog_features):\n",
        "                exog_data = store_dept_data[exog_features].values\n",
        "                # Simple forward fill only\n",
        "                exog_data = pd.DataFrame(exog_data, columns=exog_features).fillna(method='ffill').values\n",
        "\n",
        "            # EXTREME SPEED: Use ARIMAX instead of SARIMAX (no seasonality)\n",
        "            # This should be 5-10x faster than seasonal models\n",
        "            sarimax_order = (1, 1, 0)  # Minimal ARIMA: AR(1), I(1), no MA\n",
        "            seasonal_order = (0, 0, 0, 0)  # NO SEASONALITY for maximum speed\n",
        "\n",
        "            # Fit ARIMAX model (SARIMAX with no seasonal component)\n",
        "            model = SARIMAX(ts_data,\n",
        "                           exog=exog_data,\n",
        "                           order=sarimax_order,\n",
        "                           seasonal_order=seasonal_order,\n",
        "                           enforce_stationarity=False,\n",
        "                           enforce_invertibility=False,\n",
        "                           concentrate_scale=True,\n",
        "                           trend='c')  # Just constant trend\n",
        "\n",
        "            # EXTREME speed fitting\n",
        "            fitted_model = model.fit(disp=False,\n",
        "                                   maxiter=10,  # Reduced from 25\n",
        "                                   method='lbfgs',\n",
        "                                   low_memory=True,\n",
        "                                   warn_convergence=False)  # Suppress convergence warnings\n",
        "\n",
        "            models[(store, dept)] = fitted_model\n",
        "            model_orders[(store, dept)] = sarimax_order\n",
        "            seasonal_orders[(store, dept)] = seasonal_order\n",
        "            successful_models += 1\n",
        "\n",
        "            # Skip all training error calculations for maximum speed\n",
        "\n",
        "            # Progress updates every 10 processed combinations\n",
        "            if (i + 1) % 10 == 0:\n",
        "                current_time = time.time()\n",
        "                elapsed = current_time - start_time\n",
        "                speed = (i + 1) / elapsed if elapsed > 0 else 0\n",
        "                print(f\"   üìä Processed {i+1}/{len(train_combinations)} combinations | ‚úÖ Trained: {successful_models} | ‚ö†Ô∏è Skipped: {skipped_models} | ‚ùå Failed: {failed_models} | ‚ö° Speed: {speed:.1f} comb/sec\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_models += 1\n",
        "            if failed_models < 3:  # Only print first few errors\n",
        "                print(f\"   ‚ö†Ô∏è Failed to train model for Store {store}, Dept {dept}: {e}\")\n",
        "\n",
        "            # Progress updates every 10 processed combinations\n",
        "            if (i + 1) % 10 == 0:\n",
        "                current_time = time.time()\n",
        "                elapsed = current_time - start_time\n",
        "                speed = (i + 1) / elapsed if elapsed > 0 else 0\n",
        "                print(f\"   üìä Processed {i+1}/{len(train_combinations)} combinations | ‚úÖ Trained: {successful_models} | ‚ö†Ô∏è Skipped: {skipped_models} | ‚ùå Failed: {failed_models} | ‚ö° Speed: {speed:.1f} comb/sec\")\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(f\"‚úÖ SARIMAX training complete in {total_time:.1f} seconds!\")\n",
        "    print(f\"   üéØ Successful models: {successful_models}\")\n",
        "    print(f\"   ‚ö†Ô∏è Skipped models (insufficient data): {skipped_models}\")\n",
        "    print(f\"   ‚ùå Failed models: {failed_models}\")\n",
        "    print(f\"   ‚ö° Average speed: {len(train_combinations)/total_time:.1f} combinations/sec\")\n",
        "\n",
        "    return models, training_errors, model_orders, seasonal_orders\n",
        "\n"
      ],
      "metadata": {
        "id": "qK3mkwQs2eCt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_sarimax_predictions(models, val_data, model_orders, seasonal_orders):\n",
        "    \"\"\"Make predictions using trained SARIMAX models\"\"\"\n",
        "    print(\"üìà Making SARIMAX predictions...\")\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    holidays = []\n",
        "    successful_predictions = 0\n",
        "    failed_predictions = 0\n",
        "    exog_features = get_exog_features()\n",
        "\n",
        "    # Get validation combinations\n",
        "    val_combinations = val_data.groupby(['Store', 'Dept']).groups.keys()\n",
        "\n",
        "    for store, dept in val_combinations:\n",
        "        try:\n",
        "            # Check if we have a model for this combination\n",
        "            if (store, dept) not in models:\n",
        "                # Use overall mean as fallback\n",
        "                fallback_pred = val_data['Weekly_Sales'].mean()\n",
        "                store_dept_val = val_data[\n",
        "                    (val_data['Store'] == store) &\n",
        "                    (val_data['Dept'] == dept)\n",
        "                ]\n",
        "                predictions.extend([fallback_pred] * len(store_dept_val))\n",
        "                actuals.extend(store_dept_val['Weekly_Sales'].tolist())\n",
        "                holidays.extend(store_dept_val['IsHoliday'].tolist())\n",
        "                failed_predictions += len(store_dept_val)\n",
        "                continue\n",
        "\n",
        "            # Get validation data for this combination\n",
        "            store_dept_val = val_data[\n",
        "                (val_data['Store'] == store) &\n",
        "                (val_data['Dept'] == dept)\n",
        "            ].copy()\n",
        "\n",
        "            # Sort by date\n",
        "            store_dept_val = store_dept_val.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "            # Prepare external regressors for prediction\n",
        "            exog_forecast = None\n",
        "            if all(feat in store_dept_val.columns for feat in exog_features):\n",
        "                exog_forecast = store_dept_val[exog_features].values\n",
        "                # Handle missing values\n",
        "                exog_forecast = pd.DataFrame(exog_forecast, columns=exog_features).fillna(method='ffill').fillna(method='bfill').values\n",
        "\n",
        "            # Make prediction using the fitted model\n",
        "            model = models[(store, dept)]\n",
        "            n_periods = len(store_dept_val)\n",
        "\n",
        "            # Forecast future values\n",
        "            forecast = model.forecast(steps=n_periods, exog=exog_forecast)\n",
        "\n",
        "            # Store results\n",
        "            predictions.extend(forecast.tolist())\n",
        "            actuals.extend(store_dept_val['Weekly_Sales'].tolist())\n",
        "            holidays.extend(store_dept_val['IsHoliday'].tolist())\n",
        "            successful_predictions += len(store_dept_val)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Fallback prediction\n",
        "            fallback_pred = val_data['Weekly_Sales'].mean()\n",
        "            store_dept_val = val_data[\n",
        "                (val_data['Store'] == store) &\n",
        "                (val_data['Dept'] == dept)\n",
        "            ]\n",
        "            predictions.extend([fallback_pred] * len(store_dept_val))\n",
        "            actuals.extend(store_dept_val['Weekly_Sales'].tolist())\n",
        "            holidays.extend(store_dept_val['IsHoliday'].tolist())\n",
        "            failed_predictions += len(store_dept_val)\n",
        "\n",
        "    print(f\"‚úÖ Predictions complete!\")\n",
        "    print(f\"   üéØ Successful predictions: {successful_predictions}\")\n",
        "    print(f\"   ‚ùå Failed/fallback predictions: {failed_predictions}\")\n",
        "\n",
        "    return np.array(predictions), np.array(actuals), np.array(holidays)\n",
        "\n"
      ],
      "metadata": {
        "id": "1i-rjv632fia"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main experiment execution\"\"\"\n",
        "    print(\"üöÄ Starting Experiment 11: SARIMAX with Experiment 7 Features\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Setup MLflow tracking\n",
        "    experiment_name = setup_mlflow()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"SARIMAX_Exp7_Features_Complete\") as run:\n",
        "        print(f\"üîÑ Starting MLflow run: {run.info.run_id}\")\n",
        "\n",
        "        # Log experiment metadata\n",
        "        mlflow.log_param(\"experiment_type\", \"SARIMAX_with_Exp7_Features\")\n",
        "        mlflow.log_param(\"model_type\", \"SARIMAX_Individual_Models\")\n",
        "        mlflow.log_param(\"feature_engineering\", \"Experiment_7_Pipeline\")\n",
        "        mlflow.log_param(\"data_split\", \"temporal_80_20\")\n",
        "        mlflow.log_param(\"external_regressors\", str(get_exog_features()))\n",
        "        mlflow.log_param(\"seasonality\", \"weekly_52\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Get preprocessed data\n",
        "            print(\"\\nüìä Step 1: Data preprocessing...\")\n",
        "            X_train, y_train, X_val, y_val, train_holidays, val_holidays, split_info, feature_columns = get_preprocessed_data()\n",
        "\n",
        "            # Log data info\n",
        "            mlflow.log_metric(\"train_samples\", len(X_train))\n",
        "            mlflow.log_metric(\"val_samples\", len(X_val))\n",
        "            mlflow.log_metric(\"total_features\", len(feature_columns))\n",
        "            mlflow.log_param(\"split_date\", str(split_info['split_date']))\n",
        "\n",
        "            # Step 2: Prepare SARIMAX-specific data\n",
        "            print(\"\\nüìä Step 2: Preparing SARIMAX data...\")\n",
        "            train_data, val_data, train_combinations, val_combinations = prepare_sarimax_data(\n",
        "                X_train, y_train, X_val, y_val, train_holidays, val_holidays\n",
        "            )\n",
        "\n",
        "            # Log combination info\n",
        "            mlflow.log_metric(\"train_combinations\", len(train_combinations))\n",
        "            mlflow.log_metric(\"val_combinations\", len(val_combinations))\n",
        "            mlflow.log_metric(\"missing_combinations\", len(train_combinations - val_combinations))\n",
        "\n",
        "            # Step 3: Train SARIMAX models\n",
        "            print(\"\\nüìà Step 3: Training SARIMAX models...\")\n",
        "\n",
        "            # OPTION: Limit models for faster testing (uncomment next line for testing)\n",
        "            max_models_for_testing = None  # Test with only 100 models for speed\n",
        "            # max_models_for_testing = None   # Use None for full training\n",
        "\n",
        "            models, training_errors, model_orders, seasonal_orders = train_sarimax_models(\n",
        "                train_data, val_data, feature_columns, max_models=max_models_for_testing\n",
        "            )\n",
        "\n",
        "            # Log training info\n",
        "            mlflow.log_metric(\"successful_models\", len(models))\n",
        "            mlflow.log_metric(\"avg_training_mae\", np.mean(list(training_errors.values())) if training_errors else 0)\n",
        "            mlflow.log_metric(\"num_models\", len(models))\n",
        "\n",
        "            # Step 4: Make predictions\n",
        "            print(\"\\nüìà Step 4: Making predictions...\")\n",
        "            y_pred, y_true, is_holiday = make_sarimax_predictions(\n",
        "                models, val_data, model_orders, seasonal_orders\n",
        "            )\n",
        "\n",
        "            # Step 5: Calculate metrics\n",
        "            print(\"\\nüìä Step 5: Calculating metrics...\")\n",
        "\n",
        "            # Standard metrics\n",
        "            mae = mean_absolute_error(y_true, y_pred)\n",
        "            rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "            # WMAE (Competition metric)\n",
        "            wmae = calculate_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "            # Holiday vs non-holiday breakdown\n",
        "            holiday_mask = is_holiday == True\n",
        "            non_holiday_mask = is_holiday == False\n",
        "\n",
        "            holiday_mae = mean_absolute_error(y_true[holiday_mask], y_pred[holiday_mask]) if holiday_mask.sum() > 0 else 0\n",
        "            non_holiday_mae = mean_absolute_error(y_true[non_holiday_mask], y_pred[non_holiday_mask]) if non_holiday_mask.sum() > 0 else 0\n",
        "\n",
        "            # Log all metrics\n",
        "            mlflow.log_metric(\"mae\", mae)\n",
        "            mlflow.log_metric(\"rmse\", rmse)\n",
        "            mlflow.log_metric(\"r2_score\", r2)\n",
        "            mlflow.log_metric(\"wmae\", wmae)\n",
        "            mlflow.log_metric(\"holiday_mae\", holiday_mae)\n",
        "            mlflow.log_metric(\"non_holiday_mae\", non_holiday_mae)\n",
        "            mlflow.log_metric(\"holiday_samples\", holiday_mask.sum())\n",
        "            mlflow.log_metric(\"non_holiday_samples\", non_holiday_mask.sum())\n",
        "\n",
        "            # Step 6: Results summary\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"üéØ EXPERIMENT 11 RESULTS SUMMARY\")\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"üìä Validation Metrics:\")\n",
        "            print(f\"   WMAE (Competition Metric): ${wmae:,.2f}\")\n",
        "            print(f\"   MAE: ${mae:,.2f}\")\n",
        "            print(f\"   RMSE: ${rmse:,.2f}\")\n",
        "            print(f\"   R¬≤: {r2:.4f}\")\n",
        "            print(f\"\\nüìä Holiday Breakdown:\")\n",
        "            print(f\"   Holiday MAE: ${holiday_mae:,.2f} ({holiday_mask.sum():,} samples)\")\n",
        "            print(f\"   Non-Holiday MAE: ${non_holiday_mae:,.2f} ({non_holiday_mask.sum():,} samples)\")\n",
        "            print(f\"\\nüìä Model Statistics:\")\n",
        "            print(f\"   Successful models trained: {len(models):,}\")\n",
        "            print(f\"   Store-Dept combinations: {len(train_combinations):,}\")\n",
        "            print(f\"   External regressors: {len(get_exog_features())}\")\n",
        "            print(f\"   Features used: {len(feature_columns):,}\")\n",
        "\n",
        "            # Step 7: Save artifacts\n",
        "            print(f\"\\nüíæ Saving model artifacts...\")\n",
        "\n",
        "            # Save model summary\n",
        "            model_summary = {\n",
        "                'experiment_name': experiment_name,\n",
        "                'run_id': run.info.run_id,\n",
        "                'models_trained': len(models),\n",
        "                'feature_columns': feature_columns,\n",
        "                'external_regressors': get_exog_features(),\n",
        "                'model_orders': {f\"{k[0]}_{k[1]}\": v for k, v in model_orders.items()},\n",
        "                'seasonal_orders': {f\"{k[0]}_{k[1]}\": v for k, v in seasonal_orders.items()},\n",
        "                'metrics': {\n",
        "                    'wmae': wmae,\n",
        "                    'mae': mae,\n",
        "                    'rmse': rmse,\n",
        "                    'r2': r2\n",
        "                }\n",
        "            }\n",
        "\n",
        "            with open('sarimax_exp11_summary.json', 'w') as f:\n",
        "                json.dump(model_summary, f, indent=2, default=str)\n",
        "\n",
        "            mlflow.log_artifact('sarimax_exp11_summary.json')\n",
        "\n",
        "            print(\"‚úÖ Experiment 11 completed successfully!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Experiment failed: {e}\")\n",
        "            mlflow.log_param(\"error\", str(e))\n",
        "            raise\n",
        "\n",
        "    print(\"\\nüéâ Experiment 11: SARIMAX with Experiment 7 Features - COMPLETE!\")\n"
      ],
      "metadata": {
        "id": "BWVhJ0R32hNK"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "fMIqQJhA2jgk",
        "outputId": "8ebb57e3-2d30-4af5-94b4-a2ceb5e871c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Experiment 11: SARIMAX with Experiment 7 Features\n",
            "================================================================================\n",
            "üîß Setting up MLflow and DagsHub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DagsHub initialized successfully!\n",
            "‚úÖ Created new experiment: Experiment_11_SARIMAX_20250710_152458\n",
            "‚úÖ MLflow setup complete!\n",
            "üîó Tracking URI: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\n",
            "üìä Experiment: Experiment_11_SARIMAX_20250710_152458\n",
            "üîÑ Starting MLflow run: 4f7a4e33667a430f9e993993cdcb28db\n",
            "\n",
            "üìä Step 1: Data preprocessing...\n",
            "üîÑ Getting preprocessed data using pipeline...\n",
            "üìä Loading datasets...\n",
            "   üìà Train data: (421570, 5)\n",
            "   üè™ Stores data: (45, 3)\n",
            "   üéØ Features data: (8190, 12)\n",
            "   ‚úÖ Merged data: (421570, 17)\n",
            "   üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üßπ Cleaning merged data...\n",
            "   üîÑ Resolving duplicate IsHoliday columns...\n",
            "   ‚úÖ Cleaned data: (421570, 16) (was (421570, 17))\n",
            "üìÖ Creating temporal split (80/19)...\n",
            "   üìä Split date: 2012-04-13 00:00:00\n",
            "   üìà Train: 337,256 records (2010-02-05 00:00:00 to 2012-04-13 00:00:00)\n",
            "   üìâ Val: 84,314 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n",
            "üîß Fitting preprocessing pipeline on training data...\n",
            "‚úÖ Pipeline fitted on training data\n",
            "üîÑ Transforming training data...\n",
            "   üîß Encoding Type column using both one-hot and label encoding...\n",
            "   ‚úÖ Added both Type_Encoded and ['Type_A', 'Type_B', 'Type_C']\n",
            "   üóëÔ∏è Removed 45,193 outliers from training data\n",
            "‚úÖ Transform complete. Shape: (292063, 27)\n",
            "üîÑ Transforming validation data...\n",
            "   üîß Encoding Type column using both one-hot and label encoding...\n",
            "   ‚úÖ Added both Type_Encoded and ['Type_A', 'Type_B', 'Type_C']\n",
            "‚úÖ Transform complete. Shape: (84314, 26)\n",
            "‚úÖ Data preprocessing complete!\n",
            "   üìä Training shape: (292063, 25)\n",
            "   üìä Validation shape: (84314, 25)\n",
            "   üéØ Features: 25\n",
            "\n",
            "üìä Step 2: Preparing SARIMAX data...\n",
            "üìä Preparing data for SARIMAX modeling...\n",
            "üìä Loading datasets...\n",
            "   üìà Train data: (421570, 5)\n",
            "   üè™ Stores data: (45, 3)\n",
            "   üéØ Features data: (8190, 12)\n",
            "   ‚úÖ Merged data: (421570, 17)\n",
            "   üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üßπ Cleaning merged data...\n",
            "   üîÑ Resolving duplicate IsHoliday columns...\n",
            "   ‚úÖ Cleaned data: (421570, 16) (was (421570, 17))\n",
            "üìÖ Creating temporal split (80/19)...\n",
            "   üìä Split date: 2012-04-13 00:00:00\n",
            "   üìà Train: 337,256 records (2010-02-05 00:00:00 to 2012-04-13 00:00:00)\n",
            "   üìâ Val: 84,314 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n",
            "   üìà Train data shape: (337256, 16)\n",
            "   üìâ Val data shape: (84314, 16)\n",
            "   üè™ Train combinations: 3313\n",
            "   üîÆ Val combinations: 3166\n",
            "   ‚ö†Ô∏è Missing in validation: 165\n",
            "   ‚ö†Ô∏è Missing in training: 18\n",
            "\n",
            "üìà Step 3: Training SARIMAX models...\n",
            "üìà Training SARIMAX models for each Store-Dept combination (ULTRA SPEED OPTIMIZED)...\n",
            "   üìä Training models for 3313 combinations\n",
            "   üéØ Using external regressors: ['IsHoliday', 'Type_Encoded', 'Month']\n",
            "   ‚ö° EXTREME SPEED MODE: No seasonality, minimal ARIMA, ultra-fast fitting\n",
            "   üìä Processed 10/3313 combinations | ‚úÖ Trained: 10 | ‚ö†Ô∏è Skipped: 0 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 20/3313 combinations | ‚úÖ Trained: 20 | ‚ö†Ô∏è Skipped: 0 | ‚ùå Failed: 0 | ‚ö° Speed: 70.9 comb/sec\n",
            "   üìä Processed 30/3313 combinations | ‚úÖ Trained: 30 | ‚ö†Ô∏è Skipped: 0 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 40/3313 combinations | ‚úÖ Trained: 40 | ‚ö†Ô∏è Skipped: 0 | ‚ùå Failed: 0 | ‚ö° Speed: 73.2 comb/sec\n",
            "   üìä Processed 50/3313 combinations | ‚úÖ Trained: 50 | ‚ö†Ô∏è Skipped: 0 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 60/3313 combinations | ‚úÖ Trained: 58 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 77.0 comb/sec\n",
            "   üìä Processed 70/3313 combinations | ‚úÖ Trained: 68 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 78.5 comb/sec\n",
            "   üìä Processed 80/3313 combinations | ‚úÖ Trained: 78 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 79.9 comb/sec\n",
            "   üìä Processed 90/3313 combinations | ‚úÖ Trained: 88 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 81.3 comb/sec\n",
            "   üìä Processed 100/3313 combinations | ‚úÖ Trained: 98 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 82.1 comb/sec\n",
            "   üìä Processed 110/3313 combinations | ‚úÖ Trained: 108 | ‚ö†Ô∏è Skipped: 2 | ‚ùå Failed: 0 | ‚ö° Speed: 82.1 comb/sec\n",
            "   üìä Processed 120/3313 combinations | ‚úÖ Trained: 117 | ‚ö†Ô∏è Skipped: 3 | ‚ùå Failed: 0 | ‚ö° Speed: 83.5 comb/sec\n",
            "   üìä Processed 130/3313 combinations | ‚úÖ Trained: 127 | ‚ö†Ô∏è Skipped: 3 | ‚ùå Failed: 0 | ‚ö° Speed: 84.2 comb/sec\n",
            "   üìä Processed 140/3313 combinations | ‚úÖ Trained: 135 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 85.3 comb/sec\n",
            "   üìä Processed 150/3313 combinations | ‚úÖ Trained: 145 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 85.4 comb/sec\n",
            "   üìä Processed 160/3313 combinations | ‚úÖ Trained: 155 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 85.6 comb/sec\n",
            "   üìä Processed 170/3313 combinations | ‚úÖ Trained: 165 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 85.8 comb/sec\n",
            "   üìä Processed 180/3313 combinations | ‚úÖ Trained: 175 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 86.2 comb/sec\n",
            "   üìä Processed 190/3313 combinations | ‚úÖ Trained: 185 | ‚ö†Ô∏è Skipped: 5 | ‚ùå Failed: 0 | ‚ö° Speed: 86.5 comb/sec\n",
            "   üìä Processed 200/3313 combinations | ‚úÖ Trained: 194 | ‚ö†Ô∏è Skipped: 6 | ‚ùå Failed: 0 | ‚ö° Speed: 87.5 comb/sec\n",
            "   üìä Processed 210/3313 combinations | ‚úÖ Trained: 204 | ‚ö†Ô∏è Skipped: 6 | ‚ùå Failed: 0 | ‚ö° Speed: 87.1 comb/sec\n",
            "   üìä Processed 220/3313 combinations | ‚úÖ Trained: 212 | ‚ö†Ô∏è Skipped: 8 | ‚ùå Failed: 0 | ‚ö° Speed: 87.9 comb/sec\n",
            "   üìä Processed 230/3313 combinations | ‚úÖ Trained: 222 | ‚ö†Ô∏è Skipped: 8 | ‚ùå Failed: 0 | ‚ö° Speed: 88.1 comb/sec\n",
            "   üìä Processed 240/3313 combinations | ‚úÖ Trained: 232 | ‚ö†Ô∏è Skipped: 8 | ‚ùå Failed: 0 | ‚ö° Speed: 88.2 comb/sec\n",
            "   üìä Processed 250/3313 combinations | ‚úÖ Trained: 242 | ‚ö†Ô∏è Skipped: 8 | ‚ùå Failed: 0 | ‚ö° Speed: 88.3 comb/sec\n",
            "   üìä Processed 260/3313 combinations | ‚úÖ Trained: 252 | ‚ö†Ô∏è Skipped: 8 | ‚ùå Failed: 0 | ‚ö° Speed: 88.4 comb/sec\n",
            "   üìä Processed 270/3313 combinations | ‚úÖ Trained: 261 | ‚ö†Ô∏è Skipped: 9 | ‚ùå Failed: 0 | ‚ö° Speed: 88.9 comb/sec\n",
            "   üìä Processed 280/3313 combinations | ‚úÖ Trained: 271 | ‚ö†Ô∏è Skipped: 9 | ‚ùå Failed: 0 | ‚ö° Speed: 88.8 comb/sec\n",
            "   üìä Processed 290/3313 combinations | ‚úÖ Trained: 279 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.4 comb/sec\n",
            "   üìä Processed 300/3313 combinations | ‚úÖ Trained: 289 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.5 comb/sec\n",
            "   üìä Processed 310/3313 combinations | ‚úÖ Trained: 299 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.1 comb/sec\n",
            "   üìä Processed 320/3313 combinations | ‚úÖ Trained: 309 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.2 comb/sec\n",
            "   üìä Processed 330/3313 combinations | ‚úÖ Trained: 319 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.4 comb/sec\n",
            "   üìä Processed 340/3313 combinations | ‚úÖ Trained: 329 | ‚ö†Ô∏è Skipped: 11 | ‚ùå Failed: 0 | ‚ö° Speed: 89.4 comb/sec\n",
            "   üìä Processed 350/3313 combinations | ‚úÖ Trained: 338 | ‚ö†Ô∏è Skipped: 12 | ‚ùå Failed: 0 | ‚ö° Speed: 89.5 comb/sec\n",
            "   üìä Processed 360/3313 combinations | ‚úÖ Trained: 348 | ‚ö†Ô∏è Skipped: 12 | ‚ùå Failed: 0 | ‚ö° Speed: 89.1 comb/sec\n",
            "   üìä Processed 370/3313 combinations | ‚úÖ Trained: 355 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 89.3 comb/sec\n",
            "   üìä Processed 380/3313 combinations | ‚úÖ Trained: 365 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 89.0 comb/sec\n",
            "   üìä Processed 390/3313 combinations | ‚úÖ Trained: 375 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 88.4 comb/sec\n",
            "   üìä Processed 400/3313 combinations | ‚úÖ Trained: 385 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 86.7 comb/sec\n",
            "   üìä Processed 410/3313 combinations | ‚úÖ Trained: 395 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 85.2 comb/sec\n",
            "   üìä Processed 420/3313 combinations | ‚úÖ Trained: 405 | ‚ö†Ô∏è Skipped: 15 | ‚ùå Failed: 0 | ‚ö° Speed: 84.3 comb/sec\n",
            "   üìä Processed 430/3313 combinations | ‚úÖ Trained: 414 | ‚ö†Ô∏è Skipped: 16 | ‚ùå Failed: 0 | ‚ö° Speed: 84.1 comb/sec\n",
            "   üìä Processed 440/3313 combinations | ‚úÖ Trained: 422 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 83.9 comb/sec\n",
            "   üìä Processed 450/3313 combinations | ‚úÖ Trained: 432 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 78.8 comb/sec\n",
            "   üìä Processed 460/3313 combinations | ‚úÖ Trained: 442 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 78.4 comb/sec\n",
            "   üìä Processed 470/3313 combinations | ‚úÖ Trained: 452 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 77.8 comb/sec\n",
            "   üìä Processed 480/3313 combinations | ‚úÖ Trained: 462 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 77.3 comb/sec\n",
            "   üìä Processed 490/3313 combinations | ‚úÖ Trained: 472 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 76.9 comb/sec\n",
            "   üìä Processed 500/3313 combinations | ‚úÖ Trained: 482 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 76.8 comb/sec\n",
            "   üìä Processed 510/3313 combinations | ‚úÖ Trained: 492 | ‚ö†Ô∏è Skipped: 18 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 520/3313 combinations | ‚úÖ Trained: 500 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 530/3313 combinations | ‚úÖ Trained: 510 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 75.9 comb/sec\n",
            "   üìä Processed 540/3313 combinations | ‚úÖ Trained: 520 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 550/3313 combinations | ‚úÖ Trained: 530 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 560/3313 combinations | ‚úÖ Trained: 540 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 570/3313 combinations | ‚úÖ Trained: 550 | ‚ö†Ô∏è Skipped: 20 | ‚ùå Failed: 0 | ‚ö° Speed: 73.1 comb/sec\n",
            "   üìä Processed 580/3313 combinations | ‚úÖ Trained: 559 | ‚ö†Ô∏è Skipped: 21 | ‚ùå Failed: 0 | ‚ö° Speed: 72.6 comb/sec\n",
            "   üìä Processed 590/3313 combinations | ‚úÖ Trained: 568 | ‚ö†Ô∏è Skipped: 22 | ‚ùå Failed: 0 | ‚ö° Speed: 72.1 comb/sec\n",
            "   üìä Processed 600/3313 combinations | ‚úÖ Trained: 578 | ‚ö†Ô∏è Skipped: 22 | ‚ùå Failed: 0 | ‚ö° Speed: 71.5 comb/sec\n",
            "   üìä Processed 610/3313 combinations | ‚úÖ Trained: 587 | ‚ö†Ô∏è Skipped: 23 | ‚ùå Failed: 0 | ‚ö° Speed: 71.1 comb/sec\n",
            "   üìä Processed 620/3313 combinations | ‚úÖ Trained: 597 | ‚ö†Ô∏è Skipped: 23 | ‚ùå Failed: 0 | ‚ö° Speed: 70.7 comb/sec\n",
            "   üìä Processed 630/3313 combinations | ‚úÖ Trained: 607 | ‚ö†Ô∏è Skipped: 23 | ‚ùå Failed: 0 | ‚ö° Speed: 70.6 comb/sec\n",
            "   üìä Processed 640/3313 combinations | ‚úÖ Trained: 617 | ‚ö†Ô∏è Skipped: 23 | ‚ùå Failed: 0 | ‚ö° Speed: 70.8 comb/sec\n",
            "   üìä Processed 650/3313 combinations | ‚úÖ Trained: 626 | ‚ö†Ô∏è Skipped: 24 | ‚ùå Failed: 0 | ‚ö° Speed: 71.2 comb/sec\n",
            "   üìä Processed 660/3313 combinations | ‚úÖ Trained: 636 | ‚ö†Ô∏è Skipped: 24 | ‚ùå Failed: 0 | ‚ö° Speed: 71.5 comb/sec\n",
            "   üìä Processed 670/3313 combinations | ‚úÖ Trained: 644 | ‚ö†Ô∏è Skipped: 26 | ‚ùå Failed: 0 | ‚ö° Speed: 71.9 comb/sec\n",
            "   üìä Processed 680/3313 combinations | ‚úÖ Trained: 652 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 72.3 comb/sec\n",
            "   üìä Processed 690/3313 combinations | ‚úÖ Trained: 662 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 72.5 comb/sec\n",
            "   üìä Processed 700/3313 combinations | ‚úÖ Trained: 672 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 72.6 comb/sec\n",
            "   üìä Processed 710/3313 combinations | ‚úÖ Trained: 682 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 720/3313 combinations | ‚úÖ Trained: 692 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 73.0 comb/sec\n",
            "   üìä Processed 730/3313 combinations | ‚úÖ Trained: 702 | ‚ö†Ô∏è Skipped: 28 | ‚ùå Failed: 0 | ‚ö° Speed: 73.2 comb/sec\n",
            "   üìä Processed 740/3313 combinations | ‚úÖ Trained: 710 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 750/3313 combinations | ‚úÖ Trained: 720 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 760/3313 combinations | ‚úÖ Trained: 730 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 770/3313 combinations | ‚úÖ Trained: 740 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 780/3313 combinations | ‚úÖ Trained: 750 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 790/3313 combinations | ‚úÖ Trained: 760 | ‚ö†Ô∏è Skipped: 30 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 800/3313 combinations | ‚úÖ Trained: 769 | ‚ö†Ô∏è Skipped: 31 | ‚ùå Failed: 0 | ‚ö° Speed: 74.6 comb/sec\n",
            "   üìä Processed 810/3313 combinations | ‚úÖ Trained: 778 | ‚ö†Ô∏è Skipped: 32 | ‚ùå Failed: 0 | ‚ö° Speed: 74.8 comb/sec\n",
            "   üìä Processed 820/3313 combinations | ‚úÖ Trained: 786 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 830/3313 combinations | ‚úÖ Trained: 796 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 840/3313 combinations | ‚úÖ Trained: 806 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 850/3313 combinations | ‚úÖ Trained: 816 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 860/3313 combinations | ‚úÖ Trained: 826 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.6 comb/sec\n",
            "   üìä Processed 870/3313 combinations | ‚úÖ Trained: 836 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.8 comb/sec\n",
            "   üìä Processed 880/3313 combinations | ‚úÖ Trained: 846 | ‚ö†Ô∏è Skipped: 34 | ‚ùå Failed: 0 | ‚ö° Speed: 75.9 comb/sec\n",
            "   üìä Processed 890/3313 combinations | ‚úÖ Trained: 854 | ‚ö†Ô∏è Skipped: 36 | ‚ùå Failed: 0 | ‚ö° Speed: 76.2 comb/sec\n",
            "   üìä Processed 900/3313 combinations | ‚úÖ Trained: 864 | ‚ö†Ô∏è Skipped: 36 | ‚ùå Failed: 0 | ‚ö° Speed: 76.3 comb/sec\n",
            "   üìä Processed 910/3313 combinations | ‚úÖ Trained: 873 | ‚ö†Ô∏è Skipped: 37 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 920/3313 combinations | ‚úÖ Trained: 883 | ‚ö†Ô∏è Skipped: 37 | ‚ùå Failed: 0 | ‚ö° Speed: 76.5 comb/sec\n",
            "   üìä Processed 930/3313 combinations | ‚úÖ Trained: 893 | ‚ö†Ô∏è Skipped: 37 | ‚ùå Failed: 0 | ‚ö° Speed: 76.6 comb/sec\n",
            "   üìä Processed 940/3313 combinations | ‚úÖ Trained: 903 | ‚ö†Ô∏è Skipped: 37 | ‚ùå Failed: 0 | ‚ö° Speed: 76.5 comb/sec\n",
            "   üìä Processed 950/3313 combinations | ‚úÖ Trained: 912 | ‚ö†Ô∏è Skipped: 38 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 960/3313 combinations | ‚úÖ Trained: 921 | ‚ö†Ô∏è Skipped: 39 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 970/3313 combinations | ‚úÖ Trained: 929 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 980/3313 combinations | ‚úÖ Trained: 939 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 990/3313 combinations | ‚úÖ Trained: 949 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 1000/3313 combinations | ‚úÖ Trained: 959 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 1010/3313 combinations | ‚úÖ Trained: 969 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 1020/3313 combinations | ‚úÖ Trained: 979 | ‚ö†Ô∏è Skipped: 41 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 1030/3313 combinations | ‚úÖ Trained: 988 | ‚ö†Ô∏è Skipped: 42 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 1040/3313 combinations | ‚úÖ Trained: 998 | ‚ö†Ô∏è Skipped: 42 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 1050/3313 combinations | ‚úÖ Trained: 1006 | ‚ö†Ô∏è Skipped: 44 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 1060/3313 combinations | ‚úÖ Trained: 1015 | ‚ö†Ô∏è Skipped: 45 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 1070/3313 combinations | ‚úÖ Trained: 1025 | ‚ö†Ô∏è Skipped: 45 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 1080/3313 combinations | ‚úÖ Trained: 1035 | ‚ö†Ô∏è Skipped: 45 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 1090/3313 combinations | ‚úÖ Trained: 1045 | ‚ö†Ô∏è Skipped: 45 | ‚ùå Failed: 0 | ‚ö° Speed: 74.6 comb/sec\n",
            "   üìä Processed 1100/3313 combinations | ‚úÖ Trained: 1054 | ‚ö†Ô∏è Skipped: 46 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 1110/3313 combinations | ‚úÖ Trained: 1062 | ‚ö†Ô∏è Skipped: 48 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 1120/3313 combinations | ‚úÖ Trained: 1072 | ‚ö†Ô∏è Skipped: 48 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 1130/3313 combinations | ‚úÖ Trained: 1081 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 1140/3313 combinations | ‚úÖ Trained: 1091 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1150/3313 combinations | ‚úÖ Trained: 1101 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1160/3313 combinations | ‚úÖ Trained: 1111 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1170/3313 combinations | ‚úÖ Trained: 1121 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1180/3313 combinations | ‚úÖ Trained: 1131 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1190/3313 combinations | ‚úÖ Trained: 1141 | ‚ö†Ô∏è Skipped: 49 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 1200/3313 combinations | ‚úÖ Trained: 1149 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1210/3313 combinations | ‚úÖ Trained: 1159 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1220/3313 combinations | ‚úÖ Trained: 1169 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1230/3313 combinations | ‚úÖ Trained: 1179 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1240/3313 combinations | ‚úÖ Trained: 1189 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1250/3313 combinations | ‚úÖ Trained: 1199 | ‚ö†Ô∏è Skipped: 51 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1260/3313 combinations | ‚úÖ Trained: 1208 | ‚ö†Ô∏è Skipped: 52 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 1270/3313 combinations | ‚úÖ Trained: 1218 | ‚ö†Ô∏è Skipped: 52 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 1280/3313 combinations | ‚úÖ Trained: 1226 | ‚ö†Ô∏è Skipped: 54 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 1290/3313 combinations | ‚úÖ Trained: 1236 | ‚ö†Ô∏è Skipped: 54 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 1300/3313 combinations | ‚úÖ Trained: 1246 | ‚ö†Ô∏è Skipped: 54 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 1310/3313 combinations | ‚úÖ Trained: 1256 | ‚ö†Ô∏è Skipped: 54 | ‚ùå Failed: 0 | ‚ö° Speed: 75.6 comb/sec\n",
            "   üìä Processed 1320/3313 combinations | ‚úÖ Trained: 1266 | ‚ö†Ô∏è Skipped: 54 | ‚ùå Failed: 0 | ‚ö° Speed: 75.7 comb/sec\n",
            "   üìä Processed 1330/3313 combinations | ‚úÖ Trained: 1275 | ‚ö†Ô∏è Skipped: 55 | ‚ùå Failed: 0 | ‚ö° Speed: 75.8 comb/sec\n",
            "   üìä Processed 1340/3313 combinations | ‚úÖ Trained: 1284 | ‚ö†Ô∏è Skipped: 56 | ‚ùå Failed: 0 | ‚ö° Speed: 76.0 comb/sec\n",
            "   üìä Processed 1350/3313 combinations | ‚úÖ Trained: 1293 | ‚ö†Ô∏è Skipped: 57 | ‚ùå Failed: 0 | ‚ö° Speed: 76.1 comb/sec\n",
            "   üìä Processed 1360/3313 combinations | ‚úÖ Trained: 1303 | ‚ö†Ô∏è Skipped: 57 | ‚ùå Failed: 0 | ‚ö° Speed: 76.2 comb/sec\n",
            "   üìä Processed 1370/3313 combinations | ‚úÖ Trained: 1312 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.3 comb/sec\n",
            "   üìä Processed 1380/3313 combinations | ‚úÖ Trained: 1322 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 1390/3313 combinations | ‚úÖ Trained: 1332 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 1400/3313 combinations | ‚úÖ Trained: 1342 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 1410/3313 combinations | ‚úÖ Trained: 1352 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 1420/3313 combinations | ‚úÖ Trained: 1362 | ‚ö†Ô∏è Skipped: 58 | ‚ùå Failed: 0 | ‚ö° Speed: 76.4 comb/sec\n",
            "   üìä Processed 1430/3313 combinations | ‚úÖ Trained: 1371 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 76.3 comb/sec\n",
            "   üìä Processed 1440/3313 combinations | ‚úÖ Trained: 1381 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 76.0 comb/sec\n",
            "   üìä Processed 1450/3313 combinations | ‚úÖ Trained: 1391 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.8 comb/sec\n",
            "   üìä Processed 1460/3313 combinations | ‚úÖ Trained: 1401 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.6 comb/sec\n",
            "   üìä Processed 1470/3313 combinations | ‚úÖ Trained: 1411 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 1480/3313 combinations | ‚úÖ Trained: 1421 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 1490/3313 combinations | ‚úÖ Trained: 1431 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1500/3313 combinations | ‚úÖ Trained: 1441 | ‚ö†Ô∏è Skipped: 59 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 1510/3313 combinations | ‚úÖ Trained: 1449 | ‚ö†Ô∏è Skipped: 61 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 1520/3313 combinations | ‚úÖ Trained: 1459 | ‚ö†Ô∏è Skipped: 61 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1530/3313 combinations | ‚úÖ Trained: 1468 | ‚ö†Ô∏è Skipped: 62 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 1540/3313 combinations | ‚úÖ Trained: 1478 | ‚ö†Ô∏è Skipped: 62 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 1550/3313 combinations | ‚úÖ Trained: 1488 | ‚ö†Ô∏è Skipped: 62 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 1560/3313 combinations | ‚úÖ Trained: 1498 | ‚ö†Ô∏è Skipped: 62 | ‚ùå Failed: 0 | ‚ö° Speed: 73.3 comb/sec\n",
            "   üìä Processed 1570/3313 combinations | ‚úÖ Trained: 1506 | ‚ö†Ô∏è Skipped: 64 | ‚ùå Failed: 0 | ‚ö° Speed: 73.2 comb/sec\n",
            "   üìä Processed 1580/3313 combinations | ‚úÖ Trained: 1516 | ‚ö†Ô∏è Skipped: 64 | ‚ùå Failed: 0 | ‚ö° Speed: 73.1 comb/sec\n",
            "   üìä Processed 1590/3313 combinations | ‚úÖ Trained: 1524 | ‚ö†Ô∏è Skipped: 66 | ‚ùå Failed: 0 | ‚ö° Speed: 73.0 comb/sec\n",
            "   üìä Processed 1600/3313 combinations | ‚úÖ Trained: 1533 | ‚ö†Ô∏è Skipped: 67 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1610/3313 combinations | ‚úÖ Trained: 1543 | ‚ö†Ô∏è Skipped: 67 | ‚ùå Failed: 0 | ‚ö° Speed: 72.5 comb/sec\n",
            "   üìä Processed 1620/3313 combinations | ‚úÖ Trained: 1553 | ‚ö†Ô∏è Skipped: 67 | ‚ùå Failed: 0 | ‚ö° Speed: 72.4 comb/sec\n",
            "   üìä Processed 1630/3313 combinations | ‚úÖ Trained: 1563 | ‚ö†Ô∏è Skipped: 67 | ‚ùå Failed: 0 | ‚ö° Speed: 72.1 comb/sec\n",
            "   üìä Processed 1640/3313 combinations | ‚úÖ Trained: 1573 | ‚ö†Ô∏è Skipped: 67 | ‚ùå Failed: 0 | ‚ö° Speed: 71.9 comb/sec\n",
            "   üìä Processed 1650/3313 combinations | ‚úÖ Trained: 1581 | ‚ö†Ô∏è Skipped: 69 | ‚ùå Failed: 0 | ‚ö° Speed: 71.8 comb/sec\n",
            "   üìä Processed 1660/3313 combinations | ‚úÖ Trained: 1589 | ‚ö†Ô∏è Skipped: 71 | ‚ùå Failed: 0 | ‚ö° Speed: 71.9 comb/sec\n",
            "   üìä Processed 1670/3313 combinations | ‚úÖ Trained: 1599 | ‚ö†Ô∏è Skipped: 71 | ‚ùå Failed: 0 | ‚ö° Speed: 72.0 comb/sec\n",
            "   üìä Processed 1680/3313 combinations | ‚úÖ Trained: 1607 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.0 comb/sec\n",
            "   üìä Processed 1690/3313 combinations | ‚úÖ Trained: 1617 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.1 comb/sec\n",
            "   üìä Processed 1700/3313 combinations | ‚úÖ Trained: 1627 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.1 comb/sec\n",
            "   üìä Processed 1710/3313 combinations | ‚úÖ Trained: 1637 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.2 comb/sec\n",
            "   üìä Processed 1720/3313 combinations | ‚úÖ Trained: 1647 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.3 comb/sec\n",
            "   üìä Processed 1730/3313 combinations | ‚úÖ Trained: 1657 | ‚ö†Ô∏è Skipped: 73 | ‚ùå Failed: 0 | ‚ö° Speed: 72.4 comb/sec\n",
            "   üìä Processed 1740/3313 combinations | ‚úÖ Trained: 1665 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.5 comb/sec\n",
            "   üìä Processed 1750/3313 combinations | ‚úÖ Trained: 1675 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.6 comb/sec\n",
            "   üìä Processed 1760/3313 combinations | ‚úÖ Trained: 1685 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.7 comb/sec\n",
            "   üìä Processed 1770/3313 combinations | ‚úÖ Trained: 1695 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.7 comb/sec\n",
            "   üìä Processed 1780/3313 combinations | ‚úÖ Trained: 1705 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1790/3313 combinations | ‚úÖ Trained: 1715 | ‚ö†Ô∏è Skipped: 75 | ‚ùå Failed: 0 | ‚ö° Speed: 72.7 comb/sec\n",
            "   üìä Processed 1800/3313 combinations | ‚úÖ Trained: 1724 | ‚ö†Ô∏è Skipped: 76 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1810/3313 combinations | ‚úÖ Trained: 1734 | ‚ö†Ô∏è Skipped: 76 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1820/3313 combinations | ‚úÖ Trained: 1742 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1830/3313 combinations | ‚úÖ Trained: 1752 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1840/3313 combinations | ‚úÖ Trained: 1762 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1850/3313 combinations | ‚úÖ Trained: 1772 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1860/3313 combinations | ‚úÖ Trained: 1782 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1870/3313 combinations | ‚úÖ Trained: 1792 | ‚ö†Ô∏è Skipped: 78 | ‚ùå Failed: 0 | ‚ö° Speed: 72.8 comb/sec\n",
            "   üìä Processed 1880/3313 combinations | ‚úÖ Trained: 1801 | ‚ö†Ô∏è Skipped: 79 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1890/3313 combinations | ‚úÖ Trained: 1809 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1900/3313 combinations | ‚úÖ Trained: 1819 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1910/3313 combinations | ‚úÖ Trained: 1829 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1920/3313 combinations | ‚úÖ Trained: 1839 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1930/3313 combinations | ‚úÖ Trained: 1849 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 72.9 comb/sec\n",
            "   üìä Processed 1940/3313 combinations | ‚úÖ Trained: 1859 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 73.0 comb/sec\n",
            "   üìä Processed 1950/3313 combinations | ‚úÖ Trained: 1869 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 73.0 comb/sec\n",
            "   üìä Processed 1960/3313 combinations | ‚úÖ Trained: 1879 | ‚ö†Ô∏è Skipped: 81 | ‚ùå Failed: 0 | ‚ö° Speed: 73.1 comb/sec\n",
            "   üìä Processed 1970/3313 combinations | ‚úÖ Trained: 1887 | ‚ö†Ô∏è Skipped: 83 | ‚ùå Failed: 0 | ‚ö° Speed: 73.1 comb/sec\n",
            "   üìä Processed 1980/3313 combinations | ‚úÖ Trained: 1897 | ‚ö†Ô∏è Skipped: 83 | ‚ùå Failed: 0 | ‚ö° Speed: 73.2 comb/sec\n",
            "   üìä Processed 1990/3313 combinations | ‚úÖ Trained: 1907 | ‚ö†Ô∏è Skipped: 83 | ‚ùå Failed: 0 | ‚ö° Speed: 73.3 comb/sec\n",
            "   üìä Processed 2000/3313 combinations | ‚úÖ Trained: 1917 | ‚ö†Ô∏è Skipped: 83 | ‚ùå Failed: 0 | ‚ö° Speed: 73.4 comb/sec\n",
            "   üìä Processed 2010/3313 combinations | ‚úÖ Trained: 1927 | ‚ö†Ô∏è Skipped: 83 | ‚ùå Failed: 0 | ‚ö° Speed: 73.4 comb/sec\n",
            "   üìä Processed 2020/3313 combinations | ‚úÖ Trained: 1936 | ‚ö†Ô∏è Skipped: 84 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2030/3313 combinations | ‚úÖ Trained: 1946 | ‚ö†Ô∏è Skipped: 84 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2040/3313 combinations | ‚úÖ Trained: 1956 | ‚ö†Ô∏è Skipped: 84 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2050/3313 combinations | ‚úÖ Trained: 1965 | ‚ö†Ô∏è Skipped: 85 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2060/3313 combinations | ‚úÖ Trained: 1975 | ‚ö†Ô∏è Skipped: 85 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2070/3313 combinations | ‚úÖ Trained: 1985 | ‚ö†Ô∏è Skipped: 85 | ‚ùå Failed: 0 | ‚ö° Speed: 73.5 comb/sec\n",
            "   üìä Processed 2080/3313 combinations | ‚úÖ Trained: 1995 | ‚ö†Ô∏è Skipped: 85 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2090/3313 combinations | ‚úÖ Trained: 2005 | ‚ö†Ô∏è Skipped: 85 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2100/3313 combinations | ‚úÖ Trained: 2014 | ‚ö†Ô∏è Skipped: 86 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2110/3313 combinations | ‚úÖ Trained: 2023 | ‚ö†Ô∏è Skipped: 87 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2120/3313 combinations | ‚úÖ Trained: 2031 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2130/3313 combinations | ‚úÖ Trained: 2041 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2140/3313 combinations | ‚úÖ Trained: 2051 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2150/3313 combinations | ‚úÖ Trained: 2061 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2160/3313 combinations | ‚úÖ Trained: 2071 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2170/3313 combinations | ‚úÖ Trained: 2081 | ‚ö†Ô∏è Skipped: 89 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2180/3313 combinations | ‚úÖ Trained: 2089 | ‚ö†Ô∏è Skipped: 91 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2190/3313 combinations | ‚úÖ Trained: 2098 | ‚ö†Ô∏è Skipped: 92 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 2200/3313 combinations | ‚úÖ Trained: 2106 | ‚ö†Ô∏è Skipped: 94 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2210/3313 combinations | ‚úÖ Trained: 2115 | ‚ö†Ô∏è Skipped: 95 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2220/3313 combinations | ‚úÖ Trained: 2125 | ‚ö†Ô∏è Skipped: 95 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2230/3313 combinations | ‚úÖ Trained: 2134 | ‚ö†Ô∏è Skipped: 96 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2240/3313 combinations | ‚úÖ Trained: 2142 | ‚ö†Ô∏è Skipped: 98 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 2250/3313 combinations | ‚úÖ Trained: 2150 | ‚ö†Ô∏è Skipped: 100 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2260/3313 combinations | ‚úÖ Trained: 2159 | ‚ö†Ô∏è Skipped: 101 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2270/3313 combinations | ‚úÖ Trained: 2169 | ‚ö†Ô∏è Skipped: 101 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2280/3313 combinations | ‚úÖ Trained: 2178 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.6 comb/sec\n",
            "   üìä Processed 2290/3313 combinations | ‚úÖ Trained: 2188 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.6 comb/sec\n",
            "   üìä Processed 2300/3313 combinations | ‚úÖ Trained: 2198 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 2310/3313 combinations | ‚úÖ Trained: 2208 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 2320/3313 combinations | ‚úÖ Trained: 2218 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 2330/3313 combinations | ‚úÖ Trained: 2228 | ‚ö†Ô∏è Skipped: 102 | ‚ùå Failed: 0 | ‚ö° Speed: 74.8 comb/sec\n",
            "   üìä Processed 2340/3313 combinations | ‚úÖ Trained: 2236 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2350/3313 combinations | ‚úÖ Trained: 2246 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2360/3313 combinations | ‚úÖ Trained: 2256 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2370/3313 combinations | ‚úÖ Trained: 2266 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2380/3313 combinations | ‚úÖ Trained: 2276 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2390/3313 combinations | ‚úÖ Trained: 2286 | ‚ö†Ô∏è Skipped: 104 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2400/3313 combinations | ‚úÖ Trained: 2294 | ‚ö†Ô∏è Skipped: 106 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 2410/3313 combinations | ‚úÖ Trained: 2304 | ‚ö†Ô∏è Skipped: 106 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2420/3313 combinations | ‚úÖ Trained: 2313 | ‚ö†Ô∏è Skipped: 107 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2430/3313 combinations | ‚úÖ Trained: 2323 | ‚ö†Ô∏è Skipped: 107 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2440/3313 combinations | ‚úÖ Trained: 2333 | ‚ö†Ô∏è Skipped: 107 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2450/3313 combinations | ‚úÖ Trained: 2342 | ‚ö†Ô∏è Skipped: 108 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2460/3313 combinations | ‚úÖ Trained: 2347 | ‚ö†Ô∏è Skipped: 113 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2470/3313 combinations | ‚úÖ Trained: 2357 | ‚ö†Ô∏è Skipped: 113 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2480/3313 combinations | ‚úÖ Trained: 2365 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2490/3313 combinations | ‚úÖ Trained: 2375 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2500/3313 combinations | ‚úÖ Trained: 2385 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2510/3313 combinations | ‚úÖ Trained: 2395 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2520/3313 combinations | ‚úÖ Trained: 2405 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2530/3313 combinations | ‚úÖ Trained: 2415 | ‚ö†Ô∏è Skipped: 115 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2540/3313 combinations | ‚úÖ Trained: 2424 | ‚ö†Ô∏è Skipped: 116 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2550/3313 combinations | ‚úÖ Trained: 2433 | ‚ö†Ô∏è Skipped: 117 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2560/3313 combinations | ‚úÖ Trained: 2442 | ‚ö†Ô∏è Skipped: 118 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2570/3313 combinations | ‚úÖ Trained: 2452 | ‚ö†Ô∏è Skipped: 118 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 2580/3313 combinations | ‚úÖ Trained: 2462 | ‚ö†Ô∏è Skipped: 118 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2590/3313 combinations | ‚úÖ Trained: 2471 | ‚ö†Ô∏è Skipped: 119 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2600/3313 combinations | ‚úÖ Trained: 2481 | ‚ö†Ô∏è Skipped: 119 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2610/3313 combinations | ‚úÖ Trained: 2490 | ‚ö†Ô∏è Skipped: 120 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2620/3313 combinations | ‚úÖ Trained: 2500 | ‚ö†Ô∏è Skipped: 120 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2630/3313 combinations | ‚úÖ Trained: 2508 | ‚ö†Ô∏è Skipped: 122 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2640/3313 combinations | ‚úÖ Trained: 2517 | ‚ö†Ô∏è Skipped: 123 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2650/3313 combinations | ‚úÖ Trained: 2527 | ‚ö†Ô∏è Skipped: 123 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2660/3313 combinations | ‚úÖ Trained: 2537 | ‚ö†Ô∏è Skipped: 123 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2670/3313 combinations | ‚úÖ Trained: 2542 | ‚ö†Ô∏è Skipped: 128 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2680/3313 combinations | ‚úÖ Trained: 2551 | ‚ö†Ô∏è Skipped: 129 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2690/3313 combinations | ‚úÖ Trained: 2560 | ‚ö†Ô∏è Skipped: 130 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2700/3313 combinations | ‚úÖ Trained: 2569 | ‚ö†Ô∏è Skipped: 131 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2710/3313 combinations | ‚úÖ Trained: 2579 | ‚ö†Ô∏è Skipped: 131 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2720/3313 combinations | ‚úÖ Trained: 2589 | ‚ö†Ô∏è Skipped: 131 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2730/3313 combinations | ‚úÖ Trained: 2597 | ‚ö†Ô∏è Skipped: 133 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2740/3313 combinations | ‚úÖ Trained: 2606 | ‚ö†Ô∏è Skipped: 134 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2750/3313 combinations | ‚úÖ Trained: 2615 | ‚ö†Ô∏è Skipped: 135 | ‚ùå Failed: 0 | ‚ö° Speed: 73.6 comb/sec\n",
            "   üìä Processed 2760/3313 combinations | ‚úÖ Trained: 2625 | ‚ö†Ô∏è Skipped: 135 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2770/3313 combinations | ‚úÖ Trained: 2634 | ‚ö†Ô∏è Skipped: 136 | ‚ùå Failed: 0 | ‚ö° Speed: 73.7 comb/sec\n",
            "   üìä Processed 2780/3313 combinations | ‚úÖ Trained: 2644 | ‚ö†Ô∏è Skipped: 136 | ‚ùå Failed: 0 | ‚ö° Speed: 73.8 comb/sec\n",
            "   üìä Processed 2790/3313 combinations | ‚úÖ Trained: 2653 | ‚ö†Ô∏è Skipped: 137 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 2800/3313 combinations | ‚úÖ Trained: 2662 | ‚ö†Ô∏è Skipped: 138 | ‚ùå Failed: 0 | ‚ö° Speed: 73.9 comb/sec\n",
            "   üìä Processed 2810/3313 combinations | ‚úÖ Trained: 2670 | ‚ö†Ô∏è Skipped: 140 | ‚ùå Failed: 0 | ‚ö° Speed: 74.0 comb/sec\n",
            "   üìä Processed 2820/3313 combinations | ‚úÖ Trained: 2680 | ‚ö†Ô∏è Skipped: 140 | ‚ùå Failed: 0 | ‚ö° Speed: 74.1 comb/sec\n",
            "   üìä Processed 2830/3313 combinations | ‚úÖ Trained: 2689 | ‚ö†Ô∏è Skipped: 141 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2840/3313 combinations | ‚úÖ Trained: 2699 | ‚ö†Ô∏è Skipped: 141 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2850/3313 combinations | ‚úÖ Trained: 2709 | ‚ö†Ô∏è Skipped: 141 | ‚ùå Failed: 0 | ‚ö° Speed: 74.2 comb/sec\n",
            "   üìä Processed 2860/3313 combinations | ‚úÖ Trained: 2719 | ‚ö†Ô∏è Skipped: 141 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 2870/3313 combinations | ‚úÖ Trained: 2728 | ‚ö†Ô∏è Skipped: 142 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "   üìä Processed 2880/3313 combinations | ‚úÖ Trained: 2738 | ‚ö†Ô∏è Skipped: 142 | ‚ùå Failed: 0 | ‚ö° Speed: 74.4 comb/sec\n",
            "   üìä Processed 2890/3313 combinations | ‚úÖ Trained: 2746 | ‚ö†Ô∏è Skipped: 144 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2900/3313 combinations | ‚úÖ Trained: 2756 | ‚ö†Ô∏è Skipped: 144 | ‚ùå Failed: 0 | ‚ö° Speed: 74.5 comb/sec\n",
            "   üìä Processed 2910/3313 combinations | ‚úÖ Trained: 2766 | ‚ö†Ô∏è Skipped: 144 | ‚ùå Failed: 0 | ‚ö° Speed: 74.6 comb/sec\n",
            "   üìä Processed 2920/3313 combinations | ‚úÖ Trained: 2775 | ‚ö†Ô∏è Skipped: 145 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 2930/3313 combinations | ‚úÖ Trained: 2785 | ‚ö†Ô∏è Skipped: 145 | ‚ùå Failed: 0 | ‚ö° Speed: 74.7 comb/sec\n",
            "   üìä Processed 2940/3313 combinations | ‚úÖ Trained: 2795 | ‚ö†Ô∏è Skipped: 145 | ‚ùå Failed: 0 | ‚ö° Speed: 74.8 comb/sec\n",
            "   üìä Processed 2950/3313 combinations | ‚úÖ Trained: 2805 | ‚ö†Ô∏è Skipped: 145 | ‚ùå Failed: 0 | ‚ö° Speed: 74.8 comb/sec\n",
            "   üìä Processed 2960/3313 combinations | ‚úÖ Trained: 2814 | ‚ö†Ô∏è Skipped: 146 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 2970/3313 combinations | ‚úÖ Trained: 2823 | ‚ö†Ô∏è Skipped: 147 | ‚ùå Failed: 0 | ‚ö° Speed: 74.9 comb/sec\n",
            "   üìä Processed 2980/3313 combinations | ‚úÖ Trained: 2833 | ‚ö†Ô∏è Skipped: 147 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 2990/3313 combinations | ‚úÖ Trained: 2843 | ‚ö†Ô∏è Skipped: 147 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3000/3313 combinations | ‚úÖ Trained: 2853 | ‚ö†Ô∏è Skipped: 147 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3010/3313 combinations | ‚úÖ Trained: 2863 | ‚ö†Ô∏è Skipped: 147 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3020/3313 combinations | ‚úÖ Trained: 2872 | ‚ö†Ô∏è Skipped: 148 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3030/3313 combinations | ‚úÖ Trained: 2882 | ‚ö†Ô∏è Skipped: 148 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3040/3313 combinations | ‚úÖ Trained: 2890 | ‚ö†Ô∏è Skipped: 150 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3050/3313 combinations | ‚úÖ Trained: 2900 | ‚ö†Ô∏è Skipped: 150 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3060/3313 combinations | ‚úÖ Trained: 2910 | ‚ö†Ô∏è Skipped: 150 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3070/3313 combinations | ‚úÖ Trained: 2919 | ‚ö†Ô∏è Skipped: 151 | ‚ùå Failed: 0 | ‚ö° Speed: 75.0 comb/sec\n",
            "   üìä Processed 3080/3313 combinations | ‚úÖ Trained: 2928 | ‚ö†Ô∏è Skipped: 152 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 3090/3313 combinations | ‚úÖ Trained: 2935 | ‚ö†Ô∏è Skipped: 155 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 3100/3313 combinations | ‚úÖ Trained: 2943 | ‚ö†Ô∏è Skipped: 157 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 3110/3313 combinations | ‚úÖ Trained: 2953 | ‚ö†Ô∏è Skipped: 157 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 3120/3313 combinations | ‚úÖ Trained: 2963 | ‚ö†Ô∏è Skipped: 157 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 3130/3313 combinations | ‚úÖ Trained: 2973 | ‚ö†Ô∏è Skipped: 157 | ‚ùå Failed: 0 | ‚ö° Speed: 75.1 comb/sec\n",
            "   üìä Processed 3140/3313 combinations | ‚úÖ Trained: 2982 | ‚ö†Ô∏è Skipped: 158 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 3150/3313 combinations | ‚úÖ Trained: 2991 | ‚ö†Ô∏è Skipped: 159 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 3160/3313 combinations | ‚úÖ Trained: 2998 | ‚ö†Ô∏è Skipped: 162 | ‚ùå Failed: 0 | ‚ö° Speed: 75.2 comb/sec\n",
            "   üìä Processed 3170/3313 combinations | ‚úÖ Trained: 3008 | ‚ö†Ô∏è Skipped: 162 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 3180/3313 combinations | ‚úÖ Trained: 3017 | ‚ö†Ô∏è Skipped: 163 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 3190/3313 combinations | ‚úÖ Trained: 3027 | ‚ö†Ô∏è Skipped: 163 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 3200/3313 combinations | ‚úÖ Trained: 3036 | ‚ö†Ô∏è Skipped: 164 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 3210/3313 combinations | ‚úÖ Trained: 3043 | ‚ö†Ô∏è Skipped: 167 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3220/3313 combinations | ‚úÖ Trained: 3050 | ‚ö†Ô∏è Skipped: 170 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3230/3313 combinations | ‚úÖ Trained: 3060 | ‚ö†Ô∏è Skipped: 170 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3240/3313 combinations | ‚úÖ Trained: 3069 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 3250/3313 combinations | ‚úÖ Trained: 3079 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.5 comb/sec\n",
            "   üìä Processed 3260/3313 combinations | ‚úÖ Trained: 3089 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3270/3313 combinations | ‚úÖ Trained: 3099 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3280/3313 combinations | ‚úÖ Trained: 3109 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3290/3313 combinations | ‚úÖ Trained: 3119 | ‚ö†Ô∏è Skipped: 171 | ‚ùå Failed: 0 | ‚ö° Speed: 75.3 comb/sec\n",
            "   üìä Processed 3300/3313 combinations | ‚úÖ Trained: 3127 | ‚ö†Ô∏è Skipped: 173 | ‚ùå Failed: 0 | ‚ö° Speed: 75.4 comb/sec\n",
            "   üìä Processed 3310/3313 combinations | ‚úÖ Trained: 3137 | ‚ö†Ô∏è Skipped: 173 | ‚ùå Failed: 0 | ‚ö° Speed: 74.3 comb/sec\n",
            "‚úÖ SARIMAX training complete in 44.6 seconds!\n",
            "   üéØ Successful models: 3139\n",
            "   ‚ö†Ô∏è Skipped models (insufficient data): 174\n",
            "   ‚ùå Failed models: 0\n",
            "   ‚ö° Average speed: 74.3 combinations/sec\n",
            "\n",
            "üìà Step 4: Making predictions...\n",
            "üìà Making SARIMAX predictions...\n",
            "‚úÖ Predictions complete!\n",
            "   üéØ Successful predictions: 83920\n",
            "   ‚ùå Failed/fallback predictions: 394\n",
            "\n",
            "üìä Step 5: Calculating metrics...\n",
            "\n",
            "============================================================\n",
            "üéØ EXPERIMENT 11 RESULTS SUMMARY\n",
            "============================================================\n",
            "üìä Validation Metrics:\n",
            "   WMAE (Competition Metric): $3,227.31\n",
            "   MAE: $3,178.40\n",
            "   RMSE: $7,723.57\n",
            "   R¬≤: 0.8762\n",
            "\n",
            "üìä Holiday Breakdown:\n",
            "   Holiday MAE: $3,574.91 (2,966 samples)\n",
            "   Non-Holiday MAE: $3,163.94 (81,348 samples)\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Successful models trained: 3,139\n",
            "   Store-Dept combinations: 3,313\n",
            "   External regressors: 3\n",
            "   Features used: 25\n",
            "\n",
            "üíæ Saving model artifacts...\n",
            "‚úÖ Experiment 11 completed successfully!\n",
            "üèÉ View run SARIMAX_Exp7_Features_Complete at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/43/runs/4f7a4e33667a430f9e993993cdcb28db\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/43\n",
            "\n",
            "üéâ Experiment 11: SARIMAX with Experiment 7 Features - COMPLETE!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}