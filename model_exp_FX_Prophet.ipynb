{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOrU84Y/XJMykZYOfU1eeC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting/blob/lodia/model_exp_FX_Prophet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IL1lkCw8s_f3",
        "outputId": "6f280559-fd5f-41f3-ab01-4ec1c13ca69b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.7.9)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uZpkq7utnBk",
        "outputId": "0bbe9270-3672-43d5-a51e-9a4e239e0a8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle"
      ],
      "metadata": {
        "id": "pu6fo6BItnbD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "pwaNIQn6tpBq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "WWFGoj75tq2l"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle competitions download -c walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcOJtB68tuJf",
        "outputId": "8c2f0ff9-b9e8-4c88-ecf3-740a17d08ff0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 848MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip walmart-recruiting-store-sales-forecasting"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r0_2rg4txvt",
        "outputId": "ea78f801-be82-4e40-cbea-a0a5571c2f43"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "import numpy as np\n",
        "import warnings\n",
        "import joblib # Still useful for saving models if needed\n",
        "import os\n",
        "from datetime import datetime, timedelta\n",
        "import logging\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error # Needed for metrics\n",
        "import zipfile\n",
        "# áƒ’áƒáƒ¤áƒ áƒ—áƒ®áƒ˜áƒšáƒ”áƒ‘áƒ”áƒ‘áƒ˜áƒ¡ áƒ˜áƒ’áƒœáƒáƒ áƒ˜áƒ áƒ”áƒ‘áƒ\n",
        "warnings.filterwarnings('ignore')\n",
        "logging.getLogger('prophet').setLevel(logging.WARNING)\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.WARNING) # Suppress cmdstanpy warnings if they occur\n",
        "logging.getLogger('cmdstanpy').setLevel(logging.ERROR)\n"
      ],
      "metadata": {
        "id": "jRbcf6qz8-7o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartProphetPreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Preprocessing pipeline for Prophet models.\n",
        "    Focuses on preparing data in the 'ds' (Date) and 'y' (Weekly_Sales) format,\n",
        "    and handling holidays.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "        self.holidays_df = None\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load and merge necessary datasets for Prophet.\"\"\"\n",
        "        print(\"ğŸ“Š Loading datasets...\") # Changed for consistency\n",
        "\n",
        "        # Load datasets\n",
        "        train_df = pd.read_csv('train.csv')\n",
        "        features_df = pd.read_csv('features.csv')\n",
        "        stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "        print(f\"   ğŸ“ˆ Train data: {train_df.shape}\")\n",
        "        print(f\"   ğŸ“Š Features data: {features_df.shape}\") # Added for features.csv\n",
        "        print(f\"   ğŸª Stores data: {stores_df.shape}\")\n",
        "\n",
        "        # Convert Date columns to datetime\n",
        "        train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "        features_df['Date'] = pd.to_datetime(features_df['Date'])\n",
        "\n",
        "        # Merge datasets (similar to previous steps)\n",
        "        merged_df = pd.merge(train_df, features_df, on=['Store', 'Date', 'IsHoliday'], how='left')\n",
        "        train_full = pd.merge(merged_df, stores_df, on=['Store'], how='left')\n",
        "\n",
        "        # Sort by date for time series consistency\n",
        "        train_full = train_full.sort_values(by=['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "\n",
        "        print(f\"   âœ… Merged data: {train_full.shape}\")\n",
        "        print(f\"   ğŸ“… Date range: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n",
        "\n",
        "        return train_full\n",
        "\n",
        "    def create_temporal_split(self, df, train_ratio=0.8):\n",
        "        \"\"\"Create temporal split to prevent data leakage for time series.\"\"\"\n",
        "        print(f\"ğŸ“… Creating temporal split ({int(train_ratio*100)}/{int((1-train_ratio)*100)})...\")\n",
        "\n",
        "        # Sort by date to ensure temporal order\n",
        "        df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Determine split date based on a fixed ratio of unique dates\n",
        "        unique_dates = sorted(df_sorted['Date'].unique())\n",
        "        total_weeks = len(unique_dates)\n",
        "        train_weeks = int(total_weeks * train_ratio)\n",
        "\n",
        "        # Ensure train_weeks is at least 1, and not exceeding total_weeks\n",
        "        if train_weeks < 1:\n",
        "            train_weeks = 1\n",
        "        if train_weeks >= total_weeks:\n",
        "            train_weeks = total_weeks - 1 # Ensure at least one week for validation/test\n",
        "\n",
        "        split_date = unique_dates[train_weeks - 1] # End date of training\n",
        "\n",
        "        # Split data\n",
        "        train_data = df_sorted[df_sorted['Date'] <= split_date].copy()\n",
        "        val_data = df_sorted[df_sorted['Date'] > split_date].copy()\n",
        "\n",
        "        # Create split info dictionary\n",
        "        split_info = {\n",
        "            'split_date': split_date,\n",
        "            'train_size': len(train_data),\n",
        "            'val_size': len(val_data),\n",
        "            'train_date_range': (train_data['Date'].min(), train_data['Date'].max()),\n",
        "            'val_date_range': (val_data['Date'].min(), val_data['Date'].max())\n",
        "        }\n",
        "\n",
        "        print(f\"   ğŸ“Š Split date: {split_date}\")\n",
        "        print(f\"   ğŸ“ˆ Train: {len(train_data):,} records ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
        "        print(f\"   ğŸ“‰ Val: {len(val_data):,} records ({val_data['Date'].min()} to {val_data['Date'].max()})\")\n",
        "\n",
        "        return train_data, val_data, split_info\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"Fit the preprocessing pipeline (prepare holidays).\"\"\"\n",
        "        print(\"ğŸ”§ Preparing Prophet specific data (holidays)...\") # Changed for consistency\n",
        "\n",
        "        # Prophet's holidays DataFrame: requires 'holiday', 'ds' columns\n",
        "        # We define common US holidays that align with Walmart's IsHoliday flag\n",
        "        self.holidays_df = pd.DataFrame([\n",
        "            # Super Bowl: IsHoliday=True\n",
        "            {'holiday': 'SuperBowl', 'ds': '2010-02-12'},\n",
        "            {'holiday': 'SuperBowl', 'ds': '2011-02-11'},\n",
        "            {'holiday': 'SuperBowl', 'ds': '2012-02-10'},\n",
        "            # Labor Day: IsHoliday=True\n",
        "            {'holiday': 'LaborDay', 'ds': '2010-09-10'},\n",
        "            {'holiday': 'LaborDay', 'ds': '2011-09-09'},\n",
        "            {'holiday': 'LaborDay', 'ds': '2012-09-07'},\n",
        "            # Thanksgiving: IsHoliday=True\n",
        "            {'holiday': 'Thanksgiving', 'ds': '2010-11-26'},\n",
        "            {'holiday': 'Thanksgiving', 'ds': '2011-11-25'},\n",
        "            {'holiday': 'Thanksgiving', 'ds': '2012-11-23'},\n",
        "            # Christmas: IsHoliday=True (often last week of year in dataset)\n",
        "            {'holiday': 'Christmas', 'ds': '2010-12-31'},\n",
        "            {'holiday': 'Christmas', 'ds': '2011-12-30'},\n",
        "            {'holiday': 'Christmas', 'ds': '2012-12-28'},\n",
        "        ])\n",
        "        self.holidays_df['ds'] = pd.to_datetime(self.holidays_df['ds'])\n",
        "\n",
        "        # Ensure only holidays present in the training data date range are considered\n",
        "        min_date = train_data['Date'].min()\n",
        "        max_date = train_data['Date'].max()\n",
        "        self.holidays_df = self.holidays_df[\n",
        "            (self.holidays_df['ds'] >= min_date) &\n",
        "            (self.holidays_df['ds'] <= max_date)\n",
        "        ]\n",
        "\n",
        "        print(\"âœ… Pipeline fitted on training data with holiday-aware settings\") # Changed for consistency\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, data, is_validation=False): # Added is_validation for print\n",
        "        \"\"\"Transform data into Prophet's required format (ds, y).\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before transform!\")\n",
        "\n",
        "        print(f\"ğŸ”„ Transforming {'validation' if is_validation else 'training'} data...\") # Changed for consistency\n",
        "\n",
        "        df = data.copy()\n",
        "        # Rename columns to Prophet's requirements\n",
        "        df = df.rename(columns={'Date': 'ds', 'Weekly_Sales': 'y'})\n",
        "\n",
        "        # Ensure 'y' (Weekly_Sales) is not negative, as sales cannot be negative\n",
        "        df['y'] = df['y'].apply(lambda x: max(0, x))\n",
        "\n",
        "        print(f\"âœ… Transform complete. Shape: {df.shape}\") # Changed for consistency\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, train_data):\n",
        "        \"\"\"Fit and transform training data in one step.\"\"\"\n",
        "        # This will call fit() and then transform() with is_validation=False\n",
        "        return self.fit(train_data).transform(train_data, is_validation=False)\n",
        "\n",
        "    def get_preprocessed_data(self):\n",
        "        \"\"\"\n",
        "        Orchestrates preprocessing steps to get model-ready data.\n",
        "\n",
        "        Returns:\n",
        "            train_data_prophet, val_data_prophet: DataFrames ready for Prophet\n",
        "            split_info: Information about the temporal split\n",
        "            holidays_df: DataFrame of holidays for Prophet\n",
        "        \"\"\"\n",
        "        print(\"ğŸ”„ Getting preprocessed data using pipeline...\") # Changed for consistency\n",
        "\n",
        "        # Create the preprocessing pipeline\n",
        "        pipeline = WalmartProphetPreprocessingPipeline()\n",
        "\n",
        "        # Load raw data\n",
        "        train_full = pipeline.load_and_prepare_data()\n",
        "\n",
        "        # Create temporal split\n",
        "        train_data_raw, val_data_raw, split_info = pipeline.create_temporal_split(train_full)\n",
        "\n",
        "        # Fit and transform data using pipeline\n",
        "        pipeline.fit(train_data_raw)\n",
        "        train_data_prophet = pipeline.transform(train_data_raw, is_validation=False)\n",
        "        val_data_prophet = pipeline.transform(val_data_raw, is_validation=True) # Pass is_validation=True\n",
        "\n",
        "        holidays_df = pipeline.holidays_df # Get holidays after fitting pipeline\n",
        "\n",
        "        print(f\"âœ… Data preprocessing complete!\")\n",
        "        print(f\"   ğŸ“Š Training shape: {train_data_prophet.shape}\")\n",
        "        print(f\"   ğŸ“Š Validation shape: {val_data_prophet.shape}\")\n",
        "\n",
        "        return train_data_prophet, val_data_prophet, split_info, holidays_df"
      ],
      "metadata": {
        "id": "1zEpMp959Ze3"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error (WMAE) as per competition rules.\"\"\"\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "    wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "    return wmae"
      ],
      "metadata": {
        "id": "zTE1ojYE93N9"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_prophet_models(train_data_prophet, holidays_df, min_observations=50):\n",
        "    \"\"\"\n",
        "    Trains Prophet models for each unique (Store, Dept) combination.\n",
        "\n",
        "    Args:\n",
        "        train_data_prophet (pd.DataFrame): Training data in Prophet format (ds, y).\n",
        "        holidays_df (pd.DataFrame): DataFrame of holidays for Prophet.\n",
        "        min_observations (int): Minimum number of observations required to train a model.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary of trained Prophet models, keyed by (Store, Dept) tuple.\n",
        "    \"\"\"\n",
        "    print(f\"ğŸ“ˆ Training Prophet models for each Store-Dept combination...\") # Changed for consistency\n",
        "    print(f\"   â° No time limit - training all combinations\") # Added for consistency\n",
        "\n",
        "    unique_series_keys = train_data_prophet[['Store', 'Dept']].drop_duplicates().values\n",
        "    total_combinations = len(unique_series_keys)\n",
        "    print(f\"   ğŸ“Š Training models for {total_combinations} combinations\")\n",
        "    print(f\"   ğŸ¯ Training Prophet for all combinations\")\n",
        "\n",
        "    models = {}\n",
        "    successful_models = 0\n",
        "    skipped_models_insufficient_data = 0\n",
        "    failed_models_training_error = 0\n",
        "\n",
        "    for i, (store_id, dept_id) in enumerate(unique_series_keys):\n",
        "        # Progress update, similar to ARIMA\n",
        "        # Print at start (index 0), every 200 models, and at the very end\n",
        "        if i % 200 == 0 or i == total_combinations - 1: # Fixed bug for 0-index. Use i == total_combinations - 1 for the last one\n",
        "            print(f\"   âœ… Trained {i+1}/{total_combinations} models ({successful_models} successful, {skipped_models_insufficient_data + failed_models_training_error} failed)\")\n",
        "\n",
        "        series_data = train_data_prophet[\n",
        "            (train_data_prophet['Store'] == store_id) &\n",
        "            (train_data_prophet['Dept'] == dept_id)\n",
        "        ].copy()\n",
        "\n",
        "        # Check for minimum observations\n",
        "        if len(series_data) < min_observations:\n",
        "            skipped_models_insufficient_data += 1\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Initialize Prophet model\n",
        "            m = Prophet(\n",
        "                yearly_seasonality=True,\n",
        "                weekly_seasonality=True,\n",
        "                holidays=holidays_df\n",
        "            )\n",
        "\n",
        "            # Fit the model\n",
        "            m.fit(series_data)\n",
        "            models[(store_id, dept_id)] = m\n",
        "            successful_models += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_models_training_error += 1\n",
        "            # You can uncomment the line below for specific error messages, but it will be verbose\n",
        "            # if failed_models_training_error < 5: # Limit error printouts to avoid flooding console\n",
        "            #     print(f\"   âš ï¸ Failed to train model for Store {store_id}, Dept {dept_id}: {e}\")\n",
        "\n",
        "    # Final check for total counts, if loop finished without a 200 multiple at the end\n",
        "    # This ensures the last state is always printed if the loop didn't end on a % 200\n",
        "    if (total_combinations - 1) % 200 != 0 and total_combinations > 0:\n",
        "         print(f\"   âœ… Trained {total_combinations}/{total_combinations} models ({successful_models} successful, {skipped_models_insufficient_data + failed_models_training_error} failed)\")\n",
        "\n",
        "\n",
        "    print(f\"âœ… Prophet training complete!\")\n",
        "    print(f\"   ğŸ¯ Successful models: {successful_models}\")\n",
        "    print(f\"   âŒ Failed models: {skipped_models_insufficient_data + failed_models_training_error}\") # Combined failed and skipped\n",
        "    print(f\"   ğŸ“Š Coverage: {successful_models}/{total_combinations} ({successful_models/total_combinations*100:.1f}%)\")\n",
        "\n",
        "    return models"
      ],
      "metadata": {
        "id": "dnNnGP-H981W"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_prophet_predictions(models, val_data_prophet, train_data=None): # Added train_data=None for consistency\n",
        "    \"\"\"\n",
        "    Makes predictions using trained Prophet models for the validation period.\n",
        "    Handles cases where models might not exist or predictions fail.\n",
        "\n",
        "    Args:\n",
        "        models (dict): Dictionary of trained Prophet models.\n",
        "        val_data_prophet (pd.DataFrame): Validation data in Prophet format (ds, y).\n",
        "        train_data (pd.DataFrame): Not used by Prophet for forecasting, but kept for signature consistency.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (y_pred, y_true, is_holiday_flags)\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ˆ Making Prophet predictions (no fallbacks)...\") # Changed for consistency\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    holidays_flags = []\n",
        "\n",
        "    successful_predictions_count = 0\n",
        "    skipped_predictions_no_model = 0\n",
        "    failed_predictions_error = 0\n",
        "\n",
        "    unique_val_series_keys = val_data_prophet[['Store', 'Dept']].drop_duplicates().values\n",
        "    total_val_combinations = len(unique_val_series_keys)\n",
        "\n",
        "    # We don't need a frequent print here, as Prophet's predict is usually fast.\n",
        "    # The final summary will be sufficient.\n",
        "    for i, (store_id, dept_id) in enumerate(unique_val_series_keys):\n",
        "        # Get actual validation data for this series\n",
        "        current_val_series_actuals = val_data_prophet[\n",
        "            (val_data_prophet['Store'] == store_id) &\n",
        "            (val_data_prophet['Dept'] == dept_id)\n",
        "        ].copy()\n",
        "\n",
        "        if current_val_series_actuals.empty:\n",
        "            continue # No validation data for this series\n",
        "\n",
        "        # Prepare future DataFrame for Prophet prediction\n",
        "        # The future DataFrame should cover the exact dates in the validation set\n",
        "        future_dates = pd.DataFrame({'ds': current_val_series_actuals['ds']})\n",
        "\n",
        "        if (store_id, dept_id) in models:\n",
        "            try:\n",
        "                m = models[(store_id, dept_id)]\n",
        "                forecast = m.predict(future_dates)\n",
        "                yhat = forecast['yhat'].values\n",
        "\n",
        "                # Ensure predictions are not negative\n",
        "                yhat[yhat < 0] = 0\n",
        "\n",
        "                predictions.extend(yhat)\n",
        "                actuals.extend(current_val_series_actuals['y'].values)\n",
        "                holidays_flags.extend(current_val_series_actuals['IsHoliday'].values)\n",
        "                successful_predictions_count += len(yhat)\n",
        "\n",
        "            except Exception as e:\n",
        "                failed_predictions_error += len(current_val_series_actuals)\n",
        "                predictions.extend(np.zeros(len(current_val_series_actuals)))\n",
        "                actuals.extend(current_val_series_actuals['y'].values)\n",
        "                holidays_flags.extend(current_val_series_actuals['IsHoliday'].values)\n",
        "        else:\n",
        "            skipped_predictions_no_model += len(current_val_series_actuals)\n",
        "            predictions.extend(np.zeros(len(current_val_series_actuals)))\n",
        "            actuals.extend(current_val_series_actuals['y'].values)\n",
        "            holidays_flags.extend(current_val_series_actuals['IsHoliday'].values)\n",
        "\n",
        "    print(f\"âœ… Predictions complete!\")\n",
        "    print(f\"   ğŸ¯ Prophet predictions: {successful_predictions_count}\") # Changed for consistency\n",
        "    print(f\"   â­ï¸ Skipped (no model): {skipped_predictions_no_model}\") # Changed for consistency\n",
        "\n",
        "    return np.array(predictions), np.array(actuals), np.array(holidays_flags).astype(bool)\n"
      ],
      "metadata": {
        "id": "Qp98IIFo-Dhu"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main experiment execution for Prophet model.\"\"\"\n",
        "    print(\"ğŸš€ Starting Experiment Prophet: Prophet Models for Walmart Sales Forecasting\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get preprocessed data\n",
        "        print(\"\\nğŸ“Š Step 1: Data preprocessing...\")\n",
        "        train_data_prophet, val_data_prophet, split_info, holidays_df = WalmartProphetPreprocessingPipeline().get_preprocessed_data()\n",
        "\n",
        "        # Step 2: Train Prophet models\n",
        "        print(\"\\nğŸ“ˆ Step 2: Training Prophet models...\")\n",
        "        models = train_prophet_models(train_data_prophet, holidays_df)\n",
        "\n",
        "        # Step 3: Make predictions on validation set\n",
        "        print(\"\\nğŸ“ˆ Step 3: Making predictions...\")\n",
        "        y_pred_val, y_true_val, is_holiday_val = make_prophet_predictions(models, val_data_prophet)\n",
        "\n",
        "        # --- Step 3.5: Calculate training WMAE (Prophet does not have direct 'fittedvalues') ---\n",
        "        print(\"\\nğŸ“Š Step 3.5: Training performance...\")\n",
        "        print(\"ğŸ“Š Calculating training WMAE on fitted values...\")\n",
        "\n",
        "        train_predictions = []\n",
        "        train_actuals = []\n",
        "        train_holidays = []\n",
        "\n",
        "        unique_train_series_keys = train_data_prophet[['Store', 'Dept']].drop_duplicates().values\n",
        "\n",
        "        # Iterate through each Store-Dept combination to predict on training data\n",
        "        for store_id, dept_id in unique_train_series_keys:\n",
        "            series_data_train = train_data_prophet[\n",
        "                (train_data_prophet['Store'] == store_id) &\n",
        "                (train_data_prophet['Dept'] == dept_id)\n",
        "            ].copy()\n",
        "\n",
        "            if (store_id, dept_id) in models and not series_data_train.empty:\n",
        "                try:\n",
        "                    m = models[(store_id, dept_id)]\n",
        "                    # Predict on the training data's 'ds' (Date) column\n",
        "                    forecast_train = m.predict(series_data_train[['ds']])\n",
        "                    yhat_train = forecast_train['yhat'].values\n",
        "                    yhat_train[yhat_train < 0] = 0 # Ensure no negative predictions\n",
        "\n",
        "                    train_predictions.extend(yhat_train)\n",
        "                    train_actuals.extend(series_data_train['y'].values)\n",
        "                    train_holidays.extend(series_data_train['IsHoliday'].values)\n",
        "                except Exception as e:\n",
        "                    # If prediction fails for a specific series on training data,\n",
        "                    # fill with zeros to avoid breaking WMAE calculation and continue.\n",
        "                    train_predictions.extend(np.zeros(len(series_data_train)))\n",
        "                    train_actuals.extend(series_data_train['y'].values)\n",
        "                    train_holidays.extend(series_data_train['IsHoliday'].values)\n",
        "                    # print(f\"   âš ï¸ Failed to predict on training data for Store {store_id}, Dept {dept_id}: {e}\") # Uncomment for debug\n",
        "\n",
        "        if len(train_actuals) > 0:\n",
        "            train_wmae = calculate_wmae(np.array(train_actuals), np.array(train_predictions), np.array(train_holidays).astype(bool))\n",
        "            print(f\"   ğŸ“ˆ Training WMAE: ${train_wmae:,.2f}\")\n",
        "        else:\n",
        "            train_wmae = None\n",
        "            print(\"   âš ï¸ No training data points available for WMAE calculation.\")\n",
        "        # --- End of Step 3.5 ---\n",
        "\n",
        "\n",
        "        # Step 4: Calculate validation metrics\n",
        "        print(\"\\nğŸ“Š Step 4: Calculating validation metrics...\")\n",
        "\n",
        "        # Validation metrics\n",
        "        if len(y_true_val) > 0:\n",
        "            val_mae = mean_absolute_error(y_true_val, y_pred_val)\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_true_val, y_pred_val))\n",
        "            val_wmae = calculate_wmae(y_true_val, y_pred_val, is_holiday_val)\n",
        "        else:\n",
        "            val_mae, val_rmse, val_wmae = 0, 0, 0\n",
        "            print(\"   âš ï¸ Warning: No data points for evaluation. Metrics set to 0.\")\n",
        "\n",
        "        # Holiday breakdown for validation\n",
        "        holiday_mask_val = is_holiday_val.astype(bool)\n",
        "        holiday_mae_val = mean_absolute_error(y_true_val[holiday_mask_val], y_pred_val[holiday_mask_val]) if holiday_mask_val.any() else 0\n",
        "        non_holiday_mae_val = mean_absolute_error(y_true_val[~holiday_mask_val], y_pred_val[~holiday_mask_val]) if (~holiday_mask_val).any() else 0\n",
        "\n",
        "        # Print results\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"ğŸ¯ EXPERIMENT PROPHET RESULTS SUMMARY\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(\"ğŸ“Š Training Metrics:\")\n",
        "        if train_wmae is not None:\n",
        "             print(f\"   Training WMAE: ${train_wmae:,.2f}\")\n",
        "        else:\n",
        "             print(\"   Training WMAE: Not calculated (no data points).\")\n",
        "        print()\n",
        "\n",
        "\n",
        "        print(\"ğŸ“Š Validation Metrics:\")\n",
        "        print(f\"   WMAE (Competition Metric): ${val_wmae:,.2f}\")\n",
        "        print(f\"   MAE: ${val_mae:,.2f}\")\n",
        "        print(f\"   RMSE: ${val_rmse:,.2f}\")\n",
        "\n",
        "        print(\"\\nğŸ“Š Holiday Breakdown:\")\n",
        "        print(f\"   Holiday MAE: ${holiday_mae_val:,.2f} ({int(holiday_mask_val.sum())} samples)\")\n",
        "        print(f\"   Non-Holiday MAE: ${non_holiday_mae_val:,.2f} ({int((~holiday_mask_val).sum())} samples)\")\n",
        "\n",
        "        print(\"\\nğŸ“Š Model Statistics:\")\n",
        "        print(f\"   Successful models trained: {len(models):,}\")\n",
        "        print(f\"   Store-Dept combinations: {len(train_data_prophet[['Store', 'Dept']].drop_duplicates()):,}\")\n",
        "        print(f\"   No training errors calculated\")\n",
        "\n",
        "\n",
        "        print(\"\\nğŸ‰ Experiment Prophet: Individual Prophet Models - COMPLETE!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Experiment failed: {e}\")\n",
        "        raise\n"
      ],
      "metadata": {
        "id": "PXW_-NdH-Kh6"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    with zipfile.ZipFile('train.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    train = pd.read_csv('train.csv')\n",
        "    with zipfile.ZipFile('features.csv.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('.')\n",
        "    features = pd.read_csv('features.csv')\n",
        ""
      ],
      "metadata": {
        "id": "jP_3cn-P884J"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXpPxMWn-ScJ",
        "outputId": "e3cdfd88-f426-40c4-b27f-80bffd45db8a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Starting Experiment Prophet: Prophet Models for Walmart Sales Forecasting\n",
            "================================================================================\n",
            "\n",
            "ğŸ“Š Step 1: Data preprocessing...\n",
            "ğŸ”„ Getting preprocessed data using pipeline...\n",
            "ğŸ“Š Loading datasets...\n",
            "   ğŸ“ˆ Train data: (421570, 5)\n",
            "   ğŸ“Š Features data: (8190, 12)\n",
            "   ğŸª Stores data: (45, 3)\n",
            "   âœ… Merged data: (421570, 16)\n",
            "   ğŸ“… Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "ğŸ“… Creating temporal split (80/19)...\n",
            "   ğŸ“Š Split date: 2012-04-06 00:00:00\n",
            "   ğŸ“ˆ Train: 335,761 records (2010-02-05 00:00:00 to 2012-04-06 00:00:00)\n",
            "   ğŸ“‰ Val: 85,809 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n",
            "ğŸ”§ Preparing Prophet specific data (holidays)...\n",
            "âœ… Pipeline fitted on training data with holiday-aware settings\n",
            "ğŸ”„ Transforming training data...\n",
            "âœ… Transform complete. Shape: (335761, 16)\n",
            "ğŸ”„ Transforming validation data...\n",
            "âœ… Transform complete. Shape: (85809, 16)\n",
            "âœ… Data preprocessing complete!\n",
            "   ğŸ“Š Training shape: (335761, 16)\n",
            "   ğŸ“Š Validation shape: (85809, 16)\n",
            "\n",
            "ğŸ“ˆ Step 2: Training Prophet models...\n",
            "ğŸ“ˆ Training Prophet models for each Store-Dept combination...\n",
            "   â° No time limit - training all combinations\n",
            "   ğŸ“Š Training models for 3313 combinations\n",
            "   ğŸ¯ Training Prophet for all combinations\n",
            "   âœ… Trained 1/3313 models (0 successful, 0 failed)\n",
            "   âœ… Trained 201/3313 models (195 successful, 5 failed)\n",
            "   âœ… Trained 401/3313 models (389 successful, 11 failed)\n",
            "   âœ… Trained 601/3313 models (585 successful, 15 failed)\n",
            "   âœ… Trained 801/3313 models (781 successful, 19 failed)\n",
            "   âœ… Trained 1001/3313 models (981 successful, 19 failed)\n",
            "   âœ… Trained 1201/3313 models (1179 successful, 21 failed)\n",
            "   âœ… Trained 1401/3313 models (1375 successful, 25 failed)\n",
            "   âœ… Trained 1601/3313 models (1570 successful, 30 failed)\n",
            "   âœ… Trained 1801/3313 models (1761 successful, 39 failed)\n",
            "   âœ… Trained 2001/3313 models (1958 successful, 42 failed)\n",
            "   âœ… Trained 2201/3313 models (2154 successful, 46 failed)\n",
            "   âœ… Trained 2401/3313 models (2352 successful, 48 failed)\n",
            "   âœ… Trained 2601/3313 models (2549 successful, 51 failed)\n",
            "   âœ… Trained 2801/3313 models (2744 successful, 56 failed)\n",
            "   âœ… Trained 3001/3313 models (2914 successful, 86 failed)\n",
            "   âœ… Trained 3201/3313 models (2961 successful, 239 failed)\n",
            "   âœ… Trained 3313/3313 models (2966 successful, 346 failed)\n",
            "   âœ… Trained 3313/3313 models (2966 successful, 347 failed)\n",
            "âœ… Prophet training complete!\n",
            "   ğŸ¯ Successful models: 2966\n",
            "   âŒ Failed models: 347\n",
            "   ğŸ“Š Coverage: 2966/3313 (89.5%)\n",
            "\n",
            "ğŸ“ˆ Step 3: Making predictions...\n",
            "ğŸ“ˆ Making Prophet predictions (no fallbacks)...\n",
            "âœ… Predictions complete!\n",
            "   ğŸ¯ Prophet predictions: 83930\n",
            "   â­ï¸ Skipped (no model): 1879\n",
            "\n",
            "ğŸ“Š Step 3.5: Training performance...\n",
            "ğŸ“Š Calculating training WMAE on fitted values...\n",
            "   ğŸ“ˆ Training WMAE: $1,166.70\n",
            "\n",
            "ğŸ“Š Step 4: Calculating validation metrics...\n",
            "\n",
            "============================================================\n",
            "ğŸ¯ EXPERIMENT PROPHET RESULTS SUMMARY\n",
            "============================================================\n",
            "ğŸ“Š Training Metrics:\n",
            "   Training WMAE: $1,166.70\n",
            "\n",
            "ğŸ“Š Validation Metrics:\n",
            "   WMAE (Competition Metric): $1,669.21\n",
            "   MAE: $1,626.37\n",
            "   RMSE: $3,725.00\n",
            "\n",
            "ğŸ“Š Holiday Breakdown:\n",
            "   Holiday MAE: $1,979.10 (2966 samples)\n",
            "   Non-Holiday MAE: $1,613.74 (82843 samples)\n",
            "\n",
            "ğŸ“Š Model Statistics:\n",
            "   Successful models trained: 2,966\n",
            "   Store-Dept combinations: 3,313\n",
            "   No training errors calculated\n",
            "\n",
            "ğŸ‰ Experiment Prophet: Individual Prophet Models - COMPLETE!\n"
          ]
        }
      ]
    }
  ]
}