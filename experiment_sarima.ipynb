{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYs9LKYhZL8Q"
      },
      "source": [
        "# Experiment Sarima"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "iVQB8aSsZL8S",
        "outputId": "efbab32b-de9d-4ac7-f15e-bd713f1a6f72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hMounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "! unzip walmart-recruiting-store-sales-forecasting.zip\n",
        "!unzip train.csv.zip\n",
        "!unzip features.csv.zip"
      ],
      "metadata": {
        "id": "q6UtYiRfZPV3",
        "outputId": "ead47b5a-6a75-4080-810b-ac2fa507ef15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading walmart-recruiting-store-sales-forecasting.zip to /content\n",
            "\r  0% 0.00/2.70M [00:00<?, ?B/s]\n",
            "\r100% 2.70M/2.70M [00:00<00:00, 464MB/s]\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "  inflating: features.csv.zip        \n",
            "  inflating: sampleSubmission.csv.zip  \n",
            "  inflating: stores.csv              \n",
            "  inflating: test.csv.zip            \n",
            "  inflating: train.csv.zip           \n",
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n",
            "Archive:  features.csv.zip\n",
            "  inflating: features.csv            \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install statsmodels mlflow dagshub scikit-learn pandas numpy matplotlib seaborn joblib -q"
      ],
      "metadata": {
        "id": "pk8xbMl9ZSDa",
        "outputId": "0a790c31-4b72-434e-9fd8-b344379a9794",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m741.4/741.4 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.6/65.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m119.4/119.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.6/201.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Core libraries\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import dagshub\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# SARIMA and time series\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Suppress warnings\n",
        "import logging\n",
        "logging.getLogger('statsmodels').setLevel(logging.WARNING)\n",
        "\n",
        "# Additional warning suppression for cleaner output\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='No frequency information was provided')\n",
        "warnings.filterwarnings('ignore', message='An unsupported index was provided')\n",
        "warnings.filterwarnings('ignore', message='A date index has been provided, but it has no associated frequency')\n",
        "warnings.filterwarnings('ignore', message='No supported index is available')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning, module='statsmodels')\n",
        "\n"
      ],
      "metadata": {
        "id": "T1Fk_FGRaMhA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore', message='No frequency information was provided')\n",
        "warnings.filterwarnings('ignore', message='An unsupported index was provided')\n",
        "warnings.filterwarnings('ignore', message='A date index has been provided, but it has no associated frequency')\n",
        "warnings.filterwarnings('ignore', message='No supported index is available')\n",
        "warnings.filterwarnings('ignore', category=FutureWarning, module='statsmodels')\n",
        "warnings.filterwarnings('ignore', message='Maximum Likelihood optimization failed to converge')\n",
        "warnings.filterwarnings('ignore', category=UserWarning, module='statsmodels')\n"
      ],
      "metadata": {
        "id": "X-CwA0mIfBZa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WalmartSimplePreprocessingPipeline:\n",
        "    \"\"\"\n",
        "    Simplified preprocessing pipeline for SARIMA models\n",
        "    SARIMA doesn't use external regressors, so we focus on:\n",
        "    - Time series data preparation\n",
        "    - Store-Dept grouping\n",
        "    - Basic date features for context\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.fitted = False\n",
        "        self.outlier_thresholds = None\n",
        "\n",
        "    def load_and_prepare_data(self):\n",
        "        \"\"\"Load and merge train.csv, stores.csv datasets (minimal for SARIMA)\"\"\"\n",
        "        print(\"üìä Loading datasets...\")\n",
        "\n",
        "        # Load datasets\n",
        "        train_df = pd.read_csv('train.csv')\n",
        "        stores_df = pd.read_csv('stores.csv')\n",
        "\n",
        "        print(f\"   üìà Train data: {train_df.shape}\")\n",
        "        print(f\"   üè™ Stores data: {stores_df.shape}\")\n",
        "\n",
        "        # Convert Date column to datetime\n",
        "        train_df['Date'] = pd.to_datetime(train_df['Date'])\n",
        "\n",
        "        # Merge with stores for Type information (useful for outlier detection)\n",
        "        train_full = train_df.merge(stores_df, on='Store', how='left')\n",
        "\n",
        "        print(f\"   ‚úÖ Merged data: {train_full.shape}\")\n",
        "        print(f\"   üìÖ Date range: {train_full['Date'].min()} to {train_full['Date'].max()}\")\n",
        "\n",
        "        return train_full\n",
        "\n",
        "    def create_temporal_split(self, df, train_ratio=0.8):\n",
        "        \"\"\"Create temporal split to prevent data leakage\"\"\"\n",
        "        print(f\"üìÖ Creating temporal split ({int(train_ratio*100)}/{int((1-train_ratio)*100)})...\")\n",
        "\n",
        "        # Sort by date to ensure temporal order\n",
        "        df_sorted = df.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Find split point\n",
        "        split_idx = int(len(df_sorted) * train_ratio)\n",
        "        split_date = df_sorted.iloc[split_idx]['Date']\n",
        "\n",
        "        # Create splits\n",
        "        train_data = df_sorted.iloc[:split_idx].copy()\n",
        "        val_data = df_sorted.iloc[split_idx:].copy()\n",
        "\n",
        "        # Create split info dictionary\n",
        "        split_info = {\n",
        "            'split_date': split_date,\n",
        "            'train_size': len(train_data),\n",
        "            'val_size': len(val_data),\n",
        "            'train_date_range': (train_data['Date'].min(), train_data['Date'].max()),\n",
        "            'val_date_range': (val_data['Date'].min(), val_data['Date'].max())\n",
        "        }\n",
        "\n",
        "        print(f\"   üìä Split date: {split_date}\")\n",
        "        print(f\"   üìà Train: {len(train_data):,} records ({train_data['Date'].min()} to {train_data['Date'].max()})\")\n",
        "        print(f\"   üìâ Val: {len(val_data):,} records ({val_data['Date'].min()} to {val_data['Date'].max()})\")\n",
        "\n",
        "        return train_data, val_data, split_info\n",
        "\n",
        "    def fit(self, train_data):\n",
        "        \"\"\"Fit the preprocessing pipeline on training data\"\"\"\n",
        "        print(\"üîß Fitting preprocessing pipeline on training data...\")\n",
        "\n",
        "        # Fit outlier removal thresholds on training data only\n",
        "        # Separate thresholds for holiday vs non-holiday weeks since they have different patterns\n",
        "        self.outlier_thresholds = {\n",
        "            # Non-holiday weeks (regular business)\n",
        "            'non_holiday': {\n",
        "                'A': {'lower': -1000, 'upper': 35000},   # Type A stores - regular weeks\n",
        "                'B': {'lower': -500, 'upper': 20000},    # Type B stores - regular weeks\n",
        "                'C': {'lower': -200, 'upper': 12000}     # Type C stores - regular weeks\n",
        "            },\n",
        "            # Holiday weeks (higher sales expected - Super Bowl, Labor Day, Thanksgiving, Christmas)\n",
        "            'holiday': {\n",
        "                'A': {'lower': -1000, 'upper': 80000},   # Type A stores - holiday weeks (much higher)\n",
        "                'B': {'lower': -500, 'upper': 50000},    # Type B stores - holiday weeks\n",
        "                'C': {'lower': -200, 'upper': 30000}     # Type C stores - holiday weeks\n",
        "            }\n",
        "        }\n",
        "\n",
        "        print(\"‚úÖ Pipeline fitted on training data with holiday-aware outlier thresholds\")\n",
        "        self.fitted = True\n",
        "        return self\n",
        "\n",
        "    def transform(self, data, is_validation=False):\n",
        "        \"\"\"Transform data using fitted pipeline (minimal for SARIMA)\"\"\"\n",
        "        if not self.fitted:\n",
        "            raise ValueError(\"Pipeline must be fitted before transform!\")\n",
        "\n",
        "        print(f\"üîÑ Transforming {'validation' if is_validation else 'training'} data...\")\n",
        "\n",
        "        df = data.copy()\n",
        "\n",
        "        # Step 1: Create basic date features (for context, not used in SARIMA model)\n",
        "        df = self._create_basic_date_features(df)\n",
        "\n",
        "        # Step 2: Remove outliers (only on training data)\n",
        "        if not is_validation:\n",
        "            df = self._remove_outliers(df)\n",
        "\n",
        "        print(f\"‚úÖ Transform complete. Shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    def fit_transform(self, train_data):\n",
        "        \"\"\"Fit and transform training data in one step\"\"\"\n",
        "        return self.fit(train_data).transform(train_data, is_validation=False)\n",
        "\n",
        "    def _create_basic_date_features(self, df):\n",
        "        \"\"\"Create basic date features for context (not used in SARIMA)\"\"\"\n",
        "        df = df.copy()\n",
        "        df['Month'] = df['Date'].dt.month\n",
        "        df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
        "        df['WeekOfYear'] = df['Date'].dt.isocalendar().week\n",
        "        df['IsWeekend'] = (df['DayOfWeek'] >= 5).astype(int)\n",
        "        start_date = df['Date'].min()\n",
        "        df['DaysFromStart'] = (df['Date'] - start_date).dt.days\n",
        "        df['WeeksFromStart'] = df['DaysFromStart'] // 7\n",
        "        return df\n",
        "\n",
        "    def _remove_outliers(self, df):\n",
        "        \"\"\"Remove outliers from training data only - separate thresholds for holiday vs non-holiday weeks\"\"\"\n",
        "        initial_len = len(df)\n",
        "        df_clean = df.copy()\n",
        "\n",
        "        non_holiday_removed = 0\n",
        "        holiday_removed = 0\n",
        "\n",
        "        # Apply different thresholds based on holiday status\n",
        "        for store_type in ['A', 'B', 'C']:\n",
        "            # Process non-holiday weeks\n",
        "            non_holiday_mask = (df_clean['IsHoliday'] == False) & (df_clean['Type'] == store_type)\n",
        "\n",
        "            if non_holiday_mask.any():\n",
        "                thresholds = self.outlier_thresholds['non_holiday'][store_type]\n",
        "                outlier_mask = (\n",
        "                    (df_clean['Weekly_Sales'] < thresholds['lower']) |\n",
        "                    (df_clean['Weekly_Sales'] > thresholds['upper'])\n",
        "                )\n",
        "                outliers_to_remove = non_holiday_mask & outlier_mask\n",
        "                non_holiday_removed += outliers_to_remove.sum()\n",
        "                df_clean = df_clean[~outliers_to_remove]\n",
        "\n",
        "            # Process holiday weeks\n",
        "            holiday_mask = (df_clean['IsHoliday'] == True) & (df_clean['Type'] == store_type)\n",
        "\n",
        "            if holiday_mask.any():\n",
        "                thresholds = self.outlier_thresholds['holiday'][store_type]\n",
        "                outlier_mask = (\n",
        "                    (df_clean['Weekly_Sales'] < thresholds['lower']) |\n",
        "                    (df_clean['Weekly_Sales'] > thresholds['upper'])\n",
        "                )\n",
        "                outliers_to_remove = holiday_mask & outlier_mask\n",
        "                holiday_removed += outliers_to_remove.sum()\n",
        "                df_clean = df_clean[~outliers_to_remove]\n",
        "\n",
        "        total_removed = initial_len - len(df_clean)\n",
        "\n",
        "        print(f\"   üóëÔ∏è Removed {total_removed:,} outliers from training data\")\n",
        "        print(f\"      üìÖ Non-holiday outliers: {non_holiday_removed:,}\")\n",
        "        print(f\"      üéâ Holiday outliers: {holiday_removed:,}\")\n",
        "\n",
        "        return df_clean\n",
        "\n",
        "\n",
        "def setup_mlflow():\n",
        "    \"\"\"Setup MLflow and DagsHub tracking\"\"\"\n",
        "    print(\"üîß Setting up MLflow and DagsHub...\")\n",
        "\n",
        "    # End any active runs first\n",
        "    try:\n",
        "        mlflow.end_run()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    # Initialize DagsHub\n",
        "    try:\n",
        "        dagshub.init(\n",
        "            repo_owner='konstantine25b',\n",
        "            repo_name='Walmart-Recruiting---Store-Sales-Forecasting',\n",
        "            mlflow=True\n",
        "        )\n",
        "        print(\"‚úÖ DagsHub initialized successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è DagsHub init warning: {e}\")\n",
        "\n",
        "    # Set MLflow tracking URI\n",
        "    mlflow.set_tracking_uri(\"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\")\n",
        "\n",
        "    # Create unique experiment name with timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    experiment_name = f\"Experiment_SARIMA_{timestamp}\"\n",
        "\n",
        "    try:\n",
        "        experiment_id = mlflow.create_experiment(experiment_name)\n",
        "        print(f\"‚úÖ Created new experiment: {experiment_name}\")\n",
        "    except mlflow.exceptions.MlflowException as e:\n",
        "        if \"already exists\" in str(e):\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            experiment_id = experiment.experiment_id\n",
        "            print(f\"‚úÖ Using existing experiment: {experiment_name}\")\n",
        "        else:\n",
        "            # Fallback to default experiment\n",
        "            experiment_name = \"Default\"\n",
        "            mlflow.set_experiment(experiment_name)\n",
        "            print(f\"‚ö†Ô∏è Using default experiment due to: {e}\")\n",
        "\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "\n",
        "    print(f\"‚úÖ MLflow setup complete!\")\n",
        "    print(f\"üîó Tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "    print(f\"üìä Experiment: {experiment_name}\")\n",
        "\n",
        "    return experiment_name\n",
        "\n",
        "\n",
        "def get_preprocessed_data():\n",
        "    \"\"\"\n",
        "    Use preprocessing pipeline to get model-ready data\n",
        "\n",
        "    Returns:\n",
        "        train_data, val_data: Time series data ready for SARIMA\n",
        "        split_info: Information about the temporal split\n",
        "    \"\"\"\n",
        "    print(\"üîÑ Getting preprocessed data using pipeline...\")\n",
        "\n",
        "    # Create the preprocessing pipeline\n",
        "    pipeline = WalmartSimplePreprocessingPipeline()\n",
        "\n",
        "    # Load raw data\n",
        "    train_full = pipeline.load_and_prepare_data()\n",
        "\n",
        "    # Create temporal split\n",
        "    train_data, val_data, split_info = pipeline.create_temporal_split(train_full)\n",
        "\n",
        "    # Fit and transform data using pipeline\n",
        "    pipeline.fit(train_data)\n",
        "    train_processed = pipeline.transform(train_data, is_validation=False)\n",
        "    val_processed = pipeline.transform(val_data, is_validation=True)\n",
        "\n",
        "    print(f\"‚úÖ Data preprocessing complete!\")\n",
        "    print(f\"   üìä Training shape: {train_processed.shape}\")\n",
        "    print(f\"   üìä Validation shape: {val_processed.shape}\")\n",
        "\n",
        "    return train_processed, val_processed, split_info\n",
        "\n",
        "\n",
        "def calculate_wmae(y_true, y_pred, is_holiday, holiday_weight=5.0):\n",
        "    \"\"\"Calculate Weighted Mean Absolute Error (WMAE)\"\"\"\n",
        "    abs_errors = np.abs(y_true - y_pred)\n",
        "    weights = np.where(is_holiday, holiday_weight, 1.0)\n",
        "    wmae = np.sum(weights * abs_errors) / np.sum(weights)\n",
        "    return wmae\n",
        "\n",
        "\n",
        "def check_stationarity(ts, title):\n",
        "    \"\"\"Check stationarity of time series using Augmented Dickey-Fuller test\"\"\"\n",
        "    # Perform Augmented Dickey-Fuller test\n",
        "    result = adfuller(ts.dropna())\n",
        "\n",
        "    is_stationary = result[1] <= 0.05\n",
        "\n",
        "    return {\n",
        "        'title': title,\n",
        "        'adf_statistic': result[0],\n",
        "        'p_value': result[1],\n",
        "        'critical_values': result[4],\n",
        "        'is_stationary': is_stationary\n",
        "    }\n"
      ],
      "metadata": {
        "id": "XdV7_PfPadx6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sarima_order(ts, max_p=1, max_d=1, max_q=1, max_P=1, max_D=1, max_Q=1, s=52, max_time=5):\n",
        "    \"\"\"Ultra-fast SARIMA order - use (1,1,1)x(0,0,0,52) for minimal seasonal computation\"\"\"\n",
        "    # Use minimal seasonal component (0,0,0,52) - still SARIMA but almost no seasonal computation\n",
        "    # This is much faster than (0,1,0,52) while keeping SARIMA structure\n",
        "    return (1, 1, 1), (0, 0, 0, 52), None\n",
        "\n",
        "\n",
        "def train_sarima_models(train_data, val_data):\n",
        "    \"\"\"Train ultra-fast SARIMA(1,1,1)x(0,0,0,52) models for each Store-Dept combination\"\"\"\n",
        "    print(\"üìà Training ultra-fast SARIMA(1,1,1)x(0,0,0,52) models for each Store-Dept combination...\")\n",
        "    print(\"   ‚è∞ No time limit - training all combinations\")\n",
        "\n",
        "    # Get unique combinations from training data\n",
        "    train_combinations = train_data.groupby(['Store', 'Dept']).size().index.tolist()\n",
        "    print(f\"   üìä Training models for {len(train_combinations)} combinations\")\n",
        "\n",
        "    # Train all combinations with ultra-fast models\n",
        "    print(f\"   üéØ Training ultra-fast SARIMA(1,1,1)x(0,0,0,52) for all combinations\")\n",
        "\n",
        "    models = {}\n",
        "    training_errors = {}\n",
        "    model_orders = {}\n",
        "\n",
        "    successful_models = 0\n",
        "    failed_models = 0\n",
        "\n",
        "    for i, (store, dept) in enumerate(train_combinations):\n",
        "        try:\n",
        "            # Filter data for this combination\n",
        "            store_dept_data = train_data[\n",
        "                (train_data['Store'] == store) &\n",
        "                (train_data['Dept'] == dept)\n",
        "            ].copy()\n",
        "\n",
        "            # Skip if insufficient data - very aggressive for speed\n",
        "            if len(store_dept_data) < 20:  # Reduced from 30 to 20 for maximum speed\n",
        "                failed_models += 1\n",
        "                continue\n",
        "\n",
        "            # Sort by date and prepare time series with proper DatetimeIndex\n",
        "            store_dept_data = store_dept_data.sort_values('Date')\n",
        "\n",
        "            # Create proper time series with DatetimeIndex - let pandas infer frequency\n",
        "            dates = pd.DatetimeIndex(store_dept_data['Date'])\n",
        "            ts = pd.Series(\n",
        "                store_dept_data['Weekly_Sales'].values,\n",
        "                index=dates\n",
        "            )\n",
        "\n",
        "            # Check for constant series\n",
        "            if ts.var() == 0 or ts.std() < 1e-6:\n",
        "                failed_models += 1\n",
        "                continue\n",
        "\n",
        "            # Use ultra-fast SARIMA(1,1,1)x(0,0,0,52) - minimal seasonal computation\n",
        "            best_order = (1, 1, 1)\n",
        "            best_seasonal_order = (0, 0, 0, 52)\n",
        "\n",
        "            # Fit ultra-fast SARIMA model with most aggressive speed optimizations\n",
        "            model = SARIMAX(ts, order=best_order, seasonal_order=best_seasonal_order)\n",
        "            fitted_model = model.fit(\n",
        "                disp=False,              # Suppress output\n",
        "                maxiter=10,              # Reduced from 25 to 10 for maximum speed\n",
        "                method='nm',             # Nelder-Mead - fastest for simple models\n",
        "                low_memory=True,         # Memory optimization\n",
        "                warn_convergence=False,  # Don't warn about convergence\n",
        "                optim_hessian='oim',     # Fastest hessian method\n",
        "                optim_complex_step=False # Simpler optimization\n",
        "            )\n",
        "\n",
        "            models[(store, dept)] = fitted_model\n",
        "            model_orders[(store, dept)] = (best_order, best_seasonal_order)\n",
        "            successful_models += 1\n",
        "\n",
        "            # Progress updates every 50 models for faster feedback\n",
        "            if i % 50 == 0:\n",
        "                print(f\"   ‚úÖ Trained {i+1}/{len(train_combinations)} models ({successful_models} successful, {failed_models} failed)\")\n",
        "\n",
        "        except Exception as e:\n",
        "            failed_models += 1\n",
        "            if failed_models < 3:  # Only print first few errors\n",
        "                print(f\"   ‚ö†Ô∏è Failed to train model for Store {store}, Dept {dept}: {e}\")\n",
        "\n",
        "    print(f\"‚úÖ SARIMA training complete!\")\n",
        "    print(f\"   üéØ Successful models: {successful_models}\")\n",
        "    print(f\"   ‚ùå Failed models: {failed_models}\")\n",
        "    print(f\"   üìä Coverage: {successful_models}/{len(train_combinations)} ({successful_models/len(train_combinations)*100:.1f}%)\")\n",
        "\n",
        "    return models, training_errors, model_orders\n",
        "\n",
        "\n",
        "def make_sarima_predictions(models, val_data, train_data=None):\n",
        "    \"\"\"Make predictions using ONLY trained SARIMA models - no fallbacks\"\"\"\n",
        "    print(\"üìà Making SARIMA predictions (no fallbacks)...\")\n",
        "\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    holidays = []\n",
        "    successful_predictions = 0\n",
        "    skipped_predictions = 0\n",
        "\n",
        "    # Get validation combinations\n",
        "    val_combinations = val_data.groupby(['Store', 'Dept']).groups.keys()\n",
        "\n",
        "    for store, dept in val_combinations:\n",
        "        try:\n",
        "            # Get validation data for this combination\n",
        "            store_dept_val = val_data[\n",
        "                (val_data['Store'] == store) &\n",
        "                (val_data['Dept'] == dept)\n",
        "            ].copy()\n",
        "\n",
        "            # Sort by date\n",
        "            store_dept_val = store_dept_val.sort_values('Date')\n",
        "\n",
        "            # ONLY use combinations where we have trained SARIMA models\n",
        "            if (store, dept) in models:\n",
        "                # Use trained SARIMA model\n",
        "                fitted_model = models[(store, dept)]\n",
        "\n",
        "                # Make forecast with proper periods\n",
        "                n_periods = len(store_dept_val)\n",
        "\n",
        "                try:\n",
        "                    # Use forecast method with steps parameter\n",
        "                    forecast = fitted_model.forecast(steps=n_periods)\n",
        "\n",
        "                    # Convert to list if it's a single value or array\n",
        "                    if isinstance(forecast, (int, float)):\n",
        "                        forecast_list = [forecast] * n_periods\n",
        "                    elif hasattr(forecast, '__iter__'):\n",
        "                        forecast_list = list(forecast)\n",
        "                        # Ensure forecast matches validation length\n",
        "                        if len(forecast_list) < n_periods:\n",
        "                            last_val = forecast_list[-1] if forecast_list else 0\n",
        "                            forecast_list.extend([last_val] * (n_periods - len(forecast_list)))\n",
        "                        elif len(forecast_list) > n_periods:\n",
        "                            forecast_list = forecast_list[:n_periods]\n",
        "                    else:\n",
        "                        # Skip this combination if forecast fails\n",
        "                        skipped_predictions += len(store_dept_val)\n",
        "                        continue\n",
        "\n",
        "                    # Store results\n",
        "                    predictions.extend(forecast_list)\n",
        "                    actuals.extend(store_dept_val['Weekly_Sales'].tolist())\n",
        "                    holidays.extend(store_dept_val['IsHoliday'].tolist())\n",
        "                    successful_predictions += len(store_dept_val)\n",
        "\n",
        "                except Exception as forecast_error:\n",
        "                    # Skip this combination if forecasting fails\n",
        "                    skipped_predictions += len(store_dept_val)\n",
        "                    continue\n",
        "            else:\n",
        "                # Skip combinations without trained models\n",
        "                skipped_predictions += len(store_dept_val)\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            # Skip this combination if any error occurs\n",
        "            skipped_predictions += len(val_data[\n",
        "                (val_data['Store'] == store) &\n",
        "                (val_data['Dept'] == dept)\n",
        "            ])\n",
        "            continue\n",
        "\n",
        "    print(f\"‚úÖ Predictions complete!\")\n",
        "    print(f\"   üéØ SARIMA predictions: {successful_predictions}\")\n",
        "    print(f\"   ‚è≠Ô∏è Skipped (no model): {skipped_predictions}\")\n",
        "\n",
        "    return np.array(predictions), np.array(actuals), np.array(holidays)\n",
        "\n",
        "\n",
        "def calculate_training_wmae(models, train_data):\n",
        "    \"\"\"Calculate training WMAE on fitted values from trained SARIMA models\"\"\"\n",
        "    print(\"üìä Calculating training WMAE on fitted values...\")\n",
        "\n",
        "    train_predictions = []\n",
        "    train_actuals = []\n",
        "    train_holidays = []\n",
        "\n",
        "    for (store, dept), fitted_model in models.items():\n",
        "        try:\n",
        "            # Get training data for this combination\n",
        "            store_dept_data = train_data[\n",
        "                (train_data['Store'] == store) &\n",
        "                (train_data['Dept'] == dept)\n",
        "            ].copy()\n",
        "\n",
        "            # Sort by date\n",
        "            store_dept_data = store_dept_data.sort_values('Date')\n",
        "\n",
        "            # Get fitted values from the model\n",
        "            fitted_values = fitted_model.fittedvalues\n",
        "            actual_values = store_dept_data['Weekly_Sales'].values\n",
        "            holiday_values = store_dept_data['IsHoliday'].values\n",
        "\n",
        "            # Align fitted values with actual values (fitted values might be shorter)\n",
        "            if len(fitted_values) > 0 and len(fitted_values) <= len(actual_values):\n",
        "                # Take the last N actual values to match fitted values length\n",
        "                start_idx = len(actual_values) - len(fitted_values)\n",
        "                aligned_actuals = actual_values[start_idx:]\n",
        "                aligned_holidays = holiday_values[start_idx:]\n",
        "\n",
        "                train_predictions.extend(fitted_values.values)\n",
        "                train_actuals.extend(aligned_actuals)\n",
        "                train_holidays.extend(aligned_holidays)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Skip problematic models\n",
        "            continue\n",
        "\n",
        "    if len(train_predictions) > 0:\n",
        "        train_wmae = calculate_wmae(\n",
        "            np.array(train_actuals),\n",
        "            np.array(train_predictions),\n",
        "            np.array(train_holidays).astype(bool)\n",
        "        )\n",
        "        print(f\"   üìà Training WMAE: ${train_wmae:,.2f}\")\n",
        "        return train_wmae\n",
        "    else:\n",
        "        print(\"   ‚ö†Ô∏è No training predictions available for WMAE calculation\")\n",
        "        return None\n",
        "\n"
      ],
      "metadata": {
        "id": "-mcNF96aafHm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    \"\"\"Main experiment execution\"\"\"\n",
        "    print(\"üöÄ Starting Experiment SARIMA: SARIMA Models for Walmart Sales Forecasting\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Setup MLflow tracking\n",
        "    experiment_name = setup_mlflow()\n",
        "\n",
        "    with mlflow.start_run(run_name=\"SARIMA_Walmart_Sales_Complete\") as run:\n",
        "        print(f\"üîÑ Starting MLflow run: {run.info.run_id}\")\n",
        "\n",
        "        # Log experiment metadata\n",
        "        mlflow.log_param(\"experiment_type\", \"SARIMA_Individual_Models\")\n",
        "        mlflow.log_param(\"model_type\", \"SARIMA\")\n",
        "        mlflow.log_param(\"feature_engineering\", \"Minimal_Time_Series_Only\")\n",
        "        mlflow.log_param(\"data_split\", \"temporal_80_20\")\n",
        "        mlflow.log_param(\"external_regressors\", \"None\")\n",
        "        mlflow.log_param(\"outlier_removal\", \"Holiday_Aware_Thresholds\")\n",
        "        mlflow.log_param(\"holiday_weight_evaluation\", \"5x\")\n",
        "        mlflow.log_param(\"seasonal_period\", \"52_weeks\")\n",
        "        mlflow.log_param(\"sarima_order\", \"(1,1,1)x(0,0,0,52)\")\n",
        "        mlflow.log_param(\"optimization_method\", \"nelder_mead_ultra_fast\")\n",
        "        mlflow.log_param(\"min_data_points\", \"20\")\n",
        "        mlflow.log_param(\"max_iterations\", \"10\")\n",
        "\n",
        "        # Log outlier thresholds for transparency\n",
        "        mlflow.log_param(\"outlier_thresholds_non_holiday_A\", \"[-1000, 35000]\")\n",
        "        mlflow.log_param(\"outlier_thresholds_non_holiday_B\", \"[-500, 20000]\")\n",
        "        mlflow.log_param(\"outlier_thresholds_non_holiday_C\", \"[-200, 12000]\")\n",
        "        mlflow.log_param(\"outlier_thresholds_holiday_A\", \"[-1000, 80000]\")\n",
        "        mlflow.log_param(\"outlier_thresholds_holiday_B\", \"[-500, 50000]\")\n",
        "        mlflow.log_param(\"outlier_thresholds_holiday_C\", \"[-200, 30000]\")\n",
        "\n",
        "        try:\n",
        "            # Step 1: Get preprocessed data\n",
        "            print(\"\\nüìä Step 1: Data preprocessing...\")\n",
        "            train_data, val_data, split_info = get_preprocessed_data()\n",
        "\n",
        "            # Log data info\n",
        "            mlflow.log_metric(\"train_samples\", len(train_data))\n",
        "            mlflow.log_metric(\"val_samples\", len(val_data))\n",
        "            mlflow.log_param(\"split_date\", str(split_info['split_date']))\n",
        "\n",
        "            # Log Store-Dept combination info\n",
        "            train_combinations = set(zip(train_data['Store'], train_data['Dept']))\n",
        "            val_combinations = set(zip(val_data['Store'], val_data['Dept']))\n",
        "\n",
        "            mlflow.log_metric(\"train_combinations\", len(train_combinations))\n",
        "            mlflow.log_metric(\"val_combinations\", len(val_combinations))\n",
        "            mlflow.log_metric(\"missing_combinations\", len(train_combinations - val_combinations))\n",
        "\n",
        "            # Step 2: Train SARIMA models\n",
        "            print(\"\\nüìà Step 2: Training SARIMA models...\")\n",
        "            models, training_errors, model_orders = train_sarima_models(train_data, val_data)\n",
        "\n",
        "            # Log training info\n",
        "            mlflow.log_metric(\"successful_models\", len(models))\n",
        "            mlflow.log_metric(\"avg_training_mae\", np.mean(list(training_errors.values())) if training_errors else 0)\n",
        "\n",
        "            # Log model order statistics\n",
        "            if model_orders:\n",
        "                orders = [o[0] for o in model_orders.values()]  # Non-seasonal orders\n",
        "                seasonal_orders = [o[1] for o in model_orders.values()]  # Seasonal orders\n",
        "\n",
        "                avg_p = np.mean([o[0] for o in orders])\n",
        "                avg_d = np.mean([o[1] for o in orders])\n",
        "                avg_q = np.mean([o[2] for o in orders])\n",
        "                avg_P = np.mean([o[0] for o in seasonal_orders])\n",
        "                avg_D = np.mean([o[1] for o in seasonal_orders])\n",
        "                avg_Q = np.mean([o[2] for o in seasonal_orders])\n",
        "                avg_s = np.mean([o[3] for o in seasonal_orders])\n",
        "\n",
        "                mlflow.log_metric(\"avg_sarima_p\", avg_p)\n",
        "                mlflow.log_metric(\"avg_sarima_d\", avg_d)\n",
        "                mlflow.log_metric(\"avg_sarima_q\", avg_q)\n",
        "                mlflow.log_metric(\"avg_sarima_P\", avg_P)\n",
        "                mlflow.log_metric(\"avg_sarima_D\", avg_D)\n",
        "                mlflow.log_metric(\"avg_sarima_Q\", avg_Q)\n",
        "                mlflow.log_metric(\"avg_sarima_s\", avg_s)\n",
        "\n",
        "            # Step 3: Make predictions\n",
        "            print(\"\\nüìà Step 3: Making predictions...\")\n",
        "            y_pred, y_true, is_holiday = make_sarima_predictions(models, val_data, train_data)\n",
        "\n",
        "            # Step 3.5: Calculate training WMAE\n",
        "            print(\"\\nüìä Step 3.5: Training performance...\")\n",
        "            train_wmae = calculate_training_wmae(models, train_data)\n",
        "\n",
        "            # Step 4: Calculate metrics\n",
        "            print(\"\\nüìä Step 4: Calculating validation metrics...\")\n",
        "\n",
        "            # Validation metrics\n",
        "            val_mae = mean_absolute_error(y_true, y_pred)\n",
        "            val_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "            val_r2 = r2_score(y_true, y_pred)\n",
        "            val_wmae = calculate_wmae(y_true, y_pred, is_holiday)\n",
        "\n",
        "            # Holiday breakdown\n",
        "            holiday_mask = is_holiday.astype(bool)\n",
        "            holiday_mae = mean_absolute_error(y_true[holiday_mask], y_pred[holiday_mask]) if holiday_mask.any() else 0\n",
        "            non_holiday_mae = mean_absolute_error(y_true[~holiday_mask], y_pred[~holiday_mask]) if (~holiday_mask).any() else 0\n",
        "\n",
        "            # Log all metrics\n",
        "            mlflow.log_metric(\"val_wmae\", val_wmae)\n",
        "            mlflow.log_metric(\"val_mae\", val_mae)\n",
        "            mlflow.log_metric(\"val_rmse\", val_rmse)\n",
        "            mlflow.log_metric(\"val_r2\", val_r2)\n",
        "            mlflow.log_metric(\"holiday_mae\", holiday_mae)\n",
        "            mlflow.log_metric(\"non_holiday_mae\", non_holiday_mae)\n",
        "            mlflow.log_metric(\"holiday_samples\", int(holiday_mask.sum()))\n",
        "            mlflow.log_metric(\"non_holiday_samples\", int((~holiday_mask).sum()))\n",
        "\n",
        "            # Log training WMAE if available\n",
        "            if train_wmae is not None:\n",
        "                mlflow.log_metric(\"train_wmae\", train_wmae)\n",
        "\n",
        "            # Print results\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"üéØ EXPERIMENT SARIMA RESULTS SUMMARY\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            if train_wmae is not None:\n",
        "                print(\"üìä Training Metrics:\")\n",
        "                print(f\"   Training WMAE: ${train_wmae:,.2f}\")\n",
        "                print()\n",
        "\n",
        "            print(\"üìä Validation Metrics:\")\n",
        "            print(f\"   WMAE (Competition Metric): ${val_wmae:,.2f}\")\n",
        "            print(f\"   MAE: ${val_mae:,.2f}\")\n",
        "            print(f\"   RMSE: ${val_rmse:,.2f}\")\n",
        "            print(f\"   R¬≤: {val_r2:.4f}\")\n",
        "\n",
        "            print(\"\\nüìä Holiday Breakdown:\")\n",
        "            print(f\"   Holiday MAE: ${holiday_mae:,.2f} ({int(holiday_mask.sum())} samples)\")\n",
        "            print(f\"   Non-Holiday MAE: ${non_holiday_mae:,.2f} ({int((~holiday_mask).sum())} samples)\")\n",
        "\n",
        "            print(\"\\nüìä Model Statistics:\")\n",
        "            print(f\"   Successful models trained: {len(models):,}\")\n",
        "            print(f\"   Store-Dept combinations: {len(train_combinations):,}\")\n",
        "            print(f\"   Average training MAE: ${np.mean(list(training_errors.values())):,.2f}\" if training_errors else \"   No training errors calculated\")\n",
        "\n",
        "            # Save model summary\n",
        "            print(\"\\nüíæ Saving model artifacts...\")\n",
        "            model_summary = {\n",
        "                'total_models': len(models),\n",
        "                'model_orders': {f\"{k[0]}_{k[1]}\": v for k, v in model_orders.items()},\n",
        "                'training_errors': {f\"{k[0]}_{k[1]}\": v for k, v in training_errors.items()},\n",
        "                'validation_metrics': {\n",
        "                    'wmae': val_wmae,\n",
        "                    'mae': val_mae,\n",
        "                    'rmse': val_rmse,\n",
        "                    'r2': val_r2\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Save summary to file and log as artifact\n",
        "            import json\n",
        "            with open('sarima_model_summary.json', 'w') as f:\n",
        "                json.dump(model_summary, f, indent=2)\n",
        "            mlflow.log_artifact('sarima_model_summary.json')\n",
        "\n",
        "            print(\"‚úÖ Experiment SARIMA completed successfully!\")\n",
        "\n",
        "            # Get experiment and run URLs\n",
        "            experiment = mlflow.get_experiment_by_name(experiment_name)\n",
        "            if experiment:\n",
        "                experiment_id = experiment.experiment_id\n",
        "                run_id = run.info.run_id\n",
        "                base_url = \"https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\"\n",
        "\n",
        "                print(f\"üèÉ View run at: {base_url}/#/experiments/{experiment_id}/runs/{run_id}\")\n",
        "                print(f\"üìä View experiment at: {base_url}/#/experiments/{experiment_id}\")\n",
        "\n",
        "            print(f\"\\nüéâ Experiment SARIMA: Individual SARIMA Models - COMPLETE!\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Experiment failed: {e}\")\n",
        "            mlflow.log_param(\"error\", str(e))\n",
        "            raise\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "ocvVu714ajaF",
        "outputId": "f753e5c9-bfe3-427c-92be-ff40966738d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Experiment SARIMA: SARIMA Models for Walmart Sales Forecasting\n",
            "================================================================================\n",
            "üîß Setting up MLflow and DagsHub...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ DagsHub initialized successfully!\n",
            "‚úÖ Created new experiment: Experiment_SARIMA_20250715_100113\n",
            "‚úÖ MLflow setup complete!\n",
            "üîó Tracking URI: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow\n",
            "üìä Experiment: Experiment_SARIMA_20250715_100113\n",
            "üîÑ Starting MLflow run: 28b9977af949455aa97edbe3a8b2e4e8\n",
            "\n",
            "üìä Step 1: Data preprocessing...\n",
            "üîÑ Getting preprocessed data using pipeline...\n",
            "üìä Loading datasets...\n",
            "   üìà Train data: (421570, 5)\n",
            "   üè™ Stores data: (45, 3)\n",
            "   ‚úÖ Merged data: (421570, 7)\n",
            "   üìÖ Date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "üìÖ Creating temporal split (80/19)...\n",
            "   üìä Split date: 2012-04-13 00:00:00\n",
            "   üìà Train: 337,256 records (2010-02-05 00:00:00 to 2012-04-13 00:00:00)\n",
            "   üìâ Val: 84,314 records (2012-04-13 00:00:00 to 2012-10-26 00:00:00)\n",
            "üîß Fitting preprocessing pipeline on training data...\n",
            "‚úÖ Pipeline fitted on training data with holiday-aware outlier thresholds\n",
            "üîÑ Transforming training data...\n",
            "   üóëÔ∏è Removed 60,884 outliers from training data\n",
            "      üìÖ Non-holiday outliers: 59,545\n",
            "      üéâ Holiday outliers: 1,339\n",
            "‚úÖ Transform complete. Shape: (276372, 13)\n",
            "üîÑ Transforming validation data...\n",
            "‚úÖ Transform complete. Shape: (84314, 13)\n",
            "‚úÖ Data preprocessing complete!\n",
            "   üìä Training shape: (276372, 13)\n",
            "   üìä Validation shape: (84314, 13)\n",
            "\n",
            "üìà Step 2: Training SARIMA models...\n",
            "üìà Training ultra-fast SARIMA(1,1,1)x(0,0,0,52) models for each Store-Dept combination...\n",
            "   ‚è∞ No time limit - training all combinations\n",
            "   üìä Training models for 3248 combinations\n",
            "   üéØ Training ultra-fast SARIMA(1,1,1)x(0,0,0,52) for all combinations\n",
            "   ‚úÖ Trained 1/3248 models (1 successful, 0 failed)\n",
            "   ‚úÖ Trained 51/3248 models (46 successful, 5 failed)\n",
            "   ‚úÖ Trained 101/3248 models (86 successful, 15 failed)\n",
            "   ‚úÖ Trained 151/3248 models (124 successful, 27 failed)\n",
            "   ‚úÖ Trained 201/3248 models (172 successful, 29 failed)\n",
            "   ‚úÖ Trained 251/3248 models (215 successful, 36 failed)\n",
            "   ‚úÖ Trained 301/3248 models (251 successful, 50 failed)\n",
            "   ‚úÖ Trained 351/3248 models (298 successful, 53 failed)\n",
            "   ‚úÖ Trained 401/3248 models (341 successful, 60 failed)\n",
            "   ‚úÖ Trained 451/3248 models (381 successful, 70 failed)\n",
            "   ‚úÖ Trained 501/3248 models (427 successful, 74 failed)\n",
            "   ‚úÖ Trained 551/3248 models (474 successful, 77 failed)\n",
            "   ‚úÖ Trained 601/3248 models (514 successful, 87 failed)\n",
            "   ‚úÖ Trained 701/3248 models (589 successful, 112 failed)\n",
            "   ‚úÖ Trained 751/3248 models (630 successful, 121 failed)\n",
            "   ‚úÖ Trained 801/3248 models (672 successful, 129 failed)\n",
            "   ‚úÖ Trained 851/3248 models (711 successful, 140 failed)\n",
            "   ‚úÖ Trained 901/3248 models (746 successful, 155 failed)\n",
            "   ‚úÖ Trained 951/3248 models (787 successful, 164 failed)\n",
            "   ‚úÖ Trained 1001/3248 models (828 successful, 173 failed)\n",
            "   ‚úÖ Trained 1051/3248 models (867 successful, 184 failed)\n",
            "   ‚úÖ Trained 1101/3248 models (908 successful, 193 failed)\n",
            "   ‚úÖ Trained 1201/3248 models (997 successful, 204 failed)\n",
            "   ‚úÖ Trained 1251/3248 models (1038 successful, 213 failed)\n",
            "   ‚úÖ Trained 1351/3248 models (1117 successful, 234 failed)\n",
            "   ‚úÖ Trained 1451/3248 models (1202 successful, 249 failed)\n",
            "   ‚úÖ Trained 1501/3248 models (1241 successful, 260 failed)\n",
            "   ‚úÖ Trained 1601/3248 models (1321 successful, 280 failed)\n",
            "   ‚úÖ Trained 1751/3248 models (1435 successful, 316 failed)\n",
            "   ‚úÖ Trained 1801/3248 models (1473 successful, 328 failed)\n",
            "   ‚úÖ Trained 1851/3248 models (1515 successful, 336 failed)\n",
            "   ‚úÖ Trained 1901/3248 models (1561 successful, 340 failed)\n",
            "   ‚úÖ Trained 2051/3248 models (1687 successful, 364 failed)\n",
            "   ‚úÖ Trained 2101/3248 models (1724 successful, 377 failed)\n",
            "   ‚úÖ Trained 2151/3248 models (1768 successful, 383 failed)\n",
            "   ‚úÖ Trained 2201/3248 models (1809 successful, 392 failed)\n",
            "   ‚úÖ Trained 2251/3248 models (1844 successful, 407 failed)\n",
            "   ‚úÖ Trained 2401/3248 models (1974 successful, 427 failed)\n",
            "   ‚úÖ Trained 2451/3248 models (2012 successful, 439 failed)\n",
            "   ‚úÖ Trained 2501/3248 models (2060 successful, 441 failed)\n",
            "   ‚úÖ Trained 2551/3248 models (2100 successful, 451 failed)\n",
            "   ‚úÖ Trained 2651/3248 models (2176 successful, 475 failed)\n",
            "   ‚úÖ Trained 2701/3248 models (2211 successful, 490 failed)\n",
            "   ‚úÖ Trained 2751/3248 models (2248 successful, 503 failed)\n",
            "   ‚úÖ Trained 2801/3248 models (2289 successful, 512 failed)\n",
            "   ‚úÖ Trained 2851/3248 models (2330 successful, 521 failed)\n",
            "   ‚úÖ Trained 2901/3248 models (2378 successful, 523 failed)\n",
            "   ‚úÖ Trained 2951/3248 models (2420 successful, 531 failed)\n",
            "   ‚úÖ Trained 3051/3248 models (2491 successful, 560 failed)\n",
            "   ‚úÖ Trained 3101/3248 models (2523 successful, 578 failed)\n",
            "   ‚úÖ Trained 3151/3248 models (2557 successful, 594 failed)\n",
            "   ‚úÖ Trained 3201/3248 models (2596 successful, 605 failed)\n",
            "‚úÖ SARIMA training complete!\n",
            "   üéØ Successful models: 2633\n",
            "   ‚ùå Failed models: 615\n",
            "   üìä Coverage: 2633/3248 (81.1%)\n",
            "\n",
            "üìà Step 3: Making predictions...\n",
            "üìà Making SARIMA predictions (no fallbacks)...\n",
            "‚úÖ Predictions complete!\n",
            "   üéØ SARIMA predictions: 71486\n",
            "   ‚è≠Ô∏è Skipped (no model): 12828\n",
            "\n",
            "üìä Step 3.5: Training performance...\n",
            "üìä Calculating training WMAE on fitted values...\n",
            "   üìà Training WMAE: $1,586.54\n",
            "\n",
            "üìä Step 4: Calculating validation metrics...\n",
            "\n",
            "============================================================\n",
            "üéØ EXPERIMENT SARIMA RESULTS SUMMARY\n",
            "============================================================\n",
            "üìä Training Metrics:\n",
            "   Training WMAE: $1,586.54\n",
            "\n",
            "üìä Validation Metrics:\n",
            "   WMAE (Competition Metric): $2,219.86\n",
            "   MAE: $2,173.68\n",
            "   RMSE: $5,867.32\n",
            "   R¬≤: 0.6853\n",
            "\n",
            "üìä Holiday Breakdown:\n",
            "   Holiday MAE: $2,547.73 (2517 samples)\n",
            "   Non-Holiday MAE: $2,160.03 (68969 samples)\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Successful models trained: 2,633\n",
            "   Store-Dept combinations: 3,248\n",
            "   No training errors calculated\n",
            "\n",
            "üíæ Saving model artifacts...\n",
            "‚úÖ Experiment SARIMA completed successfully!\n",
            "üèÉ View run at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/58/runs/28b9977af949455aa97edbe3a8b2e4e8\n",
            "üìä View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/58\n",
            "\n",
            "üéâ Experiment SARIMA: Individual SARIMA Models - COMPLETE!\n",
            "üèÉ View run SARIMA_Walmart_Sales_Complete at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/58/runs/28b9977af949455aa97edbe3a8b2e4e8\n",
            "üß™ View experiment at: https://dagshub.com/konstantine25b/Walmart-Recruiting---Store-Sales-Forecasting.mlflow/#/experiments/58\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}